{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "533bc627-59f9-4c89-b7a3-e0397222deed",
   "metadata": {},
   "source": [
    "# ***Part 0: The Past is History, But 2025 GTC/GDC Week Was a Viibe***\n",
    "\n",
    "### **i. Last Week: Breeding AI Models Together Like PokÃ©mon to Build a Predictor**  \n",
    "\n",
    "Medicine moves slowly, but markets move fast. Weâ€™ve been repurposing my original ML pipelineâ€”the one I built years ago to navigate my own scleroderma and eventually achieve remission. Last week, we continued tinkering with the projectâ€”once built to guide treatment decisions and predict health outcomesâ€”to tackle a new challenge: forecasting financial stress and shock events.\n",
    "\n",
    "We threw **traditional ML, deep learning, diffusion models, and evolutionary algorithms** into the \"boxing ring\" to see **which could feel economic turbulence before it happened.**\n",
    "\n",
    "| **Model** | **Best At...** | **Weaknesses** |\n",
    "|------------|---------------|----------------|\n",
    "| **Elastic Net & SGD** | âœ… **Most precise in regression (RÂ² = 0.976)** âœ… **Lowest error (MSE = 0.035)** | âŒ **Weak classification confidence (ROC AUC = 0.42 - 0.66)** |\n",
    "| **CNN (Deep Learning)** | âœ… **Most stable over time** (cross-validation & residuals) | âŒ **Overfitting risk (ROC AUC = 0.51 â†’ struggles to classify stress events)** |\n",
    "| **Diffusion Model** | âœ… **Good for complex relationships** âœ… **Moderate prediction stability (RÂ² = 0.881)** | âŒ **Low classification confidence (ROC AUC = 0.54 â†’ barely better than random guessing) (failed to converge)** |\n",
    "| **GA-Optimized Logistic Regression (GA-LR)** | âœ… **Evolutionary feature selection** for stress forecasting | âŒ **Inconsistent across validation sets** âŒ **Lowest accuracy (61.4%)** |\n",
    "| **NeuroEvolution (NEAT)** | âœ… **Best classification confidence (ROC AUC = 0.72)** âœ… **Adapts well to stress events** | âŒ **Computationally expensive** |\n",
    "\n",
    "### **ii. Takeaways:**  \n",
    "- **Elastic Net & SGD** were **sharp in regression but weak at detecting stress events.**  \n",
    "- **CNNs** were **the most stable over time** but had a habit of **overfitting** (cheating despite us splitting the train/test/validate data).  \n",
    "- **Diffusion models** were **good at learning complex patterns** but **lacked confidence in predictions** (struggled to converge).\n",
    "- **GA-LR tried to evolve itself into success but lacked consistency.**  \n",
    "- **NEAT stole the show**â€”it had **the best classification confidence (ROC AUC = 0.72),** making it **great for identifying stress events**, but **itâ€™s computationally expensive** (thank you Sakana.AI for intro-ing this method)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45002390-0a69-4819-9cbf-f8c35bd76c8a",
   "metadata": {},
   "source": [
    "# ***Part 1: Breeding AI Models for an Adaptive Hybrid Ensemble***\n",
    "\n",
    "No single model is perfectâ€”so why not **evolve them like PokÃ©mon breeding?** Instead of relying on just one approach, weâ€™re dynamically selecting the best traits from multiple models, ensuring that **each new generation outperforms the last.**  \n",
    "\n",
    "This week, we're building an **adaptive ensemble that evolves based on performance.** Rather than simply merging NEATâ€™s adaptability with Elastic Netâ€™s precision, we're **selecting and combining the strongest traits from multiple parent models** while mitigating weaknesses. The result? Hybrid models that **learn, adapt, and improve beyond their predecessors**â€”just like an elite trader fine-tuning their strategy over time.  \n",
    "\n",
    "Our fully automated pipeline **evaluates models, selects top performers, and breeds new hybrids that carry forward strengths while eliminating weaknesses.** This continuous evolution ensures our AI adapts dynamically to market shifts, always improving with each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "dc1dbae5-8151-42dc-ab6a-58f6fce37b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import neat\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import ElasticNetCV, SGDRegressor, LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier  # CNN Stand-in\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score\n",
    "from sklearn.mixture import GaussianMixture  # Diffusion Model\n",
    "from deap import base, creator, tools, algorithms  # GA-LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "6947ccbf-3bed-4169-a966-d14e9a98fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# ðŸ“Œ STEP 1: PREPROCESS DATA\n",
    "# -----------------------------------------------\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Handles missing values, computes market stress, and creates lagged features.\"\"\"\n",
    "    threshold = 0.3 * len(df)\n",
    "    df_cleaned = df.dropna(axis=1, thresh=threshold)\n",
    "    df_cleaned.fillna(df_cleaned.median(numeric_only=True), inplace=True)\n",
    "\n",
    "    # âœ… Compute rolling z-scores\n",
    "    def compute_rolling_zscores(df, cols, window=90):\n",
    "        rolling_mean = df[cols].rolling(window=window, min_periods=1).mean()\n",
    "        rolling_std = df[cols].rolling(window=window, min_periods=1).std()\n",
    "        return (df[cols] - rolling_mean) / rolling_std\n",
    "\n",
    "    zscore_cols = [\"inflation\", \"Interest Rate\", \"interest rates\"]\n",
    "    df_zscores = compute_rolling_zscores(df_cleaned, zscore_cols)\n",
    "    df_zscores.columns = [f\"{col}_z\" for col in zscore_cols]\n",
    "    df_cleaned = pd.concat([df_cleaned, df_zscores], axis=1)\n",
    "\n",
    "    # âœ… Define market stress periods\n",
    "    df_cleaned[\"market_stress\"] = ((df_cleaned[\"inflation_z\"] > 1) &\n",
    "                                   (df_cleaned[\"Interest Rate_z\"] > 1) &\n",
    "                                   (df_cleaned[\"interest rates_z\"] > 1)).astype(int)\n",
    "\n",
    "    # âœ… Create lagged features\n",
    "    lag_features = [\"inflation\", \"Interest Rate\", \"interest rates\"]\n",
    "    lags = [5, 10, 30]\n",
    "    for feature in lag_features:\n",
    "        for lag in lags:\n",
    "            df_cleaned[f\"{feature}_lag{lag}\"] = df_cleaned[feature].shift(lag)\n",
    "\n",
    "    df_cleaned.dropna(inplace=True)\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "1e934df5-69e9-47c3-af4a-6b0a26aff15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… preprocess_data() passed.\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "### asserts: ###\n",
    "################\n",
    "def test_preprocess_data():\n",
    "    df = pd.read_csv(\"data/financial_data_cleaned2.csv\")\n",
    "    df_cleaned = preprocess_data(df)\n",
    "\n",
    "    assert df_cleaned.isna().sum().sum() == 0, \"NaNs found\"\n",
    "    assert all(col in df_cleaned.columns for col in [\"inflation_z\", \"Interest Rate_z\", \"interest rates_z\"]), \"Missing z-score columns\"\n",
    "    assert df_cleaned[\"market_stress\"].isin([0, 1]).all(), \"market_stress not binary\"\n",
    "\n",
    "    for feature in [\"inflation\", \"Interest Rate\", \"interest rates\"]:\n",
    "        for lag in [5, 10, 30]:\n",
    "            assert f\"{feature}_lag{lag}\" in df_cleaned.columns, f\"Missing {feature}_lag{lag}\"\n",
    "\n",
    "    print(\"âœ… preprocess_data() passed.\")\n",
    "\n",
    "# âœ… Run the test\n",
    "test_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "a4cbecab-b05d-491d-a0e8-c435273c6c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# ðŸ“Œ STEP 2: SCALE FEATURES\n",
    "# -----------------------------------------------\n",
    "\n",
    "def scale_features(df_cleaned):\n",
    "    \"\"\"Scales numerical features, excluding the target column.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    num_cols = df_cleaned.drop(columns=[\"market_stress\"]).select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df_cleaned[num_cols]), columns=num_cols)\n",
    "    df_scaled[\"market_stress\"] = df_cleaned[\"market_stress\"].values  # Add back without scaling\n",
    "\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "d59a96f7-d88f-42f2-a5a7-572d64088eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Standard Deviations After Scaling:\n",
      "Adj Close_^GSPC         1.000083\n",
      "Adj Close_^IXIC         1.000083\n",
      "Adj Close_^VIX          1.000083\n",
      "Bond Yields             1.000083\n",
      "Inflation               1.000083\n",
      "                          ...   \n",
      "Interest Rate_lag5      1.000083\n",
      "Interest Rate_lag10     1.000083\n",
      "interest rates_lag5     1.000083\n",
      "interest rates_lag10    1.000083\n",
      "interest rates_lag30    1.000083\n",
      "Length: 213, dtype: float64\n",
      "âœ… scale_features() passed.\n"
     ]
    }
   ],
   "source": [
    "def test_scale_features():\n",
    "    df = pd.read_csv(\"data/financial_data_cleaned2.csv\")\n",
    "    df_cleaned = preprocess_data(df)\n",
    "    df_scaled = scale_features(df_cleaned)\n",
    "\n",
    "    # âœ… Get numerical columns\n",
    "    num_cols = df_cleaned.drop(columns=[\"market_stress\"]).select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # âœ… Check for expected properties\n",
    "    assert \"market_stress\" in df_scaled, \"Missing target column\"\n",
    "    assert not df_scaled.isna().any().any(), \"NaNs found after scaling\"\n",
    "    assert np.allclose(df_scaled[num_cols].mean(), 0, atol=0.01), \"Mean not close to zero\"\n",
    "\n",
    "    # âœ… Adjusted standard deviation tolerance to 0.1\n",
    "    std_devs = df_scaled[num_cols].std()\n",
    "    print(f\"ðŸ“Š Standard Deviations After Scaling:\\n{std_devs}\")\n",
    "    assert np.allclose(std_devs, 1, atol=0.1), \"Std dev not ~1\"\n",
    "\n",
    "    print(\"âœ… scale_features() passed.\")\n",
    "\n",
    "# âœ… Run the test\n",
    "test_scale_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "b8db53e9-2d08-482d-8eff-72dbb85c916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# ðŸ“Œ STEP 3: APPLY PCA\n",
    "# -----------------------------------------------\n",
    "\n",
    "def apply_pca(df_scaled, n_components=50):\n",
    "    \"\"\"Applies PCA for dimensionality reduction.\"\"\"\n",
    "    df_pca_input = df_scaled.drop(columns=[\"market_stress\"])\n",
    "    pca = PCA(n_components=min(n_components, df_pca_input.shape[1]))\n",
    "    principal_components = pca.fit_transform(df_pca_input)\n",
    "\n",
    "    df_pca = pd.DataFrame(principal_components, columns=[f\"PC{i+1}\" for i in range(pca.n_components_)])\n",
    "    df_pca[\"market_stress\"] = df_scaled[\"market_stress\"].values\n",
    "    return df_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "ce531f32-577d-472e-bcaf-aa3e9157987b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Explained Variance Retained: 0.9990\n",
      "âœ… apply_pca() passed.\n"
     ]
    }
   ],
   "source": [
    "def test_apply_pca():\n",
    "    df = pd.read_csv(\"data/financial_data_cleaned2.csv\")\n",
    "    df_cleaned = preprocess_data(df)\n",
    "    df_scaled = scale_features(df_cleaned)\n",
    "    df_pca = apply_pca(df_scaled, n_components=50)\n",
    "\n",
    "    # âœ… Check that the number of principal components is correct\n",
    "    expected_n_components = min(50, df_scaled.shape[1] - 1)  # -1 for 'market_stress'\n",
    "    assert df_pca.shape[1] == expected_n_components + 1, f\"Unexpected PCA shape: {df_pca.shape}\"\n",
    "\n",
    "    # âœ… Ensure \"market_stress\" column is still present\n",
    "    assert \"market_stress\" in df_pca.columns, \"market_stress column missing after PCA\"\n",
    "\n",
    "    # âœ… Check that all principal components are numerical\n",
    "    assert df_pca.drop(columns=[\"market_stress\"]).select_dtypes(include=[np.number]).shape[1] == expected_n_components, \\\n",
    "        \"Non-numeric values found in PCA output\"\n",
    "\n",
    "    # âœ… Variance check: PCA should reduce dimensions while keeping most of the variance\n",
    "    pca = PCA(n_components=expected_n_components)\n",
    "    pca.fit(df_scaled.drop(columns=[\"market_stress\"]))\n",
    "    explained_variance = np.sum(pca.explained_variance_ratio_)\n",
    "    print(f\"ðŸ“Š Explained Variance Retained: {explained_variance:.4f}\")\n",
    "    assert explained_variance > 0.8, \"PCA did not retain enough variance (should be >80%)\"\n",
    "\n",
    "    print(\"âœ… apply_pca() passed.\")\n",
    "\n",
    "# âœ… Run the test\n",
    "test_apply_pca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "720216ca-5bf1-4cdc-bdf9-00f71c08636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# ðŸ“Œ STEP 4: SPLIT DATA\n",
    "# -----------------------------------------------\n",
    "\n",
    "def split_data(df):\n",
    "    \"\"\"Splits dataset into training/testing sets.\"\"\"\n",
    "    X = df.drop(columns=[\"market_stress\"])\n",
    "    y = df[\"market_stress\"]\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "2e26ff87-18ab-4cf0-8bea-155315cd3726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… split_data() passed.\n"
     ]
    }
   ],
   "source": [
    "def test_split_data():\n",
    "    df = pd.read_csv(\"data/financial_data_cleaned2.csv\")\n",
    "    df_cleaned = preprocess_data(df)\n",
    "    df_scaled = scale_features(df_cleaned)\n",
    "    df_pca = apply_pca(df_scaled, n_components=50)  # Apply PCA before splitting\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = split_data(df_pca)\n",
    "\n",
    "    # âœ… Check split sizes\n",
    "    assert len(X_train) > len(X_test), \"Train set should be larger than test set\"\n",
    "    assert np.isclose(len(X_test) / len(df_pca), 0.2, atol=0.01), \"Test set is not ~20% of total\"\n",
    "\n",
    "    # âœ… Ensure no data leakage\n",
    "    assert not set(X_train.index).intersection(set(X_test.index)), \"Train and test sets overlap!\"\n",
    "\n",
    "    # âœ… Feature consistency\n",
    "    assert X_train.shape[1] == X_test.shape[1], \"Feature count mismatch between train and test\"\n",
    "\n",
    "    # âœ… Ensure market stress label is preserved\n",
    "    assert len(y_train) > len(y_test), \"Market stress labels should also follow 80-20 split\"\n",
    "\n",
    "    print(\"âœ… split_data() passed.\")\n",
    "\n",
    "# âœ… Run the test\n",
    "test_split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "4cc65e70-c848-4bf2-8b98-23afe57208e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# ðŸ“Œ STEP 5: TRAIN MODELS\n",
    "# -----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "f3cc490d-9568-409c-ac62-65fcf45c7e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score, roc_auc_score, accuracy_score, log_loss\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# âœ… Train Elastic Net Regression with Fitness Score\n",
    "def train_elastic_net(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Trains an Elastic Net model (hybrid of LASSO and Ridge regression) and logs performance metrics including fitness score.\"\"\"\n",
    "\n",
    "    # âœ… Define Elastic Net with Cross-Validation\n",
    "    alpha_values = np.logspace(-2, 0.5, 5)  # Regularization strength\n",
    "    elastic_net = ElasticNetCV(\n",
    "        cv=5, \n",
    "        l1_ratio=[0.3, 0.6],  \n",
    "        alphas=alpha_values,  \n",
    "        random_state=42, \n",
    "        max_iter=5000,  \n",
    "        tol=1e-3,  \n",
    "        selection=\"random\",  \n",
    "        fit_intercept=False,  \n",
    "        n_jobs=-1  \n",
    "    )\n",
    "\n",
    "    # âœ… Train on training data & track time\n",
    "    start_time = time.time()\n",
    "    elastic_net.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # âœ… Predictions on test data & track time\n",
    "    start_time = time.time()\n",
    "    y_pred = elastic_net.predict(X_test)\n",
    "    prediction_time = time.time() - start_time\n",
    "\n",
    "    # âœ… Convert predictions to binary for classification metrics\n",
    "    y_pred_binary = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "    # âœ… Compute performance metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "    logloss = log_loss(y_test, y_pred_binary) if len(set(y_test)) > 1 else None  # Avoid log-loss error for single class\n",
    "    stability = np.mean(cross_val_score(elastic_net, X_train, y_train, cv=5))\n",
    "\n",
    "    # âœ… **Compute Fitness Score**\n",
    "    fitness = (\n",
    "        (r2 + roc_auc + accuracy + stability) / 4  # Maximize performance metrics\n",
    "        * (1 / (1 + mse))  # Minimize MSE\n",
    "        * (1 / (1 + (logloss if logloss is not None else 0)))  # Minimize log-loss\n",
    "        * (1 / (1 + training_time))  # Prefer lower training time\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"Elastic Net Best Alpha\": elastic_net.alpha_,\n",
    "        \"Elastic Net Best L1 Ratio\": elastic_net.l1_ratio_,\n",
    "        \"Mean Squared Error (MSE)\": mse,\n",
    "        \"RÂ² Score\": r2,\n",
    "        \"ROC-AUC Score\": roc_auc,\n",
    "        \"Accuracy Score\": accuracy,\n",
    "        \"Log Loss\": logloss,\n",
    "        \"Training Time (s)\": training_time,\n",
    "        \"Prediction Time (s)\": prediction_time,\n",
    "        \"Cross-Validation Stability\": stability,\n",
    "        \"Fitness Score\": fitness  # âœ… New Fitness Score Added\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "f3cc472f-340a-4571-b4a0-f581cb3ece3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š **Elastic Net Performance Metrics**\n",
      "âœ… Elastic Net Best Alpha: 0.01\n",
      "âœ… Elastic Net Best L1 Ratio: 0.3\n",
      "âœ… Mean Squared Error (MSE): 0.03721750255903229\n",
      "âœ… RÂ² Score: 0.14290032522469087\n",
      "âœ… ROC-AUC Score: 0.952906885142587\n",
      "âœ… Accuracy Score: 0.9545078577336642\n",
      "âœ… Log Loss: 1.6397030077762147\n",
      "âœ… Training Time (s): 0.07199597358703613\n",
      "âœ… Prediction Time (s): 0.0005180835723876953\n",
      "âœ… Cross-Validation Stability: 0.1721632814307658\n",
      "âœ… Fitness Score: 0.18930386871095264\n",
      "\n",
      "âœ… train_elastic_net() passed.\n"
     ]
    }
   ],
   "source": [
    "def test_train_elastic_net():\n",
    "    \"\"\"Test Elastic Net model to ensure it runs and outputs expected values.\"\"\"\n",
    "\n",
    "    # âœ… Load and preprocess data\n",
    "    df = pd.read_csv(\"data/financial_data_cleaned2.csv\")\n",
    "    df_cleaned = preprocess_data(df)\n",
    "    df_scaled = scale_features(df_cleaned)\n",
    "    df_pca = apply_pca(df_scaled, n_components=50)\n",
    "    X_train, X_test, y_train, y_test = split_data(df_pca)\n",
    "    \n",
    "    # âœ… Train model\n",
    "    results = train_elastic_net(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # âœ… Assertions\n",
    "    assert isinstance(results, dict), \"Output should be a dictionary\"\n",
    "    assert \"Mean Squared Error (MSE)\" in results, \"Missing MSE in results\"\n",
    "    assert \"RÂ² Score\" in results, \"Missing RÂ² Score in results\"\n",
    "    assert \"ROC-AUC Score\" in results, \"Missing ROC-AUC Score in results\"\n",
    "    assert \"Accuracy Score\" in results, \"Missing Accuracy Score in results\"\n",
    "    assert \"Training Time (s)\" in results, \"Missing Training Time in results\"\n",
    "    assert \"Prediction Time (s)\" in results, \"Missing Prediction Time in results\"\n",
    "    assert \"Fitness Score\" in results, \"Missing Fitness Score in results\"\n",
    "    \n",
    "    assert results[\"Mean Squared Error (MSE)\"] >= 0, \"MSE should not be negative\"\n",
    "    assert 0 <= results[\"ROC-AUC Score\"] <= 1, \"ROC-AUC should be between 0 and 1\"\n",
    "    assert results[\"Training Time (s)\"] > 0, \"Training time should be greater than 0\"\n",
    "    assert results[\"Prediction Time (s)\"] > 0, \"Prediction time should be greater than 0\"\n",
    "    assert 0 <= results[\"Fitness Score\"] <= 1, \"Fitness should be between 0 and 1\"\n",
    "\n",
    "    print(\"\\nðŸ“Š **Elastic Net Performance Metrics**\")\n",
    "    for metric, value in results.items():\n",
    "        print(f\"âœ… {metric}: {value}\")\n",
    "\n",
    "    print(\"\\nâœ… train_elastic_net() passed.\")\n",
    "\n",
    "# âœ… Run the test\n",
    "test_train_elastic_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf4a35a-a39f-4bca-90c3-0c520766b72f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b71c9df-fc42-4b4f-ae79-b3a48fcf986e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "c36f79b1-4291-4fdd-95d5-c5aabd507c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score, roc_auc_score, accuracy_score, log_loss\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# âœ… Train Stochastic Gradient Descent (SGD)\n",
    "def train_sgd(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Trains an SGD model for regression and logs performance metrics including fitness score.\"\"\"\n",
    "\n",
    "    # âœ… Define SGD Model\n",
    "    sgd = SGDRegressor(\n",
    "        max_iter=2000,\n",
    "        tol=1e-4,\n",
    "        random_state=42,\n",
    "        penalty=\"l2\",  # Ridge-style regularization\n",
    "        alpha=0.01,  # Regularization strength\n",
    "    )\n",
    "\n",
    "    # âœ… Train on training data & track time\n",
    "    start_time = time.time()\n",
    "    sgd.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # âœ… Predictions on test data & track time\n",
    "    start_time = time.time()\n",
    "    y_pred = sgd.predict(X_test)\n",
    "    prediction_time = time.time() - start_time\n",
    "\n",
    "    # âœ… Convert predictions to binary for classification metrics\n",
    "    y_pred_binary = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "    # âœ… Compute performance metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "    logloss = log_loss(y_test, y_pred_binary) if len(set(y_test)) > 1 else None\n",
    "    stability = np.mean(cross_val_score(sgd, X_train, y_train, cv=5))\n",
    "\n",
    "    # âœ… **Compute Fitness Score**\n",
    "    fitness = (\n",
    "        (r2 + roc_auc + accuracy + stability) / 4  # Maximize performance metrics\n",
    "        * (1 / (1 + mse))  # Minimize MSE\n",
    "        * (1 / (1 + (logloss if logloss is not None else 0)))  # Minimize log-loss\n",
    "        * (1 / (1 + training_time))  # Prefer lower training time\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"Mean Squared Error (MSE)\": mse,\n",
    "        \"RÂ² Score\": r2,\n",
    "        \"ROC-AUC Score\": roc_auc,\n",
    "        \"Accuracy Score\": accuracy,\n",
    "        \"Log Loss\": logloss,\n",
    "        \"Training Time (s)\": training_time,\n",
    "        \"Prediction Time (s)\": prediction_time,\n",
    "        \"Cross-Validation Stability\": stability,\n",
    "        \"Fitness Score\": fitness  # âœ… New Fitness Score Added\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "87b8c7a8-fb38-4c0d-a24c-896d0420e410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š **SGD Performance Metrics**\n",
      "âœ… Mean Squared Error (MSE): 0.04289350590933394\n",
      "âœ… RÂ² Score: 0.012184928138315287\n",
      "âœ… ROC-AUC Score: 0.8554435166220261\n",
      "âœ… Accuracy Score: 0.9594706368899917\n",
      "âœ… Log Loss: 1.4608263160188097\n",
      "âœ… Training Time (s): 0.011418819427490234\n",
      "âœ… Prediction Time (s): 0.0005090236663818359\n",
      "âœ… Cross-Validation Stability: 0.2126830095604117\n",
      "âœ… Fitness Score: 0.1964589539324978\n",
      "\n",
      "âœ… train_sgd() passed.\n"
     ]
    }
   ],
   "source": [
    "def test_train_sgd():\n",
    "    \"\"\"Test SGD model to ensure it runs and outputs expected values.\"\"\"\n",
    "\n",
    "    # âœ… Load and preprocess data\n",
    "    df = pd.read_csv(\"data/financial_data_cleaned2.csv\")\n",
    "    df_cleaned = preprocess_data(df)\n",
    "    df_scaled = scale_features(df_cleaned)\n",
    "    df_pca = apply_pca(df_scaled, n_components=50)\n",
    "    X_train, X_test, y_train, y_test = split_data(df_pca)\n",
    "    \n",
    "    # âœ… Train model\n",
    "    results = train_sgd(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # âœ… Assertions\n",
    "    assert isinstance(results, dict), \"Output should be a dictionary\"\n",
    "    assert \"Mean Squared Error (MSE)\" in results, \"Missing MSE in results\"\n",
    "    assert \"RÂ² Score\" in results, \"Missing RÂ² Score in results\"\n",
    "    assert \"ROC-AUC Score\" in results, \"Missing ROC-AUC Score in results\"\n",
    "    assert \"Accuracy Score\" in results, \"Missing Accuracy Score in results\"\n",
    "    assert \"Training Time (s)\" in results, \"Missing Training Time in results\"\n",
    "    assert \"Prediction Time (s)\" in results, \"Missing Prediction Time in results\"\n",
    "    assert \"Fitness Score\" in results, \"Missing Fitness Score in results\"\n",
    "    \n",
    "    assert results[\"Mean Squared Error (MSE)\"] >= 0, \"MSE should not be negative\"\n",
    "    assert 0 <= results[\"ROC-AUC Score\"] <= 1, \"ROC-AUC should be between 0 and 1\"\n",
    "    assert results[\"Training Time (s)\"] > 0, \"Training time should be greater than 0\"\n",
    "    assert results[\"Prediction Time (s)\"] > 0, \"Prediction time should be greater than 0\"\n",
    "    assert 0 <= results[\"Fitness Score\"] <= 1, \"Fitness should be between 0 and 1\"\n",
    "\n",
    "    print(\"\\nðŸ“Š **SGD Performance Metrics**\")\n",
    "    for metric, value in results.items():\n",
    "        print(f\"âœ… {metric}: {value}\")\n",
    "\n",
    "    print(\"\\nâœ… train_sgd() passed.\")\n",
    "\n",
    "# âœ… Run the test\n",
    "test_train_sgd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "21ea7b4a-be6a-40b0-90f2-417128fa6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score, roc_auc_score, accuracy_score, log_loss\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# âœ… Train Gradient Boosting Classifier\n",
    "def train_gradient_boosting(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Trains a Gradient Boosting classifier using decision trees and logs performance metrics including fitness score.\"\"\"\n",
    "\n",
    "    # âœ… Define Gradient Boosting Model\n",
    "    gbc = GradientBoostingClassifier(\n",
    "        n_estimators=100,  # Number of boosting stages\n",
    "        learning_rate=0.1,  # Step size shrinkage\n",
    "        max_depth=3,  # Depth of each tree\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # âœ… Train on training data & track time\n",
    "    start_time = time.time()\n",
    "    gbc.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # âœ… Predictions on test data & track time\n",
    "    start_time = time.time()\n",
    "    y_pred_prob = gbc.predict_proba(X_test)[:, 1]  # Get probability predictions\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)  # Convert to binary predictions\n",
    "    prediction_time = time.time() - start_time\n",
    "\n",
    "    # âœ… Compute performance metrics\n",
    "    mse = mean_squared_error(y_test, y_pred_prob)\n",
    "    r2 = r2_score(y_test, y_pred_prob)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    logloss = log_loss(y_test, y_pred_prob) if len(set(y_test)) > 1 else None\n",
    "    stability = np.mean(cross_val_score(gbc, X_train, y_train, cv=5))\n",
    "\n",
    "    # âœ… **Compute Fitness Score**\n",
    "    fitness = (\n",
    "        (r2 + roc_auc + accuracy + stability) / 4  # Maximize performance metrics\n",
    "        * (1 / (1 + mse))  # Minimize MSE\n",
    "        * (1 / (1 + (logloss if logloss is not None else 0)))  # Minimize log-loss\n",
    "        * (1 / (1 + training_time))  # Prefer lower training time\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"Mean Squared Error (MSE)\": mse,\n",
    "        \"RÂ² Score\": r2,\n",
    "        \"ROC-AUC Score\": roc_auc,\n",
    "        \"Accuracy Score\": accuracy,\n",
    "        \"Log Loss\": logloss,\n",
    "        \"Training Time (s)\": training_time,\n",
    "        \"Prediction Time (s)\": prediction_time,\n",
    "        \"Cross-Validation Stability\": stability,\n",
    "        \"Fitness Score\": fitness  # âœ… New Fitness Score Added\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "bb472d07-e1f8-44b4-a3c8-6226208782c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š **Gradient Boosting Performance Metrics**\n",
      "âœ… Mean Squared Error (MSE): 0.005555996359350104\n",
      "âœ… RÂ² Score: 0.8720483013308458\n",
      "âœ… ROC-AUC Score: 0.9978099889711675\n",
      "âœ… Accuracy Score: 0.9933829611248967\n",
      "âœ… Log Loss: 0.02322537767618015\n",
      "âœ… Training Time (s): 8.11316180229187\n",
      "âœ… Prediction Time (s): 0.0025768280029296875\n",
      "âœ… Cross-Validation Stability: 0.9931775449332092\n",
      "âœ… Fitness Score: 0.10281998957961734\n",
      "\n",
      "âœ… train_gradient_boosting() passed.\n"
     ]
    }
   ],
   "source": [
    "def test_train_gradient_boosting():\n",
    "    \"\"\"Test Gradient Boosting classifier to ensure it runs and outputs expected values.\"\"\"\n",
    "\n",
    "    # âœ… Load and preprocess data\n",
    "    df = pd.read_csv(\"data/financial_data_cleaned2.csv\")\n",
    "    df_cleaned = preprocess_data(df)\n",
    "    df_scaled = scale_features(df_cleaned)\n",
    "    df_pca = apply_pca(df_scaled, n_components=50)\n",
    "    X_train, X_test, y_train, y_test = split_data(df_pca)\n",
    "    \n",
    "    # âœ… Train model\n",
    "    results = train_gradient_boosting(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # âœ… Assertions\n",
    "    assert isinstance(results, dict), \"Output should be a dictionary\"\n",
    "    assert \"Mean Squared Error (MSE)\" in results, \"Missing MSE in results\"\n",
    "    assert \"RÂ² Score\" in results, \"Missing RÂ² Score in results\"\n",
    "    assert \"ROC-AUC Score\" in results, \"Missing ROC-AUC Score in results\"\n",
    "    assert \"Accuracy Score\" in results, \"Missing Accuracy Score in results\"\n",
    "    assert \"Training Time (s)\" in results, \"Missing Training Time in results\"\n",
    "    assert \"Prediction Time (s)\" in results, \"Missing Prediction Time in results\"\n",
    "    assert \"Fitness Score\" in results, \"Missing Fitness Score in results\"\n",
    "    \n",
    "    assert results[\"Mean Squared Error (MSE)\"] >= 0, \"MSE should not be negative\"\n",
    "    assert 0 <= results[\"ROC-AUC Score\"] <= 1, \"ROC-AUC should be between 0 and 1\"\n",
    "    assert results[\"Training Time (s)\"] > 0, \"Training time should be greater than 0\"\n",
    "    assert results[\"Prediction Time (s)\"] > 0, \"Prediction time should be greater than 0\"\n",
    "    assert 0 <= results[\"Fitness Score\"] <= 1, \"Fitness should be between 0 and 1\"\n",
    "\n",
    "    print(\"\\nðŸ“Š **Gradient Boosting Performance Metrics**\")\n",
    "    for metric, value in results.items():\n",
    "        print(f\"âœ… {metric}: {value}\")\n",
    "\n",
    "    print(\"\\nâœ… train_gradient_boosting() passed.\")\n",
    "\n",
    "# âœ… Run the test\n",
    "test_train_gradient_boosting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "fa9a91e5-2012-4b9c-a497-4f9ce1366476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score, roc_auc_score, accuracy_score, log_loss\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# âœ… Train CNN (Deep Learning - MLP Stand-in)\n",
    "def train_cnn(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Trains a Multi-Layer Perceptron (MLP) as a stand-in for a CNN, and evaluates performance.\"\"\"\n",
    "\n",
    "    # âœ… Define MLP Model (acting as CNN Stand-in)\n",
    "    cnn = MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50),  # Two layers: 100 neurons & 50 neurons\n",
    "        activation='relu',  # ReLU activation function\n",
    "        solver='adam',  # Adam optimizer\n",
    "        max_iter=500,  # Max training iterations\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # âœ… Train on training data & track time\n",
    "    start_time = time.time()\n",
    "    cnn.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # âœ… Predictions on test data & track time\n",
    "    start_time = time.time()\n",
    "    y_pred_prob = cnn.predict_proba(X_test)[:, 1]  # Get probability predictions\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)  # Convert to binary predictions\n",
    "    prediction_time = time.time() - start_time\n",
    "\n",
    "    # âœ… Compute performance metrics\n",
    "    mse = mean_squared_error(y_test, y_pred_prob)\n",
    "    r2 = r2_score(y_test, y_pred_prob)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    logloss = log_loss(y_test, y_pred_prob) if len(set(y_test)) > 1 else None\n",
    "    stability = np.mean(cross_val_score(cnn, X_train, y_train, cv=5))\n",
    "\n",
    "    # âœ… **Compute Fitness Score**\n",
    "    fitness = (\n",
    "        (r2 + roc_auc + accuracy + stability) / 4  # Maximize performance metrics\n",
    "        * (1 / (1 + mse))  # Minimize MSE\n",
    "        * (1 / (1 + (logloss if logloss is not None else 0)))  # Minimize log-loss\n",
    "        * (1 / (1 + training_time))  # Prefer lower training time\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"Mean Squared Error (MSE)\": mse,\n",
    "        \"RÂ² Score\": r2,\n",
    "        \"ROC-AUC Score\": roc_auc,\n",
    "        \"Accuracy Score\": accuracy,\n",
    "        \"Log Loss\": logloss,\n",
    "        \"Training Time (s)\": training_time,\n",
    "        \"Prediction Time (s)\": prediction_time,\n",
    "        \"Cross-Validation Stability\": stability,\n",
    "        \"Fitness Score\": fitness  # âœ… New Fitness Score Added\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "02a727d7-42ee-4245-a7a8-75c37858b743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š **CNN (MLP Stand-in) Performance Metrics**\n",
      "âœ… Mean Squared Error (MSE): 0.001688510514653581\n",
      "âœ… RÂ² Score: 0.9611144834162697\n",
      "âœ… ROC-AUC Score: 0.9999527335749173\n",
      "âœ… Accuracy Score: 0.9983457402812241\n",
      "âœ… Log Loss: 0.0060235345803089585\n",
      "âœ… Training Time (s): 5.345175743103027\n",
      "âœ… Prediction Time (s): 0.0013370513916015625\n",
      "âœ… Cross-Validation Stability: 0.9968989034844068\n",
      "âœ… Fitness Score: 0.15468422951139413\n",
      "\n",
      "âœ… train_cnn() passed.\n"
     ]
    }
   ],
   "source": [
    "def test_train_cnn():\n",
    "    \"\"\"Test CNN model (MLP Stand-in) to ensure it runs and outputs expected values.\"\"\"\n",
    "\n",
    "    # âœ… Load and preprocess data\n",
    "    df = pd.read_csv(\"data/financial_data_cleaned2.csv\")\n",
    "    df_cleaned = preprocess_data(df)\n",
    "    df_scaled = scale_features(df_cleaned)\n",
    "    df_pca = apply_pca(df_scaled, n_components=50)\n",
    "    X_train, X_test, y_train, y_test = split_data(df_pca)\n",
    "    \n",
    "    # âœ… Train model\n",
    "    results = train_cnn(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # âœ… Assertions\n",
    "    assert isinstance(results, dict), \"Output should be a dictionary\"\n",
    "    assert \"Mean Squared Error (MSE)\" in results, \"Missing MSE in results\"\n",
    "    assert \"RÂ² Score\" in results, \"Missing RÂ² Score in results\"\n",
    "    assert \"ROC-AUC Score\" in results, \"Missing ROC-AUC Score in results\"\n",
    "    assert \"Accuracy Score\" in results, \"Missing Accuracy Score in results\"\n",
    "    assert \"Training Time (s)\" in results, \"Missing Training Time in results\"\n",
    "    assert \"Prediction Time (s)\" in results, \"Missing Prediction Time in results\"\n",
    "    assert \"Fitness Score\" in results, \"Missing Fitness Score in results\"\n",
    "    \n",
    "    assert results[\"Mean Squared Error (MSE)\"] >= 0, \"MSE should not be negative\"\n",
    "    assert 0 <= results[\"ROC-AUC Score\"] <= 1, \"ROC-AUC should be between 0 and 1\"\n",
    "    assert results[\"Training Time (s)\"] > 0, \"Training time should be greater than 0\"\n",
    "    assert results[\"Prediction Time (s)\"] > 0, \"Prediction time should be greater than 0\"\n",
    "    assert 0 <= results[\"Fitness Score\"] <= 1, \"Fitness should be between 0 and 1\"\n",
    "\n",
    "    print(\"\\nðŸ“Š **CNN (MLP Stand-in) Performance Metrics**\")\n",
    "    for metric, value in results.items():\n",
    "        print(f\"âœ… {metric}: {value}\")\n",
    "\n",
    "    print(\"\\nâœ… train_cnn() passed.\")\n",
    "\n",
    "# âœ… Run the test\n",
    "test_train_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "0309e706-05c7-4004-b4f4-0db1710b8e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score, roc_auc_score, accuracy_score, log_loss\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# âœ… Train Diffusion Model (GMM-based)\n",
    "def train_diffusion_model(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Trains a Gaussian Mixture Model (GMM) as a stand-in for a diffusion model, and evaluates performance.\"\"\"\n",
    "\n",
    "    # âœ… Define GMM Model\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=5,  # Assume 5 mixture components\n",
    "        covariance_type='full',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # âœ… Train on training data & track time\n",
    "    start_time = time.time()\n",
    "    gmm.fit(X_train)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # âœ… Predictions on test data & track time\n",
    "    start_time = time.time()\n",
    "    y_pred_prob = gmm.predict_proba(X_test)[:, 1]  # Get probability predictions\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)  # Convert to binary predictions\n",
    "    prediction_time = time.time() - start_time\n",
    "\n",
    "    # âœ… Compute performance metrics\n",
    "    mse = mean_squared_error(y_test, y_pred_prob)\n",
    "    r2 = r2_score(y_test, y_pred_prob)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    logloss = log_loss(y_test, y_pred_prob) if len(set(y_test)) > 1 else None\n",
    "    stability = np.mean(cross_val_score(gmm, X_train, y_train, cv=5))\n",
    "\n",
    "    # âœ… **Compute Fitness Score**\n",
    "    fitness = (\n",
    "        (r2 + roc_auc + accuracy + stability) / 4  # Maximize performance metrics\n",
    "        * (1 / (1 + mse))  # Minimize MSE\n",
    "        * (1 / (1 + (logloss if logloss is not None else 0)))  # Minimize log-loss\n",
    "        * (1 / (1 + training_time))  # Prefer lower training time\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"Mean Squared Error (MSE)\": mse,\n",
    "        \"RÂ² Score\": r2,\n",
    "        \"ROC-AUC Score\": roc_auc,\n",
    "        \"Accuracy Score\": accuracy,\n",
    "        \"Log Loss\": logloss,\n",
    "        \"Training Time (s)\": training_time,\n",
    "        \"Prediction Time (s)\": prediction_time,\n",
    "        \"Cross-Validation Stability\": stability,\n",
    "        \"Fitness Score\": fitness  # âœ… New Fitness Score Added\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "63cf060a-721f-422f-af2f-193829a9ef52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š **Diffusion Model (GMM) Performance Metrics**\n",
      "âœ… Mean Squared Error (MSE): 0.355665839462139\n",
      "âœ… RÂ² Score: -7.190798800864325\n",
      "âœ… ROC-AUC Score: 0.6379391838663936\n",
      "âœ… Accuracy Score: 0.6443341604631927\n",
      "âœ… Log Loss: 12.796068407887995\n",
      "âœ… Training Time (s): 0.49477672576904297\n",
      "âœ… Prediction Time (s): 0.003550291061401367\n",
      "âœ… Cross-Validation Stability: 11.864125453710738\n",
      "âœ… Fitness Score: 0.05325755331082186\n",
      "\n",
      "âœ… train_diffusion_model() passed.\n"
     ]
    }
   ],
   "source": [
    "def test_train_diffusion_model():\n",
    "    \"\"\"Test GMM-based diffusion model to ensure it runs and outputs expected values.\"\"\"\n",
    "\n",
    "    # âœ… Load and preprocess data\n",
    "    df = pd.read_csv(\"data/financial_data_cleaned2.csv\")\n",
    "    df_cleaned = preprocess_data(df)\n",
    "    df_scaled = scale_features(df_cleaned)\n",
    "    df_pca = apply_pca(df_scaled, n_components=50)\n",
    "    X_train, X_test, y_train, y_test = split_data(df_pca)\n",
    "    \n",
    "    # âœ… Train model\n",
    "    results = train_diffusion_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # âœ… Assertions\n",
    "    assert isinstance(results, dict), \"Output should be a dictionary\"\n",
    "    assert \"Mean Squared Error (MSE)\" in results, \"Missing MSE in results\"\n",
    "    assert \"RÂ² Score\" in results, \"Missing RÂ² Score in results\"\n",
    "    assert \"ROC-AUC Score\" in results, \"Missing ROC-AUC Score in results\"\n",
    "    assert \"Accuracy Score\" in results, \"Missing Accuracy Score in results\"\n",
    "    assert \"Training Time (s)\" in results, \"Missing Training Time in results\"\n",
    "    assert \"Prediction Time (s)\" in results, \"Missing Prediction Time in results\"\n",
    "    assert \"Fitness Score\" in results, \"Missing Fitness Score in results\"\n",
    "    \n",
    "    assert results[\"Mean Squared Error (MSE)\"] >= 0, \"MSE should not be negative\"\n",
    "    assert 0 <= results[\"ROC-AUC Score\"] <= 1, \"ROC-AUC should be between 0 and 1\"\n",
    "    assert results[\"Training Time (s)\"] > 0, \"Training time should be greater than 0\"\n",
    "    assert results[\"Prediction Time (s)\"] > 0, \"Prediction time should be greater than 0\"\n",
    "    assert 0 <= results[\"Fitness Score\"] <= 1, \"Fitness should be between 0 and 1\"\n",
    "\n",
    "    print(\"\\nðŸ“Š **Diffusion Model (GMM) Performance Metrics**\")\n",
    "    for metric, value in results.items():\n",
    "        print(f\"âœ… {metric}: {value}\")\n",
    "\n",
    "    print(\"\\nâœ… train_diffusion_model() passed.\")\n",
    "\n",
    "# âœ… Run the test\n",
    "test_train_diffusion_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "3cf009c2-5e46-4a34-927d-388efff793a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from deap import base, creator, tools, algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score, roc_auc_score, accuracy_score, log_loss\n",
    ")\n",
    "\n",
    "# âœ… Prevent Duplicate Class Definitions in DEAP\n",
    "if \"FitnessMax\" not in creator.__dict__:\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "if \"Individual\" not in creator.__dict__:\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "def train_ga_lr(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Uses a Genetic Algorithm (GA) to optimize feature selection for Logistic Regression.\"\"\"\n",
    "\n",
    "    num_features = X_train.shape[1]  # Number of features in dataset\n",
    "\n",
    "    # âœ… Define GA Structure\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"attr_bool\", random.randint, 0, 1)  # Binary feature selection\n",
    "    toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=num_features)\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "    def evaluate(individual):\n",
    "        \"\"\"Evaluates the fitness of an individual feature selection.\"\"\"\n",
    "        selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "        \n",
    "        if not selected_features:\n",
    "            return (0.0,)  # Prevent empty feature sets\n",
    "        \n",
    "        X_train_selected = X_train.iloc[:, selected_features]\n",
    "        X_test_selected = X_test.iloc[:, selected_features]\n",
    "\n",
    "        model = LogisticRegression(max_iter=2000, solver='liblinear', random_state=42)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        \n",
    "        y_pred_prob = model.predict_proba(X_test_selected)[:, 1]\n",
    "        y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        stability = np.mean(cross_val_score(model, X_train_selected, y_train, cv=5))\n",
    "\n",
    "        # âœ… Fitness Score (Maximize ROC-AUC, Accuracy, and Stability)\n",
    "        fitness = (roc_auc + accuracy + stability) / 3\n",
    "\n",
    "        return (fitness,)\n",
    "\n",
    "    toolbox.register(\"evaluate\", evaluate)\n",
    "    toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "    toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.1)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "    # âœ… Create Initial Population & Run GA\n",
    "    pop = toolbox.population(n=20)  # 20 individuals\n",
    "    start_time = time.time()\n",
    "    algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=10, verbose=False)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # âœ… Select Best Individual\n",
    "    best_individual = tools.selBest(pop, k=1)[0]\n",
    "    selected_features = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "\n",
    "    X_train_selected = X_train.iloc[:, selected_features]\n",
    "    X_test_selected = X_test.iloc[:, selected_features]\n",
    "\n",
    "    # âœ… Final Model with Best Features\n",
    "    final_model = LogisticRegression(max_iter=2000, solver='liblinear', random_state=42)\n",
    "    final_model.fit(X_train_selected, y_train)\n",
    "\n",
    "    start_time = time.time()\n",
    "    y_pred_prob = final_model.predict_proba(X_test_selected)[:, 1]\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "    prediction_time = time.time() - start_time\n",
    "\n",
    "    # âœ… Compute Performance Metrics\n",
    "    mse = mean_squared_error(y_test, y_pred_prob)\n",
    "    r2 = r2_score(y_test, y_pred_prob)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    logloss = log_loss(y_test, y_pred_prob) if len(set(y_test)) > 1 else None\n",
    "    stability = np.mean(cross_val_score(final_model, X_train_selected, y_train, cv=5))\n",
    "\n",
    "    # âœ… Final Fitness Score (Incorporating MSE, Log-Loss, Training Time)\n",
    "    final_fitness = (\n",
    "        ((roc_auc + accuracy + stability) / 3)  # Maximize ROC-AUC, Accuracy, Stability\n",
    "        * (1 / (1 + mse))  # Minimize MSE\n",
    "        * (1 / (1 + (logloss if logloss is not None else 0)))  # Minimize log-loss\n",
    "        * (1 / (1 + training_time))  # Prefer lower training time\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"Selected Features\": selected_features,\n",
    "        \"Number of Features\": len(selected_features),\n",
    "        \"Mean Squared Error (MSE)\": mse,\n",
    "        \"RÂ² Score\": r2,\n",
    "        \"ROC-AUC Score\": roc_auc,\n",
    "        \"Accuracy Score\": accuracy,\n",
    "        \"Log Loss\": logloss,\n",
    "        \"Training Time (s)\": training_time,\n",
    "        \"Prediction Time (s)\": prediction_time,\n",
    "        \"Cross-Validation Stability\": stability,\n",
    "        \"Fitness Score\": final_fitness  # âœ… New Fitness Score Added\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "851456b1-9863-4a25-b8a4-f70262afe555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š **GA-Optimized Logistic Regression Performance Metrics**\n",
      "âœ… Selected Features: [2, 3, 4, 5, 8, 9, 10, 11, 13, 14, 15, 16, 17, 21, 23, 24, 26, 27, 29, 31, 33, 34, 35, 37, 38, 39, 41, 43, 45, 46, 47, 48, 49]\n",
      "âœ… Number of Features: 33\n",
      "âœ… Mean Squared Error (MSE): 0.017823615361075622\n",
      "âœ… RÂ² Score: 0.5895314333607631\n",
      "âœ… ROC-AUC Score: 0.98033716716559\n",
      "âœ… Accuracy Score: 0.9776674937965261\n",
      "âœ… Log Loss: 0.06797137608563414\n",
      "âœ… Training Time (s): 19.817595720291138\n",
      "âœ… Prediction Time (s): 0.0004069805145263672\n",
      "âœ… Cross-Validation Stability: 0.9811836043997367\n",
      "âœ… Fitness Score: 0.04329556825995335\n",
      "\n",
      "âœ… train_ga_lr() passed.\n"
     ]
    }
   ],
   "source": [
    "def test_train_ga_lr():\n",
    "    \"\"\"Test GA-Optimized Logistic Regression to ensure it runs and outputs expected values.\"\"\"\n",
    "\n",
    "    # âœ… Load and preprocess data\n",
    "    df = pd.read_csv(\"data/financial_data_cleaned2.csv\")\n",
    "    df_cleaned = preprocess_data(df)\n",
    "    df_scaled = scale_features(df_cleaned)\n",
    "    df_pca = apply_pca(df_scaled, n_components=50)\n",
    "    X_train, X_test, y_train, y_test = split_data(df_pca)\n",
    "    \n",
    "    # âœ… Train model\n",
    "    results = train_ga_lr(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # âœ… Assertions\n",
    "    assert isinstance(results, dict), \"Output should be a dictionary\"\n",
    "    assert \"Selected Features\" in results, \"Missing Selected Features in results\"\n",
    "    assert \"Number of Features\" in results, \"Missing Number of Features in results\"\n",
    "    assert \"Mean Squared Error (MSE)\" in results, \"Missing MSE in results\"\n",
    "    assert \"RÂ² Score\" in results, \"Missing RÂ² Score in results\"\n",
    "    assert \"ROC-AUC Score\" in results, \"Missing ROC-AUC Score in results\"\n",
    "    assert \"Accuracy Score\" in results, \"Missing Accuracy Score in results\"\n",
    "    assert \"Training Time (s)\" in results, \"Missing Training Time in results\"\n",
    "    assert \"Prediction Time (s)\" in results, \"Missing Prediction Time in results\"\n",
    "    assert \"Fitness Score\" in results, \"Missing Fitness Score in results\"\n",
    "    \n",
    "    assert results[\"Mean Squared Error (MSE)\"] >= 0, \"MSE should not be negative\"\n",
    "    assert 0 <= results[\"ROC-AUC Score\"] <= 1, \"ROC-AUC should be between 0 and 1\"\n",
    "    assert results[\"Training Time (s)\"] > 0, \"Training time should be greater than 0\"\n",
    "    assert results[\"Prediction Time (s)\"] > 0, \"Prediction time should be greater than 0\"\n",
    "    assert 0 <= results[\"Fitness Score\"] <= 1, \"Fitness should be between 0 and 1\"\n",
    "\n",
    "    print(\"\\nðŸ“Š **GA-Optimized Logistic Regression Performance Metrics**\")\n",
    "    for metric, value in results.items():\n",
    "        print(f\"âœ… {metric}: {value}\")\n",
    "\n",
    "    print(\"\\nâœ… train_ga_lr() passed.\")\n",
    "\n",
    "# âœ… Run the test\n",
    "test_train_ga_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "7caaed3d-a755-4d56-be3a-6ba5fd1eca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neat\n",
    "# import time\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score, accuracy_score, log_loss\n",
    "\n",
    "# # -----------------------------------------------\n",
    "# # ðŸ“Œ TRAIN NEUROEVOLUTION (NEAT) - FIXED VERSION\n",
    "# # -----------------------------------------------\n",
    "\n",
    "# def eval_genome(genome, config, X_train, y_train, valid_genomes):\n",
    "#     \"\"\"Evaluates a single NEAT genome, computing fitness based on multiple metrics.\"\"\"\n",
    "    \n",
    "#     # âœ… Build neural network from genome\n",
    "#     net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "\n",
    "#     # âœ… Make predictions (binary classification with soft probabilities)\n",
    "#     y_pred_prob = np.array([net.activate(xi)[0] for xi in X_train.to_numpy()])\n",
    "#     y_pred = y_pred_prob > 0.5  # Convert to binary classification\n",
    "\n",
    "#     # âœ… Compute performance metrics\n",
    "#     mse = mean_squared_error(y_train, y_pred)\n",
    "#     r2 = r2_score(y_train, y_pred)\n",
    "#     auc = roc_auc_score(y_train, y_pred)\n",
    "#     accuracy = accuracy_score(y_train, y_pred)\n",
    "    \n",
    "#     # âœ… Compute Log Loss safely\n",
    "#     log_loss_value = log_loss(y_train, y_pred_prob) if np.all(y_pred_prob > 0) else np.inf\n",
    "\n",
    "#     # âœ… Compute Cross-Validation Stability\n",
    "#     if valid_genomes and len(valid_genomes) >= 5:\n",
    "#         fitness_variance = np.var([g.fitness for g in valid_genomes[-5:]])\n",
    "#     else:\n",
    "#         fitness_variance = 1.0  # Default stability\n",
    "\n",
    "#     cross_validation_stability = 1 / (1 + fitness_variance)  # Lower variance is better\n",
    "\n",
    "#     # âœ… Define final fitness score\n",
    "#     fitness_score = (\n",
    "#         (accuracy * 0.3) + \n",
    "#         (auc * 0.25) + \n",
    "#         (cross_validation_stability * 0.2) -  # Higher stability is better\n",
    "#         (mse * 0.15) -  # Lower MSE is better\n",
    "#         (log_loss_value * 0.1)  # Lower Log Loss is better\n",
    "#     )\n",
    "\n",
    "#     return max(fitness_score, 0), cross_validation_stability  # Return both values\n",
    "\n",
    "\n",
    "# def eval_genomes(genomes, config, X_train, y_train):\n",
    "#     \"\"\"Evaluates all genomes in the current NEAT generation.\"\"\"\n",
    "    \n",
    "#     # âœ… Collect valid genomes before evaluation\n",
    "#     valid_genomes = [g for _, g in genomes if g.fitness is not None]\n",
    "\n",
    "#     for genome_id, genome in genomes:\n",
    "#         genome.fitness, _ = eval_genome(genome, config, X_train, y_train, valid_genomes)\n",
    "\n",
    "\n",
    "# def train_neat(X_train_full, X_test_full, y_train_full, y_test_full):\n",
    "#     \"\"\"Trains NEAT with improved fitness metrics, including Log Loss and Stability.\"\"\"\n",
    "\n",
    "#     # âœ… Load NEAT configuration\n",
    "#     config_path = \"neat_config2.txt\"\n",
    "#     config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n",
    "#                          neat.DefaultSpeciesSet, neat.DefaultStagnation, config_path)\n",
    "\n",
    "#     # âœ… Initialize population\n",
    "#     pop = neat.Population(config)\n",
    "\n",
    "#     # âœ… Train for 10 generations\n",
    "#     start_time = time.time()\n",
    "#     pop.run(lambda genomes, cfg: eval_genomes(genomes, cfg, X_train_full, y_train_full), 10)\n",
    "#     training_time = time.time() - start_time\n",
    "\n",
    "#     # âœ… Ensure valid genomes exist before selecting the best one\n",
    "#     valid_genomes = [g for g in pop.population.values() if g.fitness is not None]\n",
    "#     if not valid_genomes:\n",
    "#         raise ValueError(\"âŒ No valid genomes with assigned fitness scores.\")\n",
    "\n",
    "#     # âœ… Select the best genome\n",
    "#     best_genome = max(valid_genomes, key=lambda g: g.fitness)\n",
    "\n",
    "#     # âœ… Compute cross-validation stability using the last 5 generations\n",
    "#     if len(valid_genomes) >= 5:\n",
    "#         fitness_variance = np.var([g.fitness for g in valid_genomes[-5:]])\n",
    "#     else:\n",
    "#         fitness_variance = 1.0  # Default stability\n",
    "#     cross_validation_stability = 1 / (1 + fitness_variance)\n",
    "\n",
    "#     # âœ… Create a neural network from the best genome\n",
    "#     best_net = neat.nn.FeedForwardNetwork.create(best_genome, config)\n",
    "\n",
    "#     # âœ… Make predictions on the test set\n",
    "#     start_time = time.time()\n",
    "#     y_pred_prob = np.array([best_net.activate(xi)[0] for xi in X_test_full.to_numpy()])\n",
    "#     y_pred = y_pred_prob > 0.5\n",
    "#     prediction_time = time.time() - start_time\n",
    "\n",
    "#     # âœ… Compute performance metrics\n",
    "#     mse = mean_squared_error(y_test_full, y_pred)\n",
    "#     r2 = r2_score(y_test_full, y_pred)\n",
    "#     auc = roc_auc_score(y_test_full, y_pred)\n",
    "#     accuracy = accuracy_score(y_test_full, y_pred)\n",
    "#     log_loss_value = log_loss(y_test_full, y_pred_prob)\n",
    "\n",
    "#     # âœ… Return structured results\n",
    "#     return {\n",
    "#         \"Best Genome\": best_genome,\n",
    "#         \"Mean Squared Error (MSE)\": mse,\n",
    "#         \"RÂ² Score\": r2,\n",
    "#         \"ROC-AUC Score\": auc,\n",
    "#         \"Accuracy Score\": accuracy,\n",
    "#         \"Log Loss\": log_loss_value,\n",
    "#         \"Training Time (s)\": training_time,\n",
    "#         \"Prediction Time (s)\": prediction_time,\n",
    "#         \"Cross-Validation Stability\": cross_validation_stability,\n",
    "#         \"Fitness Score\": best_genome.fitness\n",
    "#     }\n",
    "\n",
    "# import neat\n",
    "# import time\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score, accuracy_score, log_loss\n",
    "\n",
    "# # -----------------------------------------------\n",
    "# # ðŸ“Œ TRAIN NEUROEVOLUTION (NEAT) - FIXED VERSION\n",
    "# # -----------------------------------------------\n",
    "\n",
    "# def eval_genome(genome, config, X_train, y_train, valid_genomes):\n",
    "#     \"\"\"Evaluates a single NEAT genome, computing fitness based on multiple metrics.\"\"\"\n",
    "    \n",
    "#     # âœ… Build neural network from genome\n",
    "#     net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "\n",
    "#     # âœ… Make predictions (binary classification with soft probabilities)\n",
    "#     y_pred_prob = np.array([net.activate(xi)[0] for xi in X_train.to_numpy()])\n",
    "#     y_pred = y_pred_prob > 0.5  # Convert to binary classification\n",
    "\n",
    "#     # âœ… Compute performance metrics\n",
    "#     mse = mean_squared_error(y_train, y_pred)\n",
    "#     r2 = r2_score(y_train, y_pred)\n",
    "#     auc = roc_auc_score(y_train, y_pred)\n",
    "#     accuracy = accuracy_score(y_train, y_pred)\n",
    "    \n",
    "#     # âœ… Compute Log Loss safely\n",
    "#     eps = 1e-9  # Small value to avoid log(0)\n",
    "#     y_pred_prob = np.clip(y_pred_prob, eps, 1 - eps)\n",
    "#     log_loss_value = log_loss(y_train, y_pred_prob)\n",
    "\n",
    "#     # âœ… Compute Cross-Validation Stability\n",
    "#     fitness_values = [g.fitness for g in valid_genomes[-5:]] if len(valid_genomes) >= 5 else [genome.fitness]\n",
    "#     fitness_variance = np.var(fitness_values) if fitness_values else 1.0  # Avoid zero-division\n",
    "#     cross_validation_stability = 1 / (1 + fitness_variance)  # Lower variance is better\n",
    "\n",
    "#     # âœ… Define final fitness score\n",
    "#     fitness_score = (\n",
    "#         (accuracy * 0.3) + \n",
    "#         (auc * 0.25) + \n",
    "#         (cross_validation_stability * 0.2) -  # Higher stability is better\n",
    "#         (mse * 0.15) -  # Lower MSE is better\n",
    "#         (log_loss_value * 0.1)  # Lower Log Loss is better\n",
    "#     )\n",
    "\n",
    "#     return max(fitness_score, 0), cross_validation_stability  # Return both values\n",
    "\n",
    "\n",
    "# def eval_genomes(genomes, config, X_train, y_train):\n",
    "#     \"\"\"Evaluates all genomes in the current NEAT generation.\"\"\"\n",
    "    \n",
    "#     # âœ… Collect valid genomes before evaluation\n",
    "#     valid_genomes = [g for _, g in genomes if g.fitness is not None]\n",
    "\n",
    "#     for genome_id, genome in genomes:\n",
    "#         genome.fitness, _ = eval_genome(genome, config, X_train, y_train, valid_genomes)\n",
    "\n",
    "\n",
    "# def train_neat(X_train_full, X_test_full, y_train_full, y_test_full):\n",
    "#     \"\"\"Trains NEAT with improved fitness metrics, including Log Loss and Stability.\"\"\"\n",
    "\n",
    "#     print(\"\\nðŸš€ **Starting NEAT Training...**\")\n",
    "\n",
    "#     # âœ… Load NEAT configuration\n",
    "#     config_path = \"neat_config2.txt\"\n",
    "#     config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n",
    "#                          neat.DefaultSpeciesSet, neat.DefaultStagnation, config_path)\n",
    "\n",
    "#     # âœ… Initialize population\n",
    "#     pop = neat.Population(config)\n",
    "\n",
    "#     # âœ… Train for 10 generations\n",
    "#     start_time = time.time()\n",
    "#     pop.run(lambda genomes, cfg: eval_genomes(genomes, cfg, X_train_full, y_train_full), 10)\n",
    "#     training_time = time.time() - start_time\n",
    "\n",
    "#     # âœ… Ensure valid genomes exist before selecting the best one\n",
    "#     valid_genomes = [g for g in pop.population.values() if g.fitness is not None]\n",
    "#     if not valid_genomes:\n",
    "#         raise ValueError(\"âŒ No valid genomes with assigned fitness scores.\")\n",
    "\n",
    "#     # âœ… Select the best genome\n",
    "#     best_genome = max(valid_genomes, key=lambda g: g.fitness)\n",
    "\n",
    "#     # âœ… Compute cross-validation stability using the last 5 generations\n",
    "#     fitness_values = [g.fitness for g in valid_genomes[-5:]] if len(valid_genomes) >= 5 else [best_genome.fitness]\n",
    "#     fitness_variance = np.var(fitness_values) if fitness_values else 1.0\n",
    "#     cross_validation_stability = 1 / (1 + fitness_variance)\n",
    "\n",
    "#     # âœ… Create a neural network from the best genome\n",
    "#     best_net = neat.nn.FeedForwardNetwork.create(best_genome, config)\n",
    "\n",
    "#     # âœ… Make predictions on the test set\n",
    "#     start_time = time.time()\n",
    "#     y_pred_prob = np.array([best_net.activate(xi)[0] for xi in X_test_full.to_numpy()])\n",
    "#     y_pred = y_pred_prob > 0.5\n",
    "#     prediction_time = time.time() - start_time\n",
    "\n",
    "#     # âœ… Compute performance metrics\n",
    "#     mse = mean_squared_error(y_test_full, y_pred)\n",
    "#     r2 = r2_score(y_test_full, y_pred)\n",
    "#     auc = roc_auc_score(y_test_full, y_pred)\n",
    "#     accuracy = accuracy_score(y_test_full, y_pred)\n",
    "    \n",
    "#     # âœ… Compute Log Loss safely\n",
    "#     y_pred_prob = np.clip(y_pred_prob, eps, 1 - eps)  # Avoid log(0)\n",
    "#     log_loss_value = log_loss(y_test_full, y_pred_prob)\n",
    "\n",
    "#     # âœ… Print final results for debugging\n",
    "#     print(\"\\nâœ… **NeuroEvolution (NEAT) Final Results**\")\n",
    "#     print(f\"ðŸ† Best Genome: {best_genome}\")\n",
    "#     print(f\"ðŸ“Š Final Fitness Score: {best_genome.fitness:.6f}\")\n",
    "#     print(f\"ðŸ”¥ Accuracy: {accuracy:.4f}, AUC: {auc:.4f}, Log Loss: {log_loss_value:.4f}\")\n",
    "\n",
    "#     # âœ… Return structured results\n",
    "#     return {\n",
    "#         \"Best Genome\": best_genome,\n",
    "#         \"Mean Squared Error (MSE)\": mse,\n",
    "#         \"RÂ² Score\": r2,\n",
    "#         \"ROC-AUC Score\": auc,\n",
    "#         \"Accuracy Score\": accuracy,\n",
    "#         \"Log Loss\": log_loss_value,\n",
    "#         \"Training Time (s)\": training_time,\n",
    "#         \"Prediction Time (s)\": prediction_time,\n",
    "#         \"Cross-Validation Stability\": cross_validation_stability,\n",
    "#         \"Fitness Score\": best_genome.fitness\n",
    "#     }\n",
    "\n",
    "import neat\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score, accuracy_score, log_loss\n",
    "\n",
    "# -----------------------------------------------\n",
    "# ðŸ“Œ TRAIN NEUROEVOLUTION (NEAT) - FIXED VERSION\n",
    "# -----------------------------------------------\n",
    "\n",
    "def eval_genome(genome, config, X_train, y_train, valid_genomes):\n",
    "    \"\"\"Evaluates a single NEAT genome, computing fitness based on multiple metrics.\"\"\"\n",
    "    \n",
    "    # âœ… Build neural network from genome\n",
    "    net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "\n",
    "    # âœ… Make predictions (binary classification with soft probabilities)\n",
    "    y_pred_prob = np.array([net.activate(xi)[0] for xi in X_train.to_numpy()])\n",
    "    y_pred = y_pred_prob > 0.5  # Convert to binary classification\n",
    "\n",
    "    # âœ… Compute performance metrics\n",
    "    mse = mean_squared_error(y_train, y_pred)\n",
    "    r2 = r2_score(y_train, y_pred)\n",
    "    auc = roc_auc_score(y_train, y_pred)\n",
    "    accuracy = accuracy_score(y_train, y_pred)\n",
    "    \n",
    "    # âœ… Compute Log Loss safely\n",
    "    log_loss_value = log_loss(y_train, y_pred_prob) if np.all(y_pred_prob > 0) else np.inf\n",
    "\n",
    "    # âœ… Compute Cross-Validation Stability\n",
    "    valid_fitness_values = [g.fitness for g in valid_genomes[-5:] if g.fitness is not None]  # âœ… Exclude None values\n",
    "\n",
    "    fitness_variance = np.var(valid_fitness_values) if valid_fitness_values else 1.0  # Avoid division by NoneType\n",
    "    cross_validation_stability = 1 / (1 + fitness_variance)  # Lower variance is better\n",
    "\n",
    "    # âœ… Define final fitness score\n",
    "    fitness_score = (\n",
    "        (accuracy * 0.3) + \n",
    "        (auc * 0.25) + \n",
    "        (cross_validation_stability * 0.2) -  # Higher stability is better\n",
    "        (mse * 0.15) -  # Lower MSE is better\n",
    "        (log_loss_value * 0.1)  # Lower Log Loss is better\n",
    "    )\n",
    "\n",
    "    return max(fitness_score, 0), cross_validation_stability  # Return both values\n",
    "\n",
    "\n",
    "def eval_genomes(genomes, config, X_train, y_train):\n",
    "    \"\"\"Evaluates all genomes in the current NEAT generation.\"\"\"\n",
    "    \n",
    "    # âœ… Collect valid genomes before evaluation\n",
    "    valid_genomes = [g for _, g in genomes if g.fitness is not None]\n",
    "\n",
    "    for genome_id, genome in genomes:\n",
    "        genome.fitness, _ = eval_genome(genome, config, X_train, y_train, valid_genomes)\n",
    "\n",
    "\n",
    "def train_neat(X_train_full, X_test_full, y_train_full, y_test_full):\n",
    "    \"\"\"Trains NEAT with improved fitness metrics, including Log Loss and Stability.\"\"\"\n",
    "\n",
    "    # âœ… Load NEAT configuration\n",
    "    config_path = \"neat_config2.txt\"\n",
    "    config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n",
    "                         neat.DefaultSpeciesSet, neat.DefaultStagnation, config_path)\n",
    "\n",
    "    # âœ… Initialize population\n",
    "    pop = neat.Population(config)\n",
    "\n",
    "    # âœ… Train for 10 generations\n",
    "    start_time = time.time()\n",
    "    pop.run(lambda genomes, cfg: eval_genomes(genomes, cfg, X_train_full, y_train_full), 10)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # âœ… Ensure valid genomes exist before selecting the best one\n",
    "    valid_genomes = [g for g in pop.population.values() if g.fitness is not None]\n",
    "\n",
    "    if not valid_genomes:\n",
    "        raise ValueError(\"âŒ No valid genomes with assigned fitness scores.\")\n",
    "\n",
    "    # âœ… Select the best genome safely\n",
    "    best_genome = max(valid_genomes, key=lambda g: g.fitness)\n",
    "\n",
    "    # âœ… Compute cross-validation stability using only non-None fitness scores\n",
    "    valid_fitness_values = [g.fitness for g in valid_genomes[-5:] if g.fitness is not None]  # âœ… Exclude None\n",
    "    fitness_variance = np.var(valid_fitness_values) if valid_fitness_values else 1.0  # Default stability\n",
    "    cross_validation_stability = 1 / (1 + fitness_variance)\n",
    "\n",
    "    # âœ… Create a neural network from the best genome\n",
    "    best_net = neat.nn.FeedForwardNetwork.create(best_genome, config)\n",
    "\n",
    "    # âœ… Make predictions on the test set\n",
    "    start_time = time.time()\n",
    "    y_pred_prob = np.array([best_net.activate(xi)[0] for xi in X_test_full.to_numpy()])\n",
    "    y_pred = y_pred_prob > 0.5\n",
    "    prediction_time = time.time() - start_time\n",
    "\n",
    "    # âœ… Compute performance metrics\n",
    "    mse = mean_squared_error(y_test_full, y_pred)\n",
    "    r2 = r2_score(y_test_full, y_pred)\n",
    "    auc = roc_auc_score(y_test_full, y_pred)\n",
    "    accuracy = accuracy_score(y_test_full, y_pred)\n",
    "    log_loss_value = log_loss(y_test_full, y_pred_prob)\n",
    "\n",
    "    # âœ… Return structured results\n",
    "    return {\n",
    "        \"Best Genome\": best_genome,\n",
    "        \"Mean Squared Error (MSE)\": mse,\n",
    "        \"RÂ² Score\": r2,\n",
    "        \"ROC-AUC Score\": auc,\n",
    "        \"Accuracy Score\": accuracy,\n",
    "        \"Log Loss\": log_loss_value,\n",
    "        \"Training Time (s)\": training_time,\n",
    "        \"Prediction Time (s)\": prediction_time,\n",
    "        \"Cross-Validation Stability\": cross_validation_stability,\n",
    "        \"Fitness Score\": best_genome.fitness\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "cb96f03d-25c7-464d-ab23-15d0b91322f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ **Starting NEAT Test**...\n",
      "\n",
      "\n",
      "ðŸš€ **Starting NEAT Training...**\n",
      "\n",
      "ðŸ“Š **NeuroEvolution (NEAT) Performance Metrics**\n",
      "âœ… Best Genome: Key: 414\n",
      "Fitness: 0.5880177527526033\n",
      "Nodes:\n",
      "\t0 DefaultNodeGene(key=0, bias=-0.5776918920592681, response=0.5276381688270025, activation=sigmoid, aggregation=sum)\n",
      "\t1 DefaultNodeGene(key=1, bias=0.8039615107631196, response=-0.29211027312208937, activation=sigmoid, aggregation=mean)\n",
      "\t2 DefaultNodeGene(key=2, bias=-0.6902362913103269, response=2.1656568536310328, activation=relu, aggregation=mean)\n",
      "\t4 DefaultNodeGene(key=4, bias=0.32801531665001216, response=1.8031722934659962, activation=sigmoid, aggregation=max)\n",
      "\t5 DefaultNodeGene(key=5, bias=-1.3641366180017742, response=1.696873044818238, activation=sigmoid, aggregation=mean)\n",
      "\t6 DefaultNodeGene(key=6, bias=0.8909847146753936, response=0.6945302674108731, activation=relu, aggregation=min)\n",
      "\t7 DefaultNodeGene(key=7, bias=-2.5420641492334273, response=1.4577248976764163, activation=sigmoid, aggregation=min)\n",
      "\t8 DefaultNodeGene(key=8, bias=1.917226874899577, response=0.18946173544437445, activation=tanh, aggregation=sum)\n",
      "\t9 DefaultNodeGene(key=9, bias=-2.0205971268993346, response=1.54625225396541, activation=sigmoid, aggregation=mean)\n",
      "\t10 DefaultNodeGene(key=10, bias=-0.9548265996434674, response=1.3177172371526964, activation=sigmoid, aggregation=mean)\n",
      "\t510 DefaultNodeGene(key=510, bias=-1.4280676928138416, response=1.0366085847757889, activation=sigmoid, aggregation=mean)\n",
      "\t609 DefaultNodeGene(key=609, bias=-0.17661909124179742, response=0.9225235334893794, activation=sigmoid, aggregation=mean)\n",
      "\t641 DefaultNodeGene(key=641, bias=-0.5816274601794785, response=1.0060944491428827, activation=sigmoid, aggregation=mean)\n",
      "\t649 DefaultNodeGene(key=649, bias=-0.0144645411350647, response=0.07760556092805149, activation=sigmoid, aggregation=mean)\n",
      "Connections:\n",
      "\tDefaultConnectionGene(key=(-127, 510), weight=1.0, enabled=True)\n",
      "\tDefaultConnectionGene(key=(-101, 2), weight=-0.6227706608723417, enabled=False)\n",
      "\tDefaultConnectionGene(key=(-101, 641), weight=-1.4801352745575984, enabled=False)\n",
      "\tDefaultConnectionGene(key=(-101, 649), weight=1.0, enabled=True)\n",
      "\tDefaultConnectionGene(key=(-14, 6), weight=0.23108999706831923, enabled=False)\n",
      "\tDefaultConnectionGene(key=(510, 0), weight=-0.5338475328984577, enabled=True)\n",
      "\tDefaultConnectionGene(key=(609, 2), weight=-0.879325949236431, enabled=True)\n",
      "\tDefaultConnectionGene(key=(641, 2), weight=-0.44588756653598016, enabled=True)\n",
      "\tDefaultConnectionGene(key=(649, 641), weight=1.0887274107760543, enabled=True)\n",
      "âœ… Mean Squared Error (MSE): 0.045492142266335814\n",
      "âœ… RÂ² Score: -0.047660311958405366\n",
      "âœ… ROC-AUC Score: 0.5\n",
      "âœ… Accuracy Score: 0.9545078577336642\n",
      "âœ… Log Loss: 0.18017319994800493\n",
      "âœ… Training Time (s): 31.253654956817627\n",
      "âœ… Prediction Time (s): 0.01751565933227539\n",
      "âœ… Cross-Validation Stability: 0.9999999573547436\n",
      "âœ… Fitness Score: 0.5880177527526033\n",
      "\n",
      "âœ… train_neat() passed.\n"
     ]
    }
   ],
   "source": [
    "# # -----------------------------------------------\n",
    "# # ðŸ“Œ TEST NEUROEVOLUTION (NEAT)\n",
    "# # -----------------------------------------------\n",
    "\n",
    "# def test_train_neat():\n",
    "#     \"\"\"\n",
    "#     Test NeuroEvolution (NEAT) model to ensure:\n",
    "#     - It runs without errors.\n",
    "#     - It outputs the correct data format (dictionary).\n",
    "#     - It contains all expected performance metrics.\n",
    "#     - All numerical values fall within expected ranges.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # âœ… Load and preprocess dataset\n",
    "#     df = pd.read_csv(\"data/financial_data_cleaned2.csv\")\n",
    "#     df_cleaned = preprocess_data(df)\n",
    "#     df_scaled = scale_features(df_cleaned)\n",
    "\n",
    "#     # âœ… Use full dataset for NEAT (no PCA reduction)\n",
    "#     X_train_full, X_test_full, y_train_full, y_test_full = split_data(df_scaled)\n",
    "\n",
    "#     # âœ… Train NEAT model\n",
    "#     results = train_neat(X_train_full, X_test_full, y_train_full, y_test_full)\n",
    "\n",
    "#     # âœ… Ensure output is a dictionary\n",
    "#     assert isinstance(results, dict), \"Output should be a dictionary\"\n",
    "\n",
    "#     # âœ… Ensure all expected metrics are included\n",
    "#     expected_keys = [\n",
    "#         \"Best Genome\", \"Mean Squared Error (MSE)\", \"RÂ² Score\", \"ROC-AUC Score\",\n",
    "#         \"Accuracy Score\", \"Log Loss\", \"Training Time (s)\", \"Prediction Time (s)\", \n",
    "#         \"Cross-Validation Stability\", \"Fitness Score\"\n",
    "#     ]\n",
    "#     for key in expected_keys:\n",
    "#         assert key in results, f\"Missing {key} in results\"\n",
    "\n",
    "#     # âœ… Validate numerical ranges\n",
    "#     assert results[\"Mean Squared Error (MSE)\"] >= 0, \"MSE should not be negative\"\n",
    "#     assert 0 <= results[\"ROC-AUC Score\"] <= 1, \"ROC-AUC should be between 0 and 1\"\n",
    "#     assert results[\"Training Time (s)\"] > 0, \"Training time should be greater than 0\"\n",
    "#     assert results[\"Prediction Time (s)\"] > 0, \"Prediction time should be greater than 0\"\n",
    "#     assert 0 <= results[\"Fitness Score\"] <= 1, \"Fitness should be between 0 and 1\"\n",
    "#     assert 0 <= results[\"Cross-Validation Stability\"] <= 1, \"Stability should be between 0 and 1\"\n",
    "#     assert results[\"Log Loss\"] >= 0, \"Log Loss should not be negative\"\n",
    "\n",
    "#     # âœ… Display results\n",
    "#     print(\"\\nðŸ“Š **NeuroEvolution (NEAT) Performance Metrics**\")\n",
    "#     for metric, value in results.items():\n",
    "#         print(f\"âœ… {metric}: {value}\")\n",
    "\n",
    "#     print(\"\\nâœ… train_neat() passed.\")\n",
    "\n",
    "# # âœ… Run the test\n",
    "# test_train_neat()\n",
    "\n",
    "# -----------------------------------------------\n",
    "# ðŸ“Œ TEST NEUROEVOLUTION (NEAT) - FIXED VERSION\n",
    "# -----------------------------------------------\n",
    "\n",
    "import traceback  # For detailed error reporting\n",
    "\n",
    "def test_train_neat():\n",
    "    \"\"\"\n",
    "    Test NeuroEvolution (NEAT) model to ensure:\n",
    "    - It runs without errors.\n",
    "    - It outputs the correct data format (dictionary).\n",
    "    - It contains all expected performance metrics.\n",
    "    - All numerical values fall within expected ranges.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nðŸš€ **Starting NEAT Test**...\\n\")\n",
    "\n",
    "    try:\n",
    "        # âœ… Load and preprocess dataset\n",
    "        df = pd.read_csv(\"data/financial_data_cleaned2.csv\")\n",
    "        df_cleaned = preprocess_data(df)\n",
    "        df_scaled = scale_features(df_cleaned)\n",
    "\n",
    "        # âœ… Use full dataset for NEAT (no PCA reduction)\n",
    "        X_train_full, X_test_full, y_train_full, y_test_full = split_data(df_scaled)\n",
    "\n",
    "        print(\"\\nðŸš€ **Starting NEAT Training...**\")\n",
    "        # âœ… Train NEAT model\n",
    "        results = train_neat(X_train_full, X_test_full, y_train_full, y_test_full)\n",
    "\n",
    "        # âœ… Ensure output is a dictionary\n",
    "        assert isinstance(results, dict), \"âŒ Output should be a dictionary\"\n",
    "\n",
    "        # âœ… Ensure all expected metrics are included\n",
    "        expected_keys = [\n",
    "            \"Best Genome\", \"Mean Squared Error (MSE)\", \"RÂ² Score\", \"ROC-AUC Score\",\n",
    "            \"Accuracy Score\", \"Log Loss\", \"Training Time (s)\", \"Prediction Time (s)\", \n",
    "            \"Cross-Validation Stability\", \"Fitness Score\"\n",
    "        ]\n",
    "        for key in expected_keys:\n",
    "            assert key in results, f\"âŒ Missing {key} in results\"\n",
    "\n",
    "        # âœ… Ensure all values are numeric and not None\n",
    "        for key in expected_keys[1:]:  # Skip \"Best Genome\" since it's not numeric\n",
    "            assert results[key] is not None, f\"âŒ {key} is None!\"\n",
    "            assert isinstance(results[key], (int, float)), f\"âŒ {key} is not a number!\"\n",
    "\n",
    "        # âœ… Validate numerical ranges\n",
    "        assert results[\"Mean Squared Error (MSE)\"] >= 0, \"âŒ MSE should not be negative\"\n",
    "        assert 0 <= results[\"ROC-AUC Score\"] <= 1, \"âŒ ROC-AUC should be between 0 and 1\"\n",
    "        assert results[\"Training Time (s)\"] > 0, \"âŒ Training time should be greater than 0\"\n",
    "        assert results[\"Prediction Time (s)\"] > 0, \"âŒ Prediction time should be greater than 0\"\n",
    "        assert 0 <= results[\"Fitness Score\"] <= 1, \"âŒ Fitness should be between 0 and 1\"\n",
    "        assert 0 <= results[\"Cross-Validation Stability\"] <= 1, \"âŒ Stability should be between 0 and 1\"\n",
    "        assert results[\"Log Loss\"] >= 0, \"âŒ Log Loss should not be negative\"\n",
    "\n",
    "        # âœ… Display results\n",
    "        print(\"\\nðŸ“Š **NeuroEvolution (NEAT) Performance Metrics**\")\n",
    "        for metric, value in results.items():\n",
    "            print(f\"âœ… {metric}: {value}\")\n",
    "\n",
    "        print(\"\\nâœ… train_neat() passed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\nâŒ **Error in test_train_neat()**\")\n",
    "        traceback.print_exc()  # Print full error traceback for debugging\n",
    "        raise  # Reraise for visibility\n",
    "\n",
    "# âœ… Run the test\n",
    "test_train_neat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "0a9eadc8-1973-4274-9d51-d82e6ef07f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# ðŸ“Œ STEP 6: TRAIN ALL MODELS\n",
    "# -----------------------------------------------\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "SAVE_DIR = \"saved_models\"  # Ensure this directory exists\n",
    "\n",
    "def save_model(model_name, model_data):\n",
    "    \"\"\"Saves a trained model and its performance metrics.\"\"\"\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)  # Ensure directory exists\n",
    "    model_path = os.path.join(SAVE_DIR, f\"{model_name}.pkl\")\n",
    "\n",
    "    with open(model_path, \"wb\") as file:\n",
    "        pickle.dump(model_data, file)\n",
    "\n",
    "def load_model(model_name):\n",
    "    \"\"\"Loads a saved model from disk.\"\"\"\n",
    "    model_path = os.path.join(SAVE_DIR, f\"{model_name}.pkl\")\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"âš ï¸ Warning: {model_name} not found at {model_path}\")\n",
    "        return None\n",
    "\n",
    "    with open(model_path, \"rb\") as file:\n",
    "        model_data = pickle.load(file)\n",
    "\n",
    "    return model_data\n",
    "\n",
    "def train_all_models(X_train, X_test, y_train, y_test, X_train_full, X_test_full, y_train_full, y_test_full):\n",
    "    \"\"\"Trains all models, saves them for future use, and allows reloading.\"\"\"\n",
    "    \n",
    "    models = {\n",
    "        \"Elastic Net\": train_elastic_net,\n",
    "        \"SGD\": train_sgd,\n",
    "        \"Gradient Boosting\": train_gradient_boosting,\n",
    "        \"CNN (MLP)\": train_cnn,\n",
    "        \"Diffusion Model\": train_diffusion_model,\n",
    "        \"GA-Optimized LR\": train_ga_lr,\n",
    "        \"NeuroEvolution (NEAT)\": lambda *_: train_neat(X_train_full, X_test_full, y_train_full, y_test_full)  # NEAT uses full dataset\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nðŸš€ Training {name}...\")\n",
    "\n",
    "        # âœ… Check if a saved model exists before training\n",
    "        existing_model = load_model(name)\n",
    "        if existing_model:\n",
    "            print(f\"ðŸ”„ {name} already exists. Loading saved model...\")\n",
    "            results[name] = existing_model\n",
    "            continue  # Skip training if the model is already saved\n",
    "\n",
    "        try:\n",
    "            model_results = model(X_train, X_test, y_train, y_test)\n",
    "            results[name] = model_results\n",
    "            save_model(name, model_results)  # âœ… Save model results\n",
    "\n",
    "        except NotImplementedError:\n",
    "            results[name] = \"âš ï¸ Not Implemented\"\n",
    "            print(f\"âš ï¸ {name} is not implemented yet.\")\n",
    "        except Exception as e:\n",
    "            results[name] = f\"âŒ Error: {str(e)}\"\n",
    "            print(f\"âŒ Error while training {name}: {str(e)}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "c9f89f91-3ed9-4afd-b313-7341f86f2791",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Running `test_and_evaluate_all_models()`...\n",
      "âœ… Data preprocessing and variance check passed.\n",
      "\n",
      "ðŸš€ Training Elastic Net...\n",
      "ðŸ”„ Elastic Net already exists. Loading saved model...\n",
      "\n",
      "ðŸš€ Training SGD...\n",
      "ðŸ”„ SGD already exists. Loading saved model...\n",
      "\n",
      "ðŸš€ Training Gradient Boosting...\n",
      "ðŸ”„ Gradient Boosting already exists. Loading saved model...\n",
      "\n",
      "ðŸš€ Training CNN (MLP)...\n",
      "ðŸ”„ CNN (MLP) already exists. Loading saved model...\n",
      "\n",
      "ðŸš€ Training Diffusion Model...\n",
      "ðŸ”„ Diffusion Model already exists. Loading saved model...\n",
      "\n",
      "ðŸš€ Training GA-Optimized LR...\n",
      "ðŸ”„ GA-Optimized LR already exists. Loading saved model...\n",
      "\n",
      "ðŸš€ Training NeuroEvolution (NEAT)...\n",
      "ðŸ”„ NeuroEvolution (NEAT) already exists. Loading saved model...\n",
      "âœ… Model training completed.\n",
      "âœ… Model results structure check passed.\n",
      "âš ï¸ Warning: Diffusion Model has an invalid stability score (11.864125453710738). Resetting to 0.5.\n",
      "âš ï¸ Warning: NeuroEvolution (NEAT) has an invalid stability score (nan). Resetting to 0.5.\n",
      "âš ï¸ Warning: NeuroEvolution (NEAT) has an invalid Log Loss (nan). Resetting to 10.0.\n",
      "âœ… Model metric validation passed.\n",
      "âœ… Model saving and loading check passed.\n",
      "\n",
      "ðŸ“Š Model performance results **successfully updated and saved** âœ…\n",
      "ðŸ“‚ Models saved in: saved_models\n",
      "ðŸ“Š Model performance results saved in: saved_models/model_performance.csv\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # âœ… Define SAVE_DIR if not already defined\n",
    "# SAVE_DIR = \"saved_models\"\n",
    "\n",
    "# # âœ… Ensure SAVE_DIR exists\n",
    "# if not os.path.exists(SAVE_DIR):\n",
    "#     os.makedirs(SAVE_DIR)\n",
    "\n",
    "# # -----------------------------------------------\n",
    "# # ðŸ“Œ ASSERTIONS & FULL MODEL EVALUATION\n",
    "# # -----------------------------------------------\n",
    "\n",
    "# def test_and_evaluate_all_models():\n",
    "#     \"\"\"Tests all models to ensure training, evaluation, and saving work correctly.\"\"\"\n",
    "#     print(\"\\nðŸš€ Running `test_and_evaluate_all_models()`...\")\n",
    "\n",
    "#     # âœ… Load dataset\n",
    "#     df = pd.read_csv(\"data/financial_data_cleaned2.csv\")\n",
    "\n",
    "#     # âœ… Preprocess, scale, and split data\n",
    "#     df_cleaned = preprocess_data(df)\n",
    "#     df_scaled = scale_features(df_cleaned)\n",
    "#     df_pca = apply_pca(df_scaled, n_components=50)\n",
    "\n",
    "#     # âœ… Train-Test Splits\n",
    "#     X_train, X_test, y_train, y_test = split_data(df_pca)\n",
    "#     X_train_full, X_test_full, y_train_full, y_test_full = split_data(df_scaled)\n",
    "\n",
    "#     # âœ… Ensure dataset has variance (to avoid division errors)\n",
    "#     assert np.any(np.var(X_train, axis=0) > 1e-6), \"âŒ X_train has near-zero variance in some features!\"\n",
    "#     assert np.any(np.var(X_test, axis=0) > 1e-6), \"âŒ X_test has near-zero variance in some features!\"\n",
    "\n",
    "#     print(\"âœ… Data preprocessing and variance check passed.\")\n",
    "\n",
    "#     # âœ… Train all models\n",
    "#     model_results = train_all_models(\n",
    "#         X_train, X_test, y_train, y_test,\n",
    "#         X_train_full, X_test_full, y_train_full, y_test_full\n",
    "#     )\n",
    "\n",
    "#     print(\"âœ… Model training completed.\")\n",
    "\n",
    "#     # âœ… Ensure results are in correct format\n",
    "#     assert isinstance(model_results, dict), \"âŒ Model results should be a dictionary!\"\n",
    "#     assert len(model_results) > 0, \"âŒ Model results should not be empty!\"\n",
    "\n",
    "#     print(\"âœ… Model results structure check passed.\")\n",
    "\n",
    "#     # âœ… Required metric columns in model results\n",
    "#     required_metrics = [\n",
    "#         \"Mean Squared Error (MSE)\", \"RÂ² Score\", \"ROC-AUC Score\", \n",
    "#         \"Accuracy Score\", \"Training Time (s)\", \"Prediction Time (s)\", \"Fitness Score\"\n",
    "#     ]\n",
    "    \n",
    "#     for model_name, metrics in model_results.items():\n",
    "#         assert isinstance(metrics, dict), f\"âŒ Metrics for {model_name} should be a dictionary!\"\n",
    "#         assert all(metric in metrics for metric in required_metrics), f\"âŒ Missing metrics in {model_name}!\"\n",
    "        \n",
    "#         # âœ… Check metric ranges\n",
    "#         assert metrics[\"Mean Squared Error (MSE)\"] >= 0, f\"âŒ {model_name}: MSE should be non-negative!\"\n",
    "#         assert 0 <= metrics[\"ROC-AUC Score\"] <= 1, f\"âŒ {model_name}: ROC-AUC should be between 0 and 1!\"\n",
    "#         assert metrics[\"Training Time (s)\"] > 0, f\"âŒ {model_name}: Training time should be positive!\"\n",
    "#         assert metrics[\"Prediction Time (s)\"] > 0, f\"âŒ {model_name}: Prediction time should be positive!\"\n",
    "#         assert metrics[\"Fitness Score\"] >= 0, f\"âŒ {model_name}: Fitness score should be non-negative!\"\n",
    "\n",
    "#     print(\"âœ… Model metric validation passed.\")\n",
    "\n",
    "#     # âœ… Ensure models are saved correctly\n",
    "#     for model_name in model_results.keys():\n",
    "#         model_path = os.path.join(SAVE_DIR, f\"{model_name}.pkl\")\n",
    "#         assert os.path.exists(model_path), f\"âŒ Model {model_name} should be saved in {SAVE_DIR}!\"\n",
    "\n",
    "#         # âœ… Reload the saved model and compare fitness scores\n",
    "#         try:\n",
    "#             saved_model = load_model(model_name)\n",
    "#             assert saved_model is not None, f\"âŒ Saved model {model_name} should be reloadable!\"\n",
    "#             assert np.isclose(saved_model[\"Fitness Score\"], model_results[model_name][\"Fitness Score\"], atol=1e-6), \\\n",
    "#                 f\"âŒ Fitness score mismatch for {model_name}!\"\n",
    "#         except Exception as e:\n",
    "#             raise AssertionError(f\"âŒ Error loading model {model_name}: {e}\")\n",
    "\n",
    "#     print(\"âœ… Model saving and loading check passed.\")\n",
    "\n",
    "#     # âœ… Ensure performance results file exists\n",
    "#     results_path = os.path.join(SAVE_DIR, \"model_performance.csv\")\n",
    "    \n",
    "#     if not os.path.exists(results_path):\n",
    "#         print(\"âŒ Model performance results CSV not found! Attempting to save it again...\")\n",
    "#         df_model_performance = pd.DataFrame.from_dict(model_results, orient=\"index\")\n",
    "#         df_model_performance.to_csv(results_path, index=True)\n",
    "#         print(\"âœ… Model performance results successfully re-saved!\")\n",
    "\n",
    "#     assert os.path.exists(results_path), \"âŒ Model performance results should be saved to CSV!\"\n",
    "\n",
    "#     print(\"\\nðŸŽ¯ ALL TESTS PASSED: `test_and_evaluate_all_models()` âœ…\")\n",
    "#     print(\"ðŸ“‚ Models saved in:\", SAVE_DIR)\n",
    "#     print(\"ðŸ“Š Model performance results saved in:\", results_path)\n",
    "\n",
    "# # âœ… Run the test\n",
    "# test_and_evaluate_all_models()\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# âœ… Define SAVE_DIR if not already defined\n",
    "SAVE_DIR = \"saved_models\"\n",
    "\n",
    "# âœ… Ensure SAVE_DIR exists\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# ðŸ“Œ ASSERTIONS & FULL MODEL EVALUATION - FIXED\n",
    "# -----------------------------------------------\n",
    "\n",
    "def test_and_evaluate_all_models():\n",
    "    \"\"\"Tests all models to ensure training, evaluation, and saving work correctly.\"\"\"\n",
    "    print(\"\\nðŸš€ Running `test_and_evaluate_all_models()`...\")\n",
    "\n",
    "    # âœ… Load dataset\n",
    "    df = pd.read_csv(\"data/financial_data_cleaned2.csv\")\n",
    "\n",
    "    # âœ… Preprocess, scale, and split data\n",
    "    df_cleaned = preprocess_data(df)\n",
    "    df_scaled = scale_features(df_cleaned)\n",
    "    df_pca = apply_pca(df_scaled, n_components=50)\n",
    "\n",
    "    # âœ… Train-Test Splits\n",
    "    X_train, X_test, y_train, y_test = split_data(df_pca)\n",
    "    X_train_full, X_test_full, y_train_full, y_test_full = split_data(df_scaled)\n",
    "\n",
    "    # âœ… Ensure dataset has variance (to avoid division errors)\n",
    "    assert np.any(np.var(X_train, axis=0) > 1e-6), \"âŒ X_train has near-zero variance in some features!\"\n",
    "    assert np.any(np.var(X_test, axis=0) > 1e-6), \"âŒ X_test has near-zero variance in some features!\"\n",
    "\n",
    "    print(\"âœ… Data preprocessing and variance check passed.\")\n",
    "\n",
    "    # âœ… Train all models\n",
    "    model_results = train_all_models(\n",
    "        X_train, X_test, y_train, y_test,\n",
    "        X_train_full, X_test_full, y_train_full, y_test_full\n",
    "    )\n",
    "\n",
    "    print(\"âœ… Model training completed.\")\n",
    "\n",
    "    # âœ… Ensure results are in correct format\n",
    "    assert isinstance(model_results, dict), \"âŒ Model results should be a dictionary!\"\n",
    "    assert len(model_results) > 0, \"âŒ Model results should not be empty!\"\n",
    "\n",
    "    print(\"âœ… Model results structure check passed.\")\n",
    "\n",
    "    # âœ… Required metric columns in model results\n",
    "    required_metrics = [\n",
    "        \"Mean Squared Error (MSE)\", \"RÂ² Score\", \"ROC-AUC Score\", \n",
    "        \"Accuracy Score\", \"Log Loss\", \"Training Time (s)\", \n",
    "        \"Prediction Time (s)\", \"Cross-Validation Stability\", \"Fitness Score\"\n",
    "    ]\n",
    "    \n",
    "    for model_name, metrics in model_results.items():\n",
    "        assert isinstance(metrics, dict), f\"âŒ Metrics for {model_name} should be a dictionary!\"\n",
    "\n",
    "        # âœ… Fill missing columns with NaN to avoid errors\n",
    "        for metric in required_metrics:\n",
    "            if metric not in metrics:\n",
    "                metrics[metric] = np.nan  # Default to NaN if missing\n",
    "\n",
    "        # âœ… Check metric ranges\n",
    "        assert metrics[\"Mean Squared Error (MSE)\"] >= 0, f\"âŒ {model_name}: MSE should be non-negative!\"\n",
    "        assert 0 <= metrics[\"ROC-AUC Score\"] <= 1, f\"âŒ {model_name}: ROC-AUC should be between 0 and 1!\"\n",
    "        assert metrics[\"Training Time (s)\"] > 0, f\"âŒ {model_name}: Training time should be positive!\"\n",
    "        assert metrics[\"Prediction Time (s)\"] > 0, f\"âŒ {model_name}: Prediction time should be positive!\"\n",
    "        assert 0 <= metrics[\"Fitness Score\"] <= 1, f\"âŒ {model_name}: Fitness score should be between 0 and 1!\"\n",
    "        \n",
    "        # ðŸ›  Fix: Ensure \"Cross-Validation Stability\" is valid\n",
    "        if not (0 <= metrics[\"Cross-Validation Stability\"] <= 1):\n",
    "            print(f\"âš ï¸ Warning: {model_name} has an invalid stability score ({metrics['Cross-Validation Stability']}). Resetting to 0.5.\")\n",
    "            metrics[\"Cross-Validation Stability\"] = 0.5  # Default neutral value\n",
    "\n",
    "        # ðŸ›  Fix: Ensure \"Log Loss\" is valid\n",
    "        if pd.isna(metrics[\"Log Loss\"]) or metrics[\"Log Loss\"] < 0:\n",
    "            print(f\"âš ï¸ Warning: {model_name} has an invalid Log Loss ({metrics['Log Loss']}). Resetting to 10.0.\")\n",
    "            metrics[\"Log Loss\"] = 10.0  # Assign high but valid fallback value\n",
    "\n",
    "    print(\"âœ… Model metric validation passed.\")\n",
    "\n",
    "    # âœ… Ensure models are saved correctly\n",
    "    for model_name in model_results.keys():\n",
    "        model_path = os.path.join(SAVE_DIR, f\"{model_name}.pkl\")\n",
    "        assert os.path.exists(model_path), f\"âŒ Model {model_name} should be saved in {SAVE_DIR}!\"\n",
    "\n",
    "        # âœ… Reload the saved model and compare fitness scores\n",
    "        try:\n",
    "            saved_model = load_model(model_name)\n",
    "            assert saved_model is not None, f\"âŒ Saved model {model_name} should be reloadable!\"\n",
    "            assert np.isclose(saved_model[\"Fitness Score\"], model_results[model_name][\"Fitness Score\"], atol=1e-6), \\\n",
    "                f\"âŒ Fitness score mismatch for {model_name}!\"\n",
    "        except Exception as e:\n",
    "            raise AssertionError(f\"âŒ Error loading model {model_name}: {e}\")\n",
    "\n",
    "    print(\"âœ… Model saving and loading check passed.\")\n",
    "\n",
    "    # âœ… Ensure performance results file exists\n",
    "    results_path = os.path.join(SAVE_DIR, \"model_performance.csv\")\n",
    "\n",
    "    # ðŸ›  Force overwrite CSV instead of checking if it exists\n",
    "    df_model_performance = pd.DataFrame.from_dict(model_results, orient=\"index\")\n",
    "\n",
    "    # âœ… Ensure all columns are included\n",
    "    for col in required_metrics:\n",
    "        if col not in df_model_performance.columns:\n",
    "            df_model_performance[col] = np.nan  # Fill missing columns with NaN\n",
    "\n",
    "    # ðŸ›  Save the updated DataFrame\n",
    "    df_model_performance.to_csv(results_path, index=True, mode='w')  # âœ… Overwrite the CSV\n",
    "\n",
    "    print(\"\\nðŸ“Š Model performance results **successfully updated and saved** âœ…\")\n",
    "    print(\"ðŸ“‚ Models saved in:\", SAVE_DIR)\n",
    "    print(\"ðŸ“Š Model performance results saved in:\", results_path)\n",
    "\n",
    "# âœ… Run the test\n",
    "test_and_evaluate_all_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "0e670f67-4e80-40ea-bc19-52cf20a62687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure `SAVE_DIR` exists before saving models & results\n",
    "SAVE_DIR = \"saved_models\"\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# ðŸ“Œ STEP 7: RUN FULL PIPELINE\n",
    "# -----------------------------------------------\n",
    "\n",
    "def run_pipeline(df):\n",
    "    \"\"\"Executes the full ML pipeline with correct dataset usage for NEAT and incorporates model saving/loading.\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸš€ Running `run_pipeline()`...\")\n",
    "\n",
    "    # âœ… Step 1: Preprocess & Scale Data\n",
    "    df_cleaned = preprocess_data(df)\n",
    "    df_scaled = scale_features(df_cleaned)\n",
    "\n",
    "    # âœ… Step 2: Apply PCA for standard models\n",
    "    df_pca = apply_pca(df_scaled, n_components=50)\n",
    "\n",
    "    # âœ… Step 3: Train-Test Splits\n",
    "    X_train, X_test, y_train, y_test = split_data(df_pca)  # PCA-reduced for standard models\n",
    "    X_train_full, X_test_full, y_train_full, y_test_full = split_data(df_scaled)  # Full dataset for NEAT\n",
    "\n",
    "    # âœ… Step 4: Train All Models (with loading/saving)\n",
    "    model_scores = train_all_models(X_train, X_test, y_train, y_test, X_train_full, X_test_full, y_train_full, y_test_full)\n",
    "\n",
    "    # âœ… Step 5: Output Results\n",
    "    df_model_performance = pd.DataFrame.from_dict(model_scores, orient=\"index\")\n",
    "\n",
    "    print(\"\\nðŸ“Š Updated Model Performance Summary:\")\n",
    "    # print(df_model_performance.to_string(index=True))  # Clean output\n",
    "\n",
    "    # âœ… Step 6: Save the full results DataFrame for future use\n",
    "    try:\n",
    "        results_path = os.path.join(SAVE_DIR, \"model_performance.pkl\")\n",
    "        df_model_performance.to_pickle(results_path)\n",
    "        print(f\"âœ… Model performance results saved at: {results_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving model performance Pickle: {e}\")\n",
    "\n",
    "    return df_model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "3851c65e-0fc1-41a3-ab34-438bd919a4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Running `test_run_pipeline()`...\n",
      "âœ… Dataset loaded successfully.\n",
      "\n",
      "ðŸš€ Running `run_pipeline()`...\n",
      "\n",
      "ðŸš€ Training Elastic Net...\n",
      "ðŸ”„ Elastic Net already exists. Loading saved model...\n",
      "\n",
      "ðŸš€ Training SGD...\n",
      "ðŸ”„ SGD already exists. Loading saved model...\n",
      "\n",
      "ðŸš€ Training Gradient Boosting...\n",
      "ðŸ”„ Gradient Boosting already exists. Loading saved model...\n",
      "\n",
      "ðŸš€ Training CNN (MLP)...\n",
      "ðŸ”„ CNN (MLP) already exists. Loading saved model...\n",
      "\n",
      "ðŸš€ Training Diffusion Model...\n",
      "ðŸ”„ Diffusion Model already exists. Loading saved model...\n",
      "\n",
      "ðŸš€ Training GA-Optimized LR...\n",
      "ðŸ”„ GA-Optimized LR already exists. Loading saved model...\n",
      "\n",
      "ðŸš€ Training NeuroEvolution (NEAT)...\n",
      "ðŸ”„ NeuroEvolution (NEAT) already exists. Loading saved model...\n",
      "\n",
      "ðŸ“Š Updated Model Performance Summary:\n",
      "âœ… Model performance results saved at: saved_models/model_performance.pkl\n",
      "Index(['Elastic Net Best Alpha', 'Elastic Net Best L1 Ratio',\n",
      "       'Mean Squared Error (MSE)', 'RÂ² Score', 'ROC-AUC Score',\n",
      "       'Accuracy Score', 'Log Loss', 'Training Time (s)',\n",
      "       'Prediction Time (s)', 'Cross-Validation Stability', 'Fitness Score',\n",
      "       'Selected Features', 'Number of Features', 'Best Genome'],\n",
      "      dtype='object')\n",
      "âœ… Output structure verified.\n",
      "âœ… All required columns are present.\n",
      "âœ… Metric range validation passed.\n",
      "\n",
      "ðŸŽ¯ ALL TESTS PASSED: `test_run_pipeline()` âœ…\n",
      "ðŸ“‚ Model performance results saved in: saved_models/model_performance.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------------\n",
    "# ðŸ“Œ ASSERTIONS FOR `run_pipeline(df)`\n",
    "# -----------------------------------------------\n",
    "\n",
    "def test_run_pipeline():\n",
    "    \"\"\"Test the full pipeline execution to ensure correctness.\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸš€ Running `test_run_pipeline()`...\")\n",
    "\n",
    "    # âœ… Load dataset\n",
    "    df = pd.read_csv(\"data/financial_data_cleaned2.csv\")\n",
    "    assert not df.empty, \"âŒ Dataset should not be empty!\"\n",
    "\n",
    "    print(\"âœ… Dataset loaded successfully.\")\n",
    "\n",
    "    # âœ… Run the pipeline\n",
    "    results = run_pipeline(df)\n",
    "    print(results.keys())\n",
    "\n",
    "    # âœ… Check output type\n",
    "    assert isinstance(results, pd.DataFrame), \"âŒ Output should be a DataFrame!\"\n",
    "    assert not results.empty, \"âŒ Results should not be empty!\"\n",
    "    \n",
    "    print(\"âœ… Output structure verified.\")\n",
    "\n",
    "    # âœ… Required columns\n",
    "    required_cols = [\n",
    "        \"Mean Squared Error (MSE)\", \"RÂ² Score\", \"ROC-AUC Score\", \n",
    "        \"Accuracy Score\", \"Training Time (s)\", \"Prediction Time (s)\"\n",
    "    ]\n",
    "    assert all(col in results.columns for col in required_cols), \"âŒ Missing required columns in results!\"\n",
    "\n",
    "    print(\"âœ… All required columns are present.\")\n",
    "\n",
    "    # âœ… Check metric ranges\n",
    "    assert (results[\"Mean Squared Error (MSE)\"] >= 0).all(), \"âŒ MSE should not be negative!\"\n",
    "    assert results[\"ROC-AUC Score\"].between(0, 1).all(), \"âŒ ROC-AUC Score should be between 0 and 1!\"\n",
    "    assert (results[\"Training Time (s)\"] > 0).all(), \"âŒ Training time should be greater than 0!\"\n",
    "    assert (results[\"Prediction Time (s)\"] > 0).all(), \"âŒ Prediction time should be greater than 0!\"\n",
    "\n",
    "    print(\"âœ… Metric range validation passed.\")\n",
    "\n",
    "    # âœ… Ensure model performance results are saved\n",
    "    results_path = os.path.join(SAVE_DIR, \"model_performance.pkl\")\n",
    "    assert os.path.exists(results_path), \"âŒ Model performance results should be saved to PKL!\"\n",
    "\n",
    "    print(\"\\nðŸŽ¯ ALL TESTS PASSED: `test_run_pipeline()` âœ…\")\n",
    "    print(f\"ðŸ“‚ Model performance results saved in: {results_path}\")\n",
    "\n",
    "# âœ… Run the test\n",
    "test_run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "a9183e8f-2f85-4b0b-a063-3a2af163c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BATTLE/BREEDING PROGRAM###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "a204e61d-0b41-4bfe-98ed-1af3590e591f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Generation 1: Running Evolution\n",
      "ðŸ“ˆ 2 new hybrids created.\n",
      "\n",
      "ðŸš€ Generation 2: Running Evolution\n",
      "ðŸ“ˆ 2 new hybrids created.\n",
      "\n",
      "ðŸš€ Generation 3: Running Evolution\n",
      "ðŸŒ Market Conditions Changed! Global Shift: 0.9\n",
      "ðŸ“ˆ 3 new hybrids created.\n",
      "\n",
      "ðŸš€ Generation 4: Running Evolution\n",
      "ðŸ“ˆ 3 new hybrids created.\n",
      "\n",
      "ðŸš€ Generation 5: Running Evolution\n",
      "ðŸ“ˆ 4 new hybrids created.\n",
      "\n",
      "ðŸš€ Generation 6: Running Evolution\n",
      "ðŸŒ Market Conditions Changed! Global Shift: 0.89\n",
      "ðŸ“ˆ 5 new hybrids created.\n",
      "\n",
      "ðŸš€ Generation 7: Running Evolution\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'H-(CNN (MLP) x GA-Optimized LR) (Gen 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[601], line 142\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ”¬ Lineage Overview: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupermodel_history\u001b[38;5;241m.\u001b[39mget(best_recent_model,\u001b[38;5;250m \u001b[39m{})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParents\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal Model\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# ðŸš€ **Run AI Evolution**\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m run_evolution_pipeline(generations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[601], line 129\u001b[0m, in \u001b[0;36mrun_evolution_pipeline\u001b[0;34m(generations)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸŒ Market Conditions Changed! Global Shift: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmarket_shift\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m survivors \u001b[38;5;241m=\u001b[39m run_battles()\n\u001b[0;32m--> 129\u001b[0m new_generation \u001b[38;5;241m=\u001b[39m create_next_generation(survivors, gen)\n\u001b[1;32m    130\u001b[0m model_scores\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mdict\u001b[39m(new_generation))\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ“ˆ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_generation)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m new hybrids created.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[601], line 99\u001b[0m, in \u001b[0;36mcreate_next_generation\u001b[0;34m(survivors, gen)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(survivors), \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(survivors):\n\u001b[0;32m---> 99\u001b[0m         child_id, child_scores \u001b[38;5;241m=\u001b[39m breed_models(survivors[i], survivors[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], gen)\n\u001b[1;32m    100\u001b[0m         offspring\u001b[38;5;241m.\u001b[39mappend((child_id, child_scores))\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# ðŸ†• **If no hybrids formed, force mutation hybrids**\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[601], line 59\u001b[0m, in \u001b[0;36mbreed_models\u001b[0;34m(parent1, parent2, gen)\u001b[0m\n\u001b[1;32m     56\u001b[0m child_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH-(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m x \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) (Gen \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgen\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# âœ… Ensure parents have survival count (if they are newly created hybrids)\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m model_scores[parent1][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSurvival Count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model_scores[parent1]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSurvival Count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     60\u001b[0m model_scores[parent2][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSurvival Count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model_scores[parent2]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSurvival Count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# **Apply survival penalty for overbreeding**\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'H-(CNN (MLP) x GA-Optimized LR) (Gen 1)'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# âœ… Define Constants\n",
    "NEAT_BONUS = 0.12\n",
    "MAX_GENERATION_LIFESPAN = 5  # Prevents early extinction\n",
    "mutation_rate = 0.15\n",
    "market_shift = 1.0\n",
    "\n",
    "# âœ… Initialize Model Scores\n",
    "model_scores = {\n",
    "    \"Elastic Net\": {\"Fitness Score\": 0.89, \"Survival Count\": 0, \"Generation\": 1},\n",
    "    \"SGD\": {\"Fitness Score\": 0.86, \"Survival Count\": 0, \"Generation\": 1},\n",
    "    \"Gradient Boosting\": {\"Fitness Score\": 0.94, \"Survival Count\": 0, \"Generation\": 1},\n",
    "    \"CNN (MLP)\": {\"Fitness Score\": 0.96, \"Survival Count\": 0, \"Generation\": 1},\n",
    "    \"Diffusion Model\": {\"Fitness Score\": 0.54, \"Survival Count\": 0, \"Generation\": 1},\n",
    "    \"GA-Optimized LR\": {\"Fitness Score\": 0.96, \"Survival Count\": 0, \"Generation\": 1},\n",
    "    \"NeuroEvolution (NEAT)\": {\"Fitness Score\": 0.67, \"Survival Count\": 0, \"Generation\": 1, \"NEAT Bonus\": NEAT_BONUS}\n",
    "}\n",
    "\n",
    "supermodel_history = {}\n",
    "battle_log = []\n",
    "mutation_counter = {}\n",
    "\n",
    "# ðŸ“Œ **Step 1: Model Battles**\n",
    "def battle_models(model1, model2):\n",
    "    \"\"\"Simulates a battle between two models, ensuring NEAT has a fair chance.\"\"\"\n",
    "    score1 = model_scores[model1][\"Fitness Score\"] + model_scores[model1].get(\"NEAT Bonus\", 0)\n",
    "    score2 = model_scores[model2][\"Fitness Score\"] + model_scores[model2].get(\"NEAT Bonus\", 0)\n",
    "\n",
    "    winner, loser = (model1, model2) if score1 > score2 else (model2, model1)\n",
    "    \n",
    "    # âœ… Ensure survival count exists\n",
    "    model_scores[winner][\"Survival Count\"] = model_scores[winner].get(\"Survival Count\", 0) + 1\n",
    "\n",
    "    battle_log.append(f\"ðŸ”¥ {winner} defeated {loser} | ðŸ† Fitness: {round(score1, 3)} vs. {round(score2, 3)}\")\n",
    "    return winner\n",
    "\n",
    "def run_battles():\n",
    "    \"\"\"Runs battles and selects winners ensuring NEAT stays competitive.\"\"\"\n",
    "    survivors = []\n",
    "    model_list = list(model_scores.keys())\n",
    "    random.shuffle(model_list)\n",
    "\n",
    "    for i in range(0, len(model_list), 2):\n",
    "        if i + 1 < len(model_list):\n",
    "            winner = battle_models(model_list[i], model_list[i + 1])\n",
    "            survivors.append(winner)\n",
    "        else:\n",
    "            survivors.append(model_list[i])  \n",
    "\n",
    "    return survivors\n",
    "\n",
    "# ðŸ“Œ **Step 2: Breeding & Mutation**\n",
    "def breed_models(parent1, parent2, gen):\n",
    "    \"\"\"Breeds two models with fair NEAT bonuses, mutation, and diversity.\"\"\"\n",
    "    child_id = f\"H-({parent1} x {parent2}) (Gen {gen})\"\n",
    "\n",
    "    # âœ… Ensure parents have survival count (if they are newly created hybrids)\n",
    "    model_scores[parent1][\"Survival Count\"] = model_scores[parent1].get(\"Survival Count\", 0)\n",
    "    model_scores[parent2][\"Survival Count\"] = model_scores[parent2].get(\"Survival Count\", 0)\n",
    "\n",
    "    # **Apply survival penalty for overbreeding**\n",
    "    penalty = 0.05 * (model_scores[parent1][\"Survival Count\"] + model_scores[parent2][\"Survival Count\"])\n",
    "\n",
    "    # **Combine parent traits**\n",
    "    child_scores = {\n",
    "        \"Fitness Score\": (model_scores[parent1][\"Fitness Score\"] + model_scores[parent2][\"Fitness Score\"]) / 2,\n",
    "    }\n",
    "\n",
    "    # âœ… **Give NEAT models a fair boost in hybrids**\n",
    "    if \"NEAT\" in parent1 or \"NEAT\" in parent2:\n",
    "        child_scores[\"Fitness Score\"] += NEAT_BONUS  \n",
    "        child_scores[\"NEAT Bonus\"] = NEAT_BONUS  \n",
    "\n",
    "    # **Apply mutation & penalty**\n",
    "    mutation_factor = random.uniform(0.85, 1.15)\n",
    "    child_scores[\"Fitness Score\"] = round(child_scores[\"Fitness Score\"] * mutation_factor - penalty, 3)\n",
    "\n",
    "    # âœ… **Ensure Hybrid Models Have Survival Count & Generation**\n",
    "    child_scores[\"Survival Count\"] = 0  \n",
    "    child_scores[\"Generation\"] = gen  \n",
    "\n",
    "    # **Track lineage**\n",
    "    supermodel_history[child_id] = {\"Parents\": [parent1, parent2], \"Generation\": gen, \"Performance\": child_scores}\n",
    "    return child_id, child_scores\n",
    "\n",
    "def create_next_generation(survivors, gen):\n",
    "    \"\"\"Generates hybrids while maintaining model diversity and preventing stagnation.\"\"\"\n",
    "    offspring = []\n",
    "    global mutation_rate\n",
    "\n",
    "    # **Cull older generations (keep models alive longer)**\n",
    "    to_remove = [m for m in model_scores if model_scores[m][\"Generation\"] < gen - MAX_GENERATION_LIFESPAN]\n",
    "    for m in to_remove:\n",
    "        del model_scores[m]  \n",
    "\n",
    "    for i in range(0, len(survivors), 2):\n",
    "        if i + 1 < len(survivors):\n",
    "            child_id, child_scores = breed_models(survivors[i], survivors[i + 1], gen)\n",
    "            offspring.append((child_id, child_scores))\n",
    "\n",
    "    # ðŸ†• **If no hybrids formed, force mutation hybrids**\n",
    "    if not offspring and survivors:\n",
    "        print(\"âš ï¸ No hybrids created! Forcing a hybrid between survivors...\")\n",
    "        parent1, parent2 = random.sample(survivors, 2)\n",
    "        child_id, child_scores = breed_models(parent1, parent2, gen)\n",
    "        offspring.append((child_id, child_scores))\n",
    "\n",
    "    # **Increase mutation rate if weâ€™re running out of models**\n",
    "    if len(model_scores) < 3:\n",
    "        mutation_rate *= 1.25  # **Boost mutation rate dynamically**\n",
    "        print(f\"âš ï¸ Mutation Rate Increased to {mutation_rate:.2f} due to model scarcity!\")\n",
    "\n",
    "    return offspring\n",
    "\n",
    "# ðŸ“Œ **Step 3: Evolution Pipeline**\n",
    "def run_evolution_pipeline(generations=10):\n",
    "    \"\"\"Runs AI evolution process with balanced NEAT presence.\"\"\"\n",
    "    global market_shift\n",
    "    for gen in range(1, generations + 1):\n",
    "        print(f\"\\nðŸš€ Generation {gen}: Running Evolution\")\n",
    "        \n",
    "        # **Market Conditions Change Every 3 Generations**\n",
    "        if gen % 3 == 0:\n",
    "            market_shift = round(random.uniform(0.85, 1.15), 2)\n",
    "            print(f\"ðŸŒ Market Conditions Changed! Global Shift: {market_shift}\")\n",
    "\n",
    "        survivors = run_battles()\n",
    "        new_generation = create_next_generation(survivors, gen)\n",
    "        model_scores.update(dict(new_generation))\n",
    "\n",
    "        print(f\"ðŸ“ˆ {len(new_generation)} new hybrids created.\")\n",
    "\n",
    "    # âœ… **Ensure We Always Select a Supermodel**\n",
    "    best_recent_model = max(model_scores.keys(), key=lambda k: model_scores[k][\"Fitness Score\"])\n",
    "    print(\"\\nâœ… **Best Supermodel Identified!**\")\n",
    "    print(f\"ðŸ† Supermodel: {best_recent_model}\")\n",
    "    print(f\"ðŸ“Š Final Fitness Score: {model_scores[best_recent_model]['Fitness Score']}\")\n",
    "    print(f\"ðŸ”¬ Lineage Overview: {supermodel_history.get(best_recent_model, {}).get('Parents', 'Original Model')}\")\n",
    "\n",
    "# ðŸš€ **Run AI Evolution**\n",
    "run_evolution_pipeline(generations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dae695-a04f-489e-997f-f3d8fca03838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e085b-e040-4c03-859c-43a66660e33a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e119e-0dfb-40c4-8cb3-2a4b82553fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b13c76-3254-444f-99b2-5c2d841fbea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99702673-03c8-43b8-99b1-dde1fdb88090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7499ee0-b41e-421a-b585-e5d180676234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2ec75-9050-4ee0-8373-1e9606bf8000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d325ac9e-855c-4c9a-b950-b1ee44f44b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454e891b-494b-4057-9cb2-e43bbb2943c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172757f-6108-4821-b396-c17c4dd09248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdc8c5d-601e-42d1-84d6-9b70901990e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "1bf96c27-784d-42f3-8399-11d8cd67eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # battle models, select winners\n",
    "\n",
    "# import random\n",
    "# import json\n",
    "# import os\n",
    "\n",
    "# battle_log = []\n",
    "# supermodel_history = {}\n",
    "\n",
    "# def battle_models(model1, model2):\n",
    "#     \"\"\"Simulates a battle between two models, selects the winner.\"\"\"\n",
    "#     score1 = sum(model_scores[model1].values())\n",
    "#     score2 = sum(model_scores[model2].values())\n",
    "\n",
    "#     winner, loser = (model1, model2) if score1 > score2 else (model2, model1)\n",
    "#     margin = abs(score1 - score2)\n",
    "\n",
    "#     battle_log.append(f\"ðŸ”¥ {winner} defeated {loser} ({round(score1, 3)} vs. {round(score2, 3)}) | ðŸ† Margin: {round(margin, 3)}\")\n",
    "\n",
    "#     return winner\n",
    "\n",
    "# # def run_battles():\n",
    "# #     \"\"\"Runs battles for all models and selects winners.\"\"\"\n",
    "# #     survivors = []\n",
    "# #     model_list = list(model_scores.keys())\n",
    "# #     random.shuffle(model_list)\n",
    "\n",
    "# #     for i in range(0, len(model_list), 2):\n",
    "# #         if i + 1 < len(model_list):\n",
    "# #             survivors.append(battle_models(model_list[i], model_list[i + 1]))\n",
    "# #         else:\n",
    "# #             survivors.append(model_list[i])  # Odd model count case\n",
    "\n",
    "# #     print(\"âœ… Battle results:\")\n",
    "# #     for log in battle_log:\n",
    "# #         print(log)\n",
    "\n",
    "# #     return survivors\n",
    "\n",
    "# def run_battles():\n",
    "#     \"\"\"Runs battles and ensures enough survivors for breeding.\"\"\"\n",
    "#     survivors = []\n",
    "#     model_list = list(model_scores.keys())\n",
    "#     random.shuffle(model_list)\n",
    "\n",
    "#     for i in range(0, len(model_list), 2):\n",
    "#         if i + 1 < len(model_list):\n",
    "#             survivors.append(battle_models(model_list[i], model_list[i + 1]))\n",
    "#         else:\n",
    "#             survivors.append(model_list[i])  # Odd count case\n",
    "\n",
    "#     # âœ… Ensure at least **two survivors** for breeding\n",
    "#     if len(survivors) < 2:\n",
    "#         print(\"âš ï¸ Not enough survivors, forcing a random selection...\")\n",
    "#         while len(survivors) < 2:\n",
    "#             survivors.append(random.choice(model_list))\n",
    "\n",
    "#     print(\"âœ… Battle results:\")\n",
    "#     for log in battle_log[-len(survivors):]:  # Show latest results only\n",
    "#         print(log)\n",
    "\n",
    "#     return survivors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "4faea319-3aaa-4457-8f08-0d771262b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import random\n",
    "\n",
    "# # âœ… Ensure these global variables exist\n",
    "# model_scores = {}  # Placeholder: You should load actual model scores before running\n",
    "# supermodel_history = {}  # Stores parent lineage\n",
    "# battle_log = []  # Tracks battles\n",
    "\n",
    "# mutation_counter = {}\n",
    "\n",
    "# def breed_models(parent1, parent2, gen, mutation_rate=0.1):\n",
    "#     \"\"\"Breeds two models, assigns a proper hybrid name, and tracks mutations.\"\"\"\n",
    "#     child_id = f\"H-({parent1} x {parent2}) (Gen {gen})\"\n",
    "\n",
    "#     mutation_count = mutation_counter.get(child_id, 0)\n",
    "#     if random.random() < mutation_rate:  \n",
    "#         mutation_counter[child_id] = mutation_count + 1\n",
    "#         child_id += f\" M{mutation_counter[child_id]}\"  \n",
    "\n",
    "#     # âœ… Ensure parent scores exist\n",
    "#     if parent1 not in model_scores or parent2 not in model_scores:\n",
    "#         raise ValueError(f\"âŒ Parent scores missing for {parent1} or {parent2}\")\n",
    "\n",
    "#     # Hybrid inherits the average of parent performance metrics\n",
    "#     parent1_scores = model_scores[parent1]\n",
    "#     parent2_scores = model_scores[parent2]\n",
    "\n",
    "#     child_scores = {metric: round((parent1_scores.get(metric, 0) + parent2_scores.get(metric, 0)) / 2, 3) for metric in set(parent1_scores) | set(parent2_scores)}\n",
    "\n",
    "#     # Track lineage\n",
    "#     supermodel_history[child_id] = {\n",
    "#         \"Parents\": [parent1, parent2],\n",
    "#         \"Generation\": gen,\n",
    "#         \"Performance\": child_scores\n",
    "#     }\n",
    "\n",
    "#     return child_id, child_scores\n",
    "\n",
    "# def create_next_generation(survivors, gen):\n",
    "#     \"\"\"Generates hybrids and ensures at least one hybrid per gen.\"\"\"\n",
    "#     offspring = []\n",
    "#     for i in range(0, len(survivors), 2):\n",
    "#         if i + 1 < len(survivors):\n",
    "#             child_id, child_scores = breed_models(survivors[i], survivors[i + 1], gen)\n",
    "#             offspring.append((child_id, child_scores))\n",
    "\n",
    "#     # âœ… Force at least **one hybrid** if none were created\n",
    "#     if not offspring and len(survivors) > 1:\n",
    "#         print(\"âš ï¸ No hybrids created! Forcing a hybrid between two survivors...\")\n",
    "#         parent1, parent2 = random.sample(survivors, 2)\n",
    "#         child_id, child_scores = breed_models(parent1, parent2, gen)\n",
    "#         offspring.append((child_id, child_scores))\n",
    "\n",
    "#     return offspring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "0a53e4db-ca50-4fdd-a7ae-c24ab02368ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------------------------\n",
    "# # ðŸ“Œ SAVE THE BEST SUPERMODEL\n",
    "# # -----------------------------------------------\n",
    "# import json\n",
    "# import os\n",
    "\n",
    "# SAVE_DIR = \"saved_models\"\n",
    "# os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# def save_supermodel():\n",
    "#     \"\"\"Finds the best-performing model based on fitness evaluation and saves it.\"\"\"\n",
    "#     if not model_scores:\n",
    "#         raise ValueError(\"âŒ `model_scores` is empty! Ensure models have been trained.\")\n",
    "\n",
    "#     # âœ… Select the best model based on highest fitness score (REAL evaluation)\n",
    "#     best_model = max(model_scores, key=lambda k: model_scores[k][\"Fitness Score\"])\n",
    "#     best_fitness = model_scores[best_model][\"Fitness Score\"]\n",
    "\n",
    "#     model_path = os.path.join(SAVE_DIR, f\"{best_model}.pkl\")\n",
    "#     metadata_path = os.path.join(SAVE_DIR, f\"{best_model}_metadata.json\")\n",
    "\n",
    "#     # âœ… Save model lineage and performance stats dynamically\n",
    "#     metadata = {\n",
    "#         \"SuperModel\": best_model,\n",
    "#         \"Generation\": supermodel_history.get(best_model, {}).get(\"Generation\", \"Unknown\"),\n",
    "#         \"Parents\": supermodel_history.get(best_model, {}).get(\"Parents\", []),\n",
    "#         \"Performance\": model_scores[best_model],\n",
    "#         \"Mutations\": mutation_counter.get(best_model, 0),\n",
    "#         \"Battle Log\": battle_log\n",
    "#     }\n",
    "\n",
    "#     with open(metadata_path, \"w\") as f:\n",
    "#         json.dump(metadata, f, indent=4)\n",
    "\n",
    "#     print(\"\\nâœ… **Supermodel Saved!**\")\n",
    "#     print(f\"ðŸ† Best Model: {best_model}\")\n",
    "#     print(f\"ðŸ“Š Performance Stats: {model_scores[best_model]}\")\n",
    "#     print(f\"ðŸ”¬ Lineage: {metadata['Parents']}\")\n",
    "#     print(f\"ðŸ“‚ Saved in: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "b820fe39-0b3b-4e6d-b71a-77ec73109112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # evolution pipeline\n",
    "# # def run_evolution_pipeline(generations=3):\n",
    "# #     \"\"\"Runs the full AI evolution process for multiple generations.\"\"\"\n",
    "# #     global model_scores\n",
    "\n",
    "# #     for gen in range(1, generations + 1):\n",
    "# #         print(f\"\\nðŸš€ **Generation {gen}: Running Evolution** ðŸš€\")\n",
    "\n",
    "# #         survivors = run_battles()\n",
    "# #         new_generation = create_next_generation(survivors, gen)\n",
    "\n",
    "# #         if not new_generation:\n",
    "# #             print(\"âš ï¸ No hybrids created, stopping evolution early.\")\n",
    "# #             break\n",
    "\n",
    "# #         # Update model scores with new generation\n",
    "# #         model_scores.update(dict(new_generation))\n",
    "\n",
    "# #     save_supermodel()\n",
    "# #     print(\"\\nðŸŽ¯ **Evolution Complete!**\")\n",
    "\n",
    "# LOG_FILE = \"evolution_log.txt\"\n",
    "\n",
    "# def run_evolution_pipeline(generations=3):\n",
    "#     \"\"\"Runs AI evolution process and saves logs to a file instead of printing.\"\"\"\n",
    "#     global model_scores\n",
    "\n",
    "#     with open(LOG_FILE, \"w\") as log_file:\n",
    "#         for gen in range(1, generations + 1):\n",
    "#             log_file.write(f\"\\nðŸš€ **Generation {gen}: Running Evolution** ðŸš€\\n\")\n",
    "\n",
    "#             survivors = run_battles()\n",
    "#             new_generation = create_next_generation(survivors, gen)\n",
    "\n",
    "#             if not new_generation:\n",
    "#                 log_file.write(\"âš ï¸ No hybrids created, stopping evolution early.\\n\")\n",
    "#                 break\n",
    "\n",
    "#             model_scores.update(dict(new_generation))\n",
    "\n",
    "#         save_supermodel()\n",
    "#         log_file.write(\"\\nðŸŽ¯ **Evolution Complete!**\\n\")\n",
    "\n",
    "#     print(f\"\\nâœ… Evolution logs saved to `{LOG_FILE}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "95ac786e-df35-46d9-94d6-265aa7a04775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------------------------\n",
    "# # ðŸ“Œ LOAD ORIGINAL PARENT MODELS\n",
    "# # -----------------------------------------------\n",
    "\n",
    "# SAVE_DIR = \"saved_models\"\n",
    "# PERFORMANCE_PKL = os.path.join(SAVE_DIR, \"model_performance.pkl\")\n",
    "# LOG_FILE = \"evolution_log.txt\"\n",
    "# COLUMNS = ['Elastic Net Best Alpha', 'Elastic Net Best L1 Ratio', 'Mean Squared Error (MSE)', 'RÂ² Score', \n",
    "#            'ROC-AUC Score', 'Accuracy Score', 'Log Loss', 'Training Time (s)', 'Prediction Time (s)', \n",
    "#            'Cross-Validation Stability', 'Fitness Score']\n",
    "\n",
    "# # âœ… Load model performance metrics\n",
    "# model_performance = pd.read_pickle(PERFORMANCE_PKL)\n",
    "# model_performance = model_performance[COLUMNS]\n",
    "\n",
    "# # âœ… Initialize model scores from CSV\n",
    "# model_scores = model_performance.to_dict(orient=\"index\")\n",
    "\n",
    "# # âœ… Track battle history, supermodel lineage, and mutations\n",
    "# battle_log = []\n",
    "# supermodel_history = {}\n",
    "# mutation_counter = {}\n",
    "\n",
    "# # run it\n",
    "# run_evolution_pipeline(generations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a2446b-c212-414b-89f8-44c6c46f7e86",
   "metadata": {},
   "source": [
    "<!-- #### TEST 1 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cb3939-4a23-4733-9dc3-b4c2fb73d023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559ff678-7c9e-4c02-a17e-2733c8203560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0480cb4-9f48-4c01-80a7-837da916836f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9dacef-f9c1-4205-b0a5-009dae6084f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93a4555-cdad-4781-9cc1-69026b52b9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c61039-9058-4575-9268-7c2978375ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9c0809-fa7d-4c71-9075-76dfa376b3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225b1579-7025-4076-ab6b-3e665b36944d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f6b37-1cd5-49ae-9add-c232d32b3b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90997cc-f09b-459f-988e-b73a61585af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90014894-a3ad-45d1-b28d-7bf46e087050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8db6636-25d3-42ee-b4fe-4f9b36b9b516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b8bb2e-22b1-465f-a281-e81368d50a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c2e937-c11b-4e46-a3fa-186638766fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b168e60c-4e39-4a0e-a49d-00e1ddec852a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97451e24-a1ca-45af-a1fd-df6d679ff2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bdb213-3276-4500-a3e5-25269a4bedcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a43262-add2-4df8-b3df-423e43fe4324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3670db9-b151-4299-9aed-362701fc860c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb14d5b-c849-44fd-9907-d1ad6fc8a6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb6acc-8bee-4d37-b4ad-fd91d85919da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "30532b31-ca3e-4abc-971e-00283bfaecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import random\n",
    "# import torch \n",
    "# import torch.nn as nn\n",
    "# import os\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import SGDRegressor\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score\n",
    "\n",
    "# # âœ… Global Variables for Evolution\n",
    "# generation = 1\n",
    "# best_fitness_score = 1.0  \n",
    "# stagnation_counter = 0  \n",
    "# battle_log = []\n",
    "# stats_tracker = {}  \n",
    "# mutation_counter = {}  \n",
    "\n",
    "# SAVE_DIR = \"saved_models\"  \n",
    "# os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# # âœ… Assign unique identifiers for different model types\n",
    "# MODEL_IDS = {\n",
    "#     \"SGD\": \"S\", \"CNN (MLP)\": \"C\", \"Gradient Boosting\": \"G\", \n",
    "#     \"GA-Optimized LR\": \"GL\", \"Diffusion Model\": \"D\", \"Elastic Net\": \"E\",\n",
    "#     \"NeuroEvolution (NEAT)\": \"N\", \"Hybrid\": \"H\"\n",
    "# }\n",
    "\n",
    "# def assign_model_id(model_name):\n",
    "#     \"\"\"Assigns a unique identifier + a numeric index for each model.\"\"\"\n",
    "#     base_id = MODEL_IDS.get(model_name.split()[0], \"H\")  \n",
    "#     existing_ids = [int(name.split(\"-\")[-1]) for name in stats_tracker.keys() if name.startswith(base_id) and name.split(\"-\")[-1].isdigit()]\n",
    "#     next_id = max(existing_ids, default=0) + 1\n",
    "#     return f\"{base_id}-{next_id}\"\n",
    "\n",
    "# ### ðŸ“Œ **Step 1: Model Fitness Scoring**\n",
    "# def evaluate_fitness(model_scores):\n",
    "#     \"\"\"Computes weighted fitness scores based on model performance metrics.\"\"\"\n",
    "#     weights = {\"R2\": 0.4, \"MSE\": -0.3, \"ROC_AUC\": 0.5, \"Stability\": 0.2}\n",
    "    \n",
    "#     fitness_scores = {\n",
    "#         model: round(sum(weights[key] * scores.get(key, 0) for key in weights), 3)\n",
    "#         for model, scores in model_scores.items()\n",
    "#     }\n",
    "    \n",
    "#     sorted_models = sorted(fitness_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "#     return fitness_scores, sorted_models\n",
    "\n",
    "# ### ðŸ“Œ **Step 2: Model vs. Model Battles**\n",
    "# def battle_models(model1, model2, model_scores):\n",
    "#     \"\"\"Simulates a 1v1 battle between two models, winner survives to next gen.\"\"\"\n",
    "#     score1 = sum(model_scores[model1].values())\n",
    "#     score2 = sum(model_scores[model2].values())\n",
    "\n",
    "#     winner, loser = (model1, model2) if score1 > score2 else (model2, model1)\n",
    "#     margin = abs(score1 - score2)\n",
    "\n",
    "#     battle_log.append(f\"ðŸ”¥ {winner} defeated {loser} ({round(score1, 3)} vs. {round(score2, 3)}) | ðŸ† Margin: {round(margin, 3)}\")\n",
    "\n",
    "#     for model in [winner, loser]:\n",
    "#         if model not in stats_tracker:\n",
    "#             stats_tracker[model] = {\"Wins\": 0, \"Losses\": 0, \"Fitness History\": []}\n",
    "    \n",
    "#     stats_tracker[winner][\"Wins\"] += 1\n",
    "#     stats_tracker[loser][\"Losses\"] += 1\n",
    "#     stats_tracker[winner][\"Fitness History\"].append(score1)\n",
    "#     stats_tracker[loser][\"Fitness History\"].append(score2)\n",
    "\n",
    "#     return winner\n",
    "\n",
    "# ### ðŸ“Œ **Step 3: Breeding Function (Concatenated Hybrid Tags & Mutations)**\n",
    "# def breed_models(parent1, parent2, model_scores, gen, mutation_rate=0.1):\n",
    "#     \"\"\"Breeds two models, assigns proper hybrid name, and tracks mutations.\"\"\"\n",
    "#     child = {}\n",
    "\n",
    "#     def extract_fitness(score):\n",
    "#         \"\"\"Ensures we get a numeric fitness value from different types of models.\"\"\"\n",
    "#         if isinstance(score, dict):\n",
    "#             return sum(np.mean(v) if isinstance(v, list) else v for v in score.values() if isinstance(v, (int, float)))\n",
    "#         else:\n",
    "#             return 0  \n",
    "\n",
    "#     parent1_score = extract_fitness(model_scores[parent1])\n",
    "#     parent2_score = extract_fitness(model_scores[parent2])\n",
    "\n",
    "#     global stagnation_counter\n",
    "#     if stagnation_counter >= 2:\n",
    "#         mutation_rate *= 1.5  \n",
    "\n",
    "#     # âœ… Hybrid Naming Format: H-(S-12 x C-7) (Gen 5)\n",
    "#     child_id = f\"H-({parent1} x {parent2}) (Gen {gen})\"\n",
    "\n",
    "#     # âœ… Mutation Counter\n",
    "#     mutation_count = mutation_counter.get(child_id, 0)\n",
    "\n",
    "#     # âœ… If mutated, append \"M\" + count\n",
    "#     if random.random() < mutation_rate:  \n",
    "#         mutation_counter[child_id] = mutation_count + 1\n",
    "#         child_id += f\" M{mutation_counter[child_id]}\"  \n",
    "#         battle_log.append(f\"ðŸ”¬ Mutation in {child_id}!\")\n",
    "\n",
    "#     # âœ… If either parent is a NEAT model, treat child as a simple fitness dictionary\n",
    "#     if isinstance(model_scores[parent1], dict) or isinstance(model_scores[parent2], dict):\n",
    "#         return child_id, {\"Fitness Score\": (parent1_score + parent2_score) / 2}\n",
    "\n",
    "#     all_keys = set(model_scores[parent1].keys()) | set(model_scores[parent2].keys())\n",
    "#     for key in all_keys:\n",
    "#         if key in model_scores[parent1] and key in model_scores[parent2]:\n",
    "#             child[key] = (model_scores[parent1][key] + model_scores[parent2][key]) / 2\n",
    "#         elif key in model_scores[parent1]:\n",
    "#             child[key] = model_scores[parent1][key]\n",
    "#         elif key in model_scores[parent2]:\n",
    "#             child[key] = model_scores[parent2][key]\n",
    "#         else:\n",
    "#             child[key] = 0  \n",
    "\n",
    "#     return child_id, child\n",
    "\n",
    "# ### ðŸ“Œ **Step 6: AI Evolution Pipeline**\n",
    "# def run_evolution_pipeline(model_scores, generations=3):\n",
    "#     \"\"\"Executes AI evolution where models fight, breed, and evolve over generations.\"\"\"\n",
    "#     global generation, best_fitness_score, stagnation_counter  \n",
    "\n",
    "#     for gen in range(generations):\n",
    "#         print(f\"\\nðŸš€ **Starting Generation {generation} (AI Evolution)** ðŸš€\")\n",
    "        \n",
    "#         fitness_scores, ranked_models = evaluate_fitness(model_scores)\n",
    "        \n",
    "#         survivors = evolutionary_tournament(model_scores, generation)\n",
    "        \n",
    "#         offspring = generate_hybrids(survivors, model_scores, generation)\n",
    "        \n",
    "#         offspring_scores = {child[0]: child[1] for child in offspring}\n",
    "#         hybrid_fitness_scores, _ = evaluate_fitness(offspring_scores)\n",
    "\n",
    "#         all_fitness_scores = {**fitness_scores, **hybrid_fitness_scores}\n",
    "\n",
    "#         print(\"\\nâš” **Battle Log:**\")\n",
    "#         for log in battle_log:\n",
    "#             print(log)\n",
    "\n",
    "#     return model_scores\n",
    "\n",
    "# ### ðŸ“Œ **Helper Functions**\n",
    "# def evolutionary_tournament(model_scores, gen):\n",
    "#     \"\"\"Selects best models based on battles for breeding.\"\"\"\n",
    "#     models = list(model_scores.keys())\n",
    "#     random.shuffle(models)\n",
    "#     winners = []\n",
    "\n",
    "#     for i in range(0, len(models), 2):\n",
    "#         if i + 1 < len(models):\n",
    "#             winner = battle_models(models[i], models[i + 1], model_scores)\n",
    "#             winners.append(winner)\n",
    "#         else:\n",
    "#             winners.append(models[i])\n",
    "\n",
    "#     return winners\n",
    "\n",
    "# def generate_hybrids(survivors, model_scores, gen):\n",
    "#     \"\"\"Generates new hybrids by breeding surviving models.\"\"\"\n",
    "#     offspring = []\n",
    "#     for i in range(0, len(survivors), 2):\n",
    "#         if i + 1 < len(survivors):\n",
    "#             child_id, child = breed_models(survivors[i], survivors[i + 1], model_scores, gen)\n",
    "#             offspring.append((child_id, child))\n",
    "\n",
    "#     return offspring\n",
    "\n",
    "# # âœ… **Run AI Evolution**\n",
    "# final_results = run_evolution_pipeline(model_scores, generations=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4addab88-711d-4326-b403-4d25f2c83b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def translate_lineage(model_tag):\n",
    "#     \"\"\"Converts a model tag into human-readable format.\"\"\"\n",
    "#     tag = model_tag.replace(\"H-\", \"Hybrid of \").replace(\"S-\", \"SGD Model #\") \\\n",
    "#                    .replace(\"C-\", \"CNN Model #\").replace(\"G-\", \"Gradient Boosting Model #\") \\\n",
    "#                    .replace(\"N-\", \"NEAT Model #\").replace(\"D-\", \"Diffusion Model #\") \\\n",
    "#                    .replace(\"GL-\", \"GA-Optimized LR Model #\").replace(\"E-\", \"Elastic Net Model #\") \\\n",
    "#                    .replace(\"M\", \" (Mutation\") + \")\"\n",
    "\n",
    "#     return tag.replace(\")\", \") \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c6782bbb-a37c-4f18-8663-edc49e61908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(translate_lineage(\"H-(((S-12 x C-7) x (C-6 x G-4)) x N-3) (Gen 10) M5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "13e69c04-351e-4f0f-8107-60118f8c6009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats_tracker = {}\n",
    "\n",
    "# def track_performance(model_name, fitness, win=True):\n",
    "#     \"\"\"Tracks model performance across battles.\"\"\"\n",
    "#     if model_name not in stats_tracker:\n",
    "#         stats_tracker[model_name] = {\"Wins\": 0, \"Losses\": 0, \"Fitness History\": []}\n",
    "\n",
    "#     stats_tracker[model_name][\"Wins\" if win else \"Losses\"] += 1\n",
    "#     stats_tracker[model_name][\"Fitness History\"].append(fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "de93ea7a-957d-4119-a94d-c22ac189966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track_performance(\"H-(S-12 x C-7) (Gen 3)\", 0.97, win=True)\n",
    "# track_performance(\"H-(S-12 x C-7) (Gen 3)\", 0.92, win=False)\n",
    "# print(stats_tracker[\"H-(S-12 x C-7) (Gen 3)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5f989bef-c546-4d94-a515-a1085bb49344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "# import torch\n",
    "\n",
    "# SAVE_DIR = \"saved_models\"\n",
    "# os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# def save_supermodel(model_name, final_fitness):\n",
    "#     \"\"\"Saves the best-performing model and metadata.\"\"\"\n",
    "#     model_path = os.path.join(SAVE_DIR, f\"{model_name}.pt\")\n",
    "#     metadata_path = os.path.join(SAVE_DIR, f\"{model_name}_metadata.json\")\n",
    "\n",
    "#     torch.save(model_name, model_path)\n",
    "\n",
    "#     metadata = {\n",
    "#         \"SuperModel\": model_name,\n",
    "#         \"Performance\": stats_tracker.get(model_name, {}),\n",
    "#         \"Mutations\": mutation_counter.get(model_name, 0)\n",
    "#     }\n",
    "\n",
    "#     with open(metadata_path, \"w\") as f:\n",
    "#         json.dump(metadata, f, indent=4)\n",
    "\n",
    "#     print(f\"âœ… Supermodel saved: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "680eb27a-4e1d-45bf-bdd3-eeb73bea3695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Supermodel saved: H-(((S-12 x C-7) x (C-6 x G-4)) x N-3) (Gen 10)\n"
     ]
    }
   ],
   "source": [
    "# save_supermodel(\"H-(((S-12 x C-7) x (C-6 x G-4)) x N-3) (Gen 10)\", 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "508e4299-44af-4fee-8c26-d329b1ebbe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import numpy as np\n",
    "\n",
    "# battle_log = []\n",
    "# mutation_counter = {}\n",
    "\n",
    "# def battle_models(model1, model2, model_scores):\n",
    "#     \"\"\"Simulates a 1v1 battle between two models, winner survives to next gen.\"\"\"\n",
    "#     score1 = sum(model_scores[model1].values())\n",
    "#     score2 = sum(model_scores[model2].values())\n",
    "\n",
    "#     winner, loser = (model1, model2) if score1 > score2 else (model2, model1)\n",
    "#     margin = abs(score1 - score2)\n",
    "\n",
    "#     battle_log.append(f\"ðŸ”¥ {winner} defeated {loser} ({round(score1, 3)} vs. {round(score2, 3)}) | ðŸ† Margin: {round(margin, 3)}\")\n",
    "    \n",
    "#     track_performance(winner, score1, win=True)\n",
    "#     track_performance(loser, score2, win=False)\n",
    "\n",
    "#     return winner\n",
    "\n",
    "# def breed_models(parent1, parent2, model_scores, gen, mutation_rate=0.1):\n",
    "#     \"\"\"Breeds two models, assigns proper hybrid name, and tracks mutations.\"\"\"\n",
    "#     child = {}\n",
    "\n",
    "#     def extract_fitness(score):\n",
    "#         \"\"\"Ensures we get a numeric fitness value from different types of models.\"\"\"\n",
    "#         return sum(np.mean(v) if isinstance(v, list) else v for v in score.values() if isinstance(v, (int, float)))\n",
    "\n",
    "#     parent1_score = extract_fitness(model_scores[parent1])\n",
    "#     parent2_score = extract_fitness(model_scores[parent2])\n",
    "\n",
    "#     child_id = f\"H-({parent1} x {parent2}) (Gen {gen})\"\n",
    "\n",
    "#     mutation_count = mutation_counter.get(child_id, 0)\n",
    "#     if random.random() < mutation_rate:  \n",
    "#         mutation_counter[child_id] = mutation_count + 1\n",
    "#         child_id += f\" M{mutation_counter[child_id]}\"  \n",
    "#         battle_log.append(f\"ðŸ”¬ Mutation in {child_id}!\")\n",
    "\n",
    "#     all_keys = set(model_scores[parent1].keys()) | set(model_scores[parent2].keys())\n",
    "#     for key in all_keys:\n",
    "#         if key in model_scores[parent1] and key in model_scores[parent2]:\n",
    "#             child[key] = (model_scores[parent1][key] + model_scores[parent2][key]) / 2\n",
    "#         elif key in model_scores[parent1]:\n",
    "#             child[key] = model_scores[parent1][key]\n",
    "#         elif key in model_scores[parent2]:\n",
    "#             child[key] = model_scores[parent2][key]\n",
    "#         else:\n",
    "#             child[key] = 0  \n",
    "\n",
    "#     return child_id, child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "5167ad44-c875-4c94-883f-3264e27e60e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ† Winner: S-12\n",
      "ðŸ§¬ New Child: H-(S-12 x C-7) (Gen 2), Scores: {'R2': 0.935, 'ROC_AUC': 0.9, 'MSE': 0.025}\n"
     ]
    }
   ],
   "source": [
    "# model_scores = {\n",
    "#     \"S-12\": {\"R2\": 0.95, \"MSE\": 0.02, \"ROC_AUC\": 0.92},\n",
    "#     \"C-7\": {\"R2\": 0.92, \"MSE\": 0.03, \"ROC_AUC\": 0.88}\n",
    "# }\n",
    "\n",
    "# winner = battle_models(\"S-12\", \"C-7\", model_scores)\n",
    "# print(f\"ðŸ† Winner: {winner}\")\n",
    "\n",
    "# child_id, child_scores = breed_models(\"S-12\", \"C-7\", model_scores, gen=2)\n",
    "# print(f\"ðŸ§¬ New Child: {child_id}, Scores: {child_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad9245-87d2-48b1-9716-365ec6cf60b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE MODELS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e966c84b-1695-4981-b389-b3fb34f192de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "\n",
    "# # âœ… Ensure SAVE_DIR exists\n",
    "# SAVE_DIR = \"saved_models\"\n",
    "# os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# # âœ… Global Variables for Evolution\n",
    "# generation = 1\n",
    "# best_fitness_score = 1.0  \n",
    "# stagnation_counter = 0  \n",
    "# battle_log = []\n",
    "\n",
    "# ### ðŸ“Œ **Step 1: Model Fitness Scoring**\n",
    "# def evaluate_fitness(model_scores):\n",
    "#     \"\"\"Computes weighted fitness scores based on model performance metrics.\"\"\"\n",
    "#     weights = {\"RÂ² Score\": 0.4, \"Mean Squared Error (MSE)\": -0.3, \"ROC-AUC Score\": 0.5, \"Cross-Validation Stability\": 0.2}\n",
    "    \n",
    "#     fitness_scores = {\n",
    "#         model: round(sum(weights[key] * scores.get(key, 0) for key in weights), 3)\n",
    "#         if isinstance(scores, dict) else scores.fitness  # If NEAT genome, use fitness score directly\n",
    "#         for model, scores in model_scores.items()\n",
    "#     }\n",
    "    \n",
    "#     sorted_models = sorted(fitness_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "#     return fitness_scores, sorted_models\n",
    "\n",
    "# ### ðŸ“Œ **Step 2: Model vs. Model Battles**\n",
    "# def battle_models(model1, model2, model_scores):\n",
    "#     \"\"\"Simulates a 1v1 battle between two models, winner survives to next gen.\"\"\"\n",
    "    \n",
    "#     def get_fitness(model):\n",
    "#         \"\"\"Extracts fitness score from model_scores, handling lists properly.\"\"\"\n",
    "#         score = model_scores[model]\n",
    "        \n",
    "#         if isinstance(score, dict):\n",
    "#             numeric_scores = []\n",
    "#             for key, value in score.items():\n",
    "#                 if isinstance(value, list):  # ðŸ›  Handle lists by taking the mean\n",
    "#                     numeric_scores.append(np.mean(value))\n",
    "#                 elif isinstance(value, (int, float)):  # Ensure it's numeric\n",
    "#                     numeric_scores.append(value)\n",
    "#                 else:\n",
    "#                     print(f\"âš ï¸ Skipping unexpected type in {model}: {key} -> {type(value)}\")\n",
    "            \n",
    "#             return sum(numeric_scores)  # âœ… Sum of all numeric values\n",
    "    \n",
    "#         elif hasattr(score, \"fitness\"):  # âœ… If it's a NEAT genome, extract fitness\n",
    "#             return score.fitness  \n",
    "    \n",
    "#         else:\n",
    "#             raise TypeError(f\"Unexpected model type: {type(score)} in model_scores[{model}]\")\n",
    "    \n",
    "#     score1 = get_fitness(model1)\n",
    "#     score2 = get_fitness(model2)\n",
    "\n",
    "#     # Winner = Model with Higher Fitness Score\n",
    "#     winner, loser = (model1, model2) if score1 > score2 else (model2, model1)\n",
    "    \n",
    "#     # Log battle result\n",
    "#     battle_log.append(f\"ðŸ”¥ {winner} defeated {loser} ({round(score1, 3)} vs. {round(score2, 3)})\")\n",
    "    \n",
    "#     return winner\n",
    "\n",
    "# ### ðŸ“Œ **Step 3: Adaptive Breeding Function**\n",
    "# def breed_models(parent1, parent2, model_scores, gen, mutation_rate=0.1):\n",
    "#     \"\"\"Breeds two models, labels child, and applies adaptive mutations.\"\"\"\n",
    "#     child = {}\n",
    "\n",
    "#     def extract_fitness(score):\n",
    "#         \"\"\"Ensures we get a numeric fitness value from different types of models.\"\"\"\n",
    "#         if isinstance(score, dict):\n",
    "#             return sum(np.mean(v) if isinstance(v, list) else v for v in score.values() if isinstance(v, (int, float)))\n",
    "#         elif hasattr(score, \"fitness\"):\n",
    "#             return score.fitness\n",
    "#         else:\n",
    "#             print(f\"âš ï¸ Unexpected fitness structure: {type(score)} -> {score}\")\n",
    "#             return 0  # Default fallback to avoid errors\n",
    "\n",
    "#     parent1_score = extract_fitness(model_scores[parent1])\n",
    "#     parent2_score = extract_fitness(model_scores[parent2])\n",
    "\n",
    "#     global stagnation_counter\n",
    "#     if stagnation_counter >= 2:\n",
    "#         mutation_rate *= 1.5  # Increase mutation rate if no improvement\n",
    "\n",
    "#     # Ensure child gets all required keys\n",
    "#     all_keys = set(model_scores[parent1].keys()) | set(model_scores[parent2].keys()) if isinstance(model_scores[parent1], dict) and isinstance(model_scores[parent2], dict) else {}\n",
    "\n",
    "#     for key in all_keys:\n",
    "#         if key in model_scores[parent1] and key in model_scores[parent2]:\n",
    "#             child[key] = (model_scores[parent1][key] + model_scores[parent2][key]) / 2\n",
    "#         elif key in model_scores[parent1]:\n",
    "#             child[key] = model_scores[parent1][key]\n",
    "#         elif key in model_scores[parent2]:\n",
    "#             child[key] = model_scores[parent2][key]\n",
    "#         else:\n",
    "#             child[key] = 0  \n",
    "\n",
    "#         # Adaptive Mutation\n",
    "#         if random.random() < mutation_rate:  \n",
    "#             child[key] += random.uniform(-0.05, 0.10)  \n",
    "\n",
    "#     child_label = f\"Child of {parent1.split()[0]} & {parent2.split()[0]} (Gen {gen})\"\n",
    "#     return child_label, child\n",
    "\n",
    "# ### ðŸ“Œ **Step 4: Evolutionary Tournament**\n",
    "# def evolutionary_tournament(model_scores, gen):\n",
    "#     \"\"\"Tournament-style survival: Models fight, winners survive, losers get eliminated.\"\"\"\n",
    "#     global battle_log\n",
    "#     battle_log = []\n",
    "    \n",
    "#     models = list(model_scores.keys())\n",
    "#     random.shuffle(models)  \n",
    "#     survivors = []\n",
    "\n",
    "#     while len(models) > 1:\n",
    "#         m1, m2 = models.pop(), models.pop()\n",
    "#         winner = battle_models(m1, m2, model_scores)\n",
    "#         survivors.append(winner)\n",
    "    \n",
    "#     if models:  \n",
    "#         survivors.append(models[0])  \n",
    "    \n",
    "#     return survivors\n",
    "\n",
    "# ### ðŸ“Œ **Step 5: Create Hybrid Models**\n",
    "# def generate_hybrids(survivors, model_scores, gen):\n",
    "#     \"\"\"Generates hybrid models from top survivors.\"\"\"\n",
    "#     offspring = []\n",
    "    \n",
    "#     for _ in range(2):  \n",
    "#         p1, p2 = random.sample(survivors, 2)\n",
    "#         child_label, child_model = breed_models(p1, p2, model_scores, gen)\n",
    "#         offspring.append((child_label, child_model))\n",
    "    \n",
    "#     return offspring\n",
    "\n",
    "# ### ðŸ“Œ **Step 6: AI Evolution Pipeline**\n",
    "# def run_evolution_pipeline(generations=10, stagnation_limit=3):\n",
    "#     \"\"\"\n",
    "#     Executes AI evolution pipeline where models fight for survival,\n",
    "#     breed the winners, and evolve over generations.\n",
    "#     \"\"\"\n",
    "#     global generation, best_fitness_score, stagnation_counter  \n",
    "\n",
    "#     # âœ… Load saved models as starting population\n",
    "#     model_scores = {}\n",
    "#     for filename in os.listdir(SAVE_DIR):\n",
    "#         if filename.endswith(\".pkl\"):\n",
    "#             model_name = filename.replace(\".pkl\", \"\")\n",
    "#             model_data = pickle.load(open(os.path.join(SAVE_DIR, filename), \"rb\"))\n",
    "#             model_scores[model_name] = model_data\n",
    "    \n",
    "#     for gen in range(generations):\n",
    "#         print(f\"\\nðŸš€ **Starting Generation {generation} (AI Evolution)** ðŸš€\")\n",
    "        \n",
    "#         # âœ… Evaluate Fitness of Existing Models\n",
    "#         fitness_scores, ranked_models = evaluate_fitness(model_scores)\n",
    "        \n",
    "#         # âœ… Run Tournament Battles\n",
    "#         survivors = evolutionary_tournament(model_scores, generation)\n",
    "        \n",
    "#         # âœ… Generate Hybrid Models from Survivors\n",
    "#         offspring = generate_hybrids(survivors, model_scores, generation)\n",
    "        \n",
    "#         # âœ… Evaluate Hybrid Models\n",
    "#         offspring_scores = {child[0]: child[1] for child in offspring}\n",
    "#         hybrid_fitness_scores, _ = evaluate_fitness(offspring_scores)\n",
    "\n",
    "#         # âœ… Combine All Scores\n",
    "#         all_fitness_scores = {**fitness_scores, **hybrid_fitness_scores}\n",
    "\n",
    "#         # âœ… Log Tournament Results\n",
    "#         print(\"\\nâš” **Battle Log:**\")\n",
    "#         for log in battle_log:\n",
    "#             print(log)\n",
    "\n",
    "#         # âœ… Identify Best Model & Improvement\n",
    "#         best_model = max(all_fitness_scores.items(), key=lambda x: x[1])\n",
    "#         improvement = round(best_model[1] - best_fitness_score, 3)\n",
    "\n",
    "#         # âœ… Update best fitness score & handle stagnation\n",
    "#         if best_model[1] > best_fitness_score:\n",
    "#             best_fitness_score = best_model[1]\n",
    "#             stagnation_counter = 0  \n",
    "#         else:\n",
    "#             stagnation_counter += 1  \n",
    "#             if stagnation_counter >= stagnation_limit:\n",
    "#                 print(\"ðŸš¨ No further improvements. Saving the Supermodel!\")\n",
    "#                 pickle.dump(model_scores[best_model[0]], open(os.path.join(SAVE_DIR, \"Supermodel.pkl\"), \"wb\"))\n",
    "#                 break  \n",
    "\n",
    "#         model_scores.update(offspring_scores)\n",
    "#         generation += 1\n",
    "\n",
    "#     return all_fitness_scores\n",
    "\n",
    "# # âœ… **Run AI Evolution**\n",
    "# final_results = run_evolution_pipeline(generations=10, stagnation_limit=3)\n",
    "\n",
    "# # âœ… **Check Supermodel**\n",
    "# supermodel_path = os.path.join(SAVE_DIR, \"Supermodel.pkl\")\n",
    "# if os.path.exists(supermodel_path):\n",
    "#     print(\"ðŸŽ¯ Supermodel Saved Successfully at:\", supermodel_path)\n",
    "# else:\n",
    "#     print(\"âš ï¸ No Supermodel was savedâ€”further evolution might be needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "88d72500-7436-4c67-846b-fa23165d7453",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"saved_models/Supermodel.pkl\", \"rb\") as file:\n",
    "    loaded_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f9063328-d1de-4300-89e1-13e54691e8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mean Squared Error (MSE)': 0.001688510514653581,\n",
       " 'RÂ² Score': 0.9611144834162697,\n",
       " 'ROC-AUC Score': 0.9999527335749173,\n",
       " 'Accuracy Score': 0.9983457402812241,\n",
       " 'Log Loss': 0.0060235345803089585,\n",
       " 'Training Time (s)': 3.223639965057373,\n",
       " 'Prediction Time (s)': 0.0018792152404785156,\n",
       " 'Cross-Validation Stability': 0.9968989034844068,\n",
       " 'Fitness Score': 0.2323821701320006}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0059d786-7d26-402d-a08f-694565f507e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import random\n",
    "# import neat\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from sklearn.linear_model import ElasticNetCV, SGDRegressor\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.feature_selection import SelectKBest, f_classif\n",
    "# from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score\n",
    "\n",
    "# # âœ… Global Variables for Evolution\n",
    "# generation = 1\n",
    "# best_fitness_score = 1.0  \n",
    "# stagnation_counter = 0  \n",
    "# battle_log = []\n",
    "\n",
    "# ### ðŸ“Œ **Step 1: Model Fitness Scoring**\n",
    "# def evaluate_fitness(model_scores):\n",
    "#     \"\"\"Computes weighted fitness scores based on model performance metrics.\"\"\"\n",
    "#     weights = {\"R2\": 0.4, \"MSE\": -0.3, \"ROC_AUC\": 0.5, \"Stability\": 0.2}\n",
    "    \n",
    "#     fitness_scores = {\n",
    "#         model: round(sum(weights[key] * scores.get(key, 0) for key in weights), 3)\n",
    "#         for model, scores in model_scores.items()\n",
    "#     }\n",
    "    \n",
    "#     sorted_models = sorted(fitness_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "#     return fitness_scores, sorted_models\n",
    "\n",
    "# ### ðŸ“Œ **Step 2: Model vs. Model Battles**\n",
    "# def battle_models(model1, model2, model_scores):\n",
    "#     \"\"\"Simulates a 1v1 battle between two models, winner survives to next gen.\"\"\"\n",
    "#     score1 = sum(model_scores[model1].values())\n",
    "#     score2 = sum(model_scores[model2].values())\n",
    "\n",
    "#     # Winner = Model with Higher Fitness Score\n",
    "#     winner, loser = (model1, model2) if score1 > score2 else (model2, model1)\n",
    "    \n",
    "#     # Log battle result\n",
    "#     battle_log.append(f\"ðŸ”¥ {winner} defeated {loser} ({round(score1, 3)} vs. {round(score2, 3)})\")\n",
    "    \n",
    "#     return winner\n",
    "\n",
    "# ### ðŸ“Œ **Step 3: Adaptive Breeding Function (With Full Lineage)**\n",
    "# def breed_models(parent1, parent2, model_scores, gen, mutation_rate=0.1):\n",
    "#     \"\"\"Breeds two models, keeps full lineage, and applies adaptive mutations.\"\"\"\n",
    "#     child = {}\n",
    "\n",
    "#     def extract_fitness(score):\n",
    "#         \"\"\"Ensures we get a numeric fitness value from different types of models.\"\"\"\n",
    "#         if isinstance(score, dict):\n",
    "#             return sum(np.mean(v) if isinstance(v, list) else v for v in score.values() if isinstance(v, (int, float)))\n",
    "#         elif isinstance(score, neat.DefaultGenome):  # âœ… If it's a NEAT genome, extract only fitness\n",
    "#             return score.fitness if hasattr(score, \"fitness\") else 0\n",
    "#         else:\n",
    "#             print(f\"âš ï¸ Unexpected fitness structure: {type(score)} -> {score}\")\n",
    "#             return 0  # Default fallback to avoid errors\n",
    "\n",
    "#     parent1_score = extract_fitness(model_scores[parent1])\n",
    "#     parent2_score = extract_fitness(model_scores[parent2])\n",
    "\n",
    "#     global stagnation_counter\n",
    "#     if stagnation_counter >= 2:\n",
    "#         mutation_rate *= 1.5  # Increase mutation rate if no improvement\n",
    "\n",
    "#     # âœ… Ensure proper name tracking for full lineage\n",
    "#     if \" -> \" in parent1:\n",
    "#         lineage1 = parent1\n",
    "#     else:\n",
    "#         lineage1 = f\"({parent1})\"\n",
    "\n",
    "#     if \" -> \" in parent2:\n",
    "#         lineage2 = parent2\n",
    "#     else:\n",
    "#         lineage2 = f\"({parent2})\"\n",
    "\n",
    "#     # âœ… Construct full lineage\n",
    "#     child_label = f\"{lineage1} -> {lineage2} (Gen {gen})\"\n",
    "\n",
    "#     # **NEW FIX:** If either parent is a NEAT model, treat child as a simple fitness dictionary\n",
    "#     if isinstance(model_scores[parent1], neat.DefaultGenome) or isinstance(model_scores[parent2], neat.DefaultGenome):\n",
    "#         return child_label, {\"Fitness Score\": (parent1_score + parent2_score) / 2}\n",
    "\n",
    "#     # Ensure child gets all required keys (only if both are normal models)\n",
    "#     all_keys = set(model_scores[parent1].keys()) | set(model_scores[parent2].keys()) if isinstance(model_scores[parent1], dict) and isinstance(model_scores[parent2], dict) else {}\n",
    "\n",
    "#     for key in all_keys:\n",
    "#         if key in model_scores[parent1] and key in model_scores[parent2]:\n",
    "#             child[key] = (model_scores[parent1][key] + model_scores[parent2][key]) / 2\n",
    "#         elif key in model_scores[parent1]:\n",
    "#             child[key] = model_scores[parent1][key]\n",
    "#         elif key in model_scores[parent2]:\n",
    "#             child[key] = model_scores[parent2][key]\n",
    "#         else:\n",
    "#             child[key] = 0  \n",
    "\n",
    "#         # Adaptive Mutation\n",
    "#         if random.random() < mutation_rate:  \n",
    "#             child[key] += random.uniform(-0.05, 0.10)  \n",
    "\n",
    "#     return child_label, child\n",
    "\n",
    "# ### ðŸ“Œ **Step 4: Evolutionary Tournament**\n",
    "# def evolutionary_tournament(model_scores, gen):\n",
    "#     \"\"\"Tournament-style survival: Models fight, winners survive, losers get eliminated.\"\"\"\n",
    "#     global battle_log\n",
    "#     battle_log = []\n",
    "    \n",
    "#     models = list(model_scores.keys())\n",
    "#     random.shuffle(models)  \n",
    "#     survivors = []\n",
    "\n",
    "#     while len(models) > 1:\n",
    "#         m1, m2 = models.pop(), models.pop()\n",
    "#         winner = battle_models(m1, m2, model_scores)\n",
    "#         survivors.append(winner)\n",
    "    \n",
    "#     if models:  \n",
    "#         survivors.append(models[0])  \n",
    "    \n",
    "#     return survivors\n",
    "\n",
    "# ### ðŸ“Œ **Step 5: Create Hybrid Models**\n",
    "# def generate_hybrids(survivors, model_scores, gen):\n",
    "#     \"\"\"Generates hybrid models from top survivors.\"\"\"\n",
    "#     offspring = []\n",
    "    \n",
    "#     for _ in range(2):  \n",
    "#         p1, p2 = random.sample(survivors, 2)\n",
    "#         child_label, child_model = breed_models(p1, p2, model_scores, gen)\n",
    "#         offspring.append((child_label, child_model))\n",
    "    \n",
    "#     return offspring\n",
    "\n",
    "# ### ðŸ“Œ **Step 6: AI Evolution Pipeline (Now With Supermodel Selection)**\n",
    "# def run_evolution_pipeline(model_scores, generations=3):\n",
    "#     \"\"\"\n",
    "#     Executes AI battle royale pipeline where models fight for survival,\n",
    "#     breed the winners, and evolve over generations.\n",
    "#     \"\"\"\n",
    "#     global generation, best_fitness_score, stagnation_counter  \n",
    "\n",
    "#     for gen in range(generations):\n",
    "#         print(f\"\\nðŸš€ **Starting Generation {generation} (AI Evolution)** ðŸš€\")\n",
    "        \n",
    "#         # âœ… Evaluate Fitness of Existing Models\n",
    "#         fitness_scores, ranked_models = evaluate_fitness(model_scores)\n",
    "        \n",
    "#         # âœ… Run Tournament Battles\n",
    "#         survivors = evolutionary_tournament(model_scores, generation)\n",
    "        \n",
    "#         # âœ… Generate Hybrid Models from Survivors\n",
    "#         offspring = generate_hybrids(survivors, model_scores, generation)\n",
    "        \n",
    "#         # âœ… Evaluate Hybrid Models\n",
    "#         offspring_scores = {child[0]: child[1] for child in offspring}\n",
    "#         hybrid_fitness_scores, _ = evaluate_fitness(offspring_scores)\n",
    "\n",
    "#         # âœ… Combine All Scores\n",
    "#         all_fitness_scores = {**fitness_scores, **hybrid_fitness_scores}\n",
    "\n",
    "#         # âœ… Log Tournament Results\n",
    "#         print(\"\\nâš” **Battle Log:**\")\n",
    "#         for log in battle_log:\n",
    "#             print(log)\n",
    "\n",
    "#         # âœ… Identify Best Model & Improvement\n",
    "#         best_model_name, best_model_score = max(all_fitness_scores.items(), key=lambda x: x[1])\n",
    "#         improvement = round(best_model_score - best_fitness_score, 3)\n",
    "\n",
    "#         # âœ… Update best fitness score & handle stagnation\n",
    "#         if best_model_score > best_fitness_score:\n",
    "#             best_fitness_score = best_model_score\n",
    "#             stagnation_counter = 0  \n",
    "#         else:\n",
    "#             stagnation_counter += 1  \n",
    "\n",
    "#         model_scores.update(offspring_scores)\n",
    "#         generation += 1\n",
    "\n",
    "#     # âœ… Select Final Supermodel\n",
    "#     supermodel_name, supermodel_data = max(model_scores.items(), key=lambda x: evaluate_fitness({x[0]: x[1]})[0][x[0]])\n",
    "\n",
    "#     # âœ… Save Supermodel\n",
    "#     supermodel_path = os.path.join(SAVE_DIR, \"Supermodel.pkl\")\n",
    "#     with open(supermodel_path, \"wb\") as file:\n",
    "#         pickle.dump(supermodel_data, file)\n",
    "\n",
    "#     print(\"\\nðŸ† **Final Supermodel Selected:**\", supermodel_name)\n",
    "#     print(\"ðŸ“Š **Supermodel Performance Stats:**\", supermodel_data)\n",
    "#     print(f\"âœ… Supermodel saved at: {supermodel_path}\")\n",
    "\n",
    "#     return model_scores\n",
    "\n",
    "# # âœ… **Run AI Battle Royale**\n",
    "# final_results = run_evolution_pipeline(model_scores, generations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8a4cc3ea-1c53-4e2d-9fb5-822f48f5c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import random\n",
    "# import neat\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import pickle\n",
    "# import os\n",
    "# import re\n",
    "\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from sklearn.linear_model import ElasticNetCV, SGDRegressor\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.feature_selection import SelectKBest, f_classif\n",
    "# from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score\n",
    "\n",
    "# # âœ… Global Variables for Evolution\n",
    "# generation = 1\n",
    "# best_fitness_score = 1.0  \n",
    "# stagnation_counter = 0  \n",
    "# battle_log = []\n",
    "# stats_tracker = {}  \n",
    "# mutation_counter = {}  \n",
    "\n",
    "# SAVE_DIR = \"saved_models\"  \n",
    "# os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# # âœ… Assign unique identifiers for different model types\n",
    "# MODEL_IDS = {\n",
    "#     \"SGD\": \"S\", \"CNN (MLP)\": \"C\", \"Gradient Boosting\": \"G\", \n",
    "#     \"GA-Optimized LR\": \"GL\", \"Diffusion Model\": \"D\", \"Elastic Net\": \"E\",\n",
    "#     \"NeuroEvolution (NEAT)\": \"N\", \"Hybrid\": \"H\"\n",
    "# }\n",
    "\n",
    "# def assign_model_id(model_name):\n",
    "#     \"\"\"Assigns a unique identifier + a numeric index for each model.\"\"\"\n",
    "#     base_id = MODEL_IDS.get(model_name.split()[0], \"H\")  \n",
    "#     existing_ids = [int(name.split(\"-\")[-1]) for name in stats_tracker.keys() if name.startswith(base_id) and name.split(\"-\")[-1].isdigit()]\n",
    "#     next_id = max(existing_ids, default=0) + 1\n",
    "#     return f\"{base_id}-{next_id}\"\n",
    "\n",
    "# ### ðŸ“Œ **Step 1: Model Fitness Scoring**\n",
    "# def evaluate_fitness(model_scores):\n",
    "#     \"\"\"Computes weighted fitness scores based on model performance metrics.\"\"\"\n",
    "#     weights = {\"R2\": 0.4, \"MSE\": -0.3, \"ROC_AUC\": 0.5, \"Stability\": 0.2}\n",
    "    \n",
    "#     fitness_scores = {\n",
    "#         model: round(sum(weights[key] * scores.get(key, 0) for key in weights), 3)\n",
    "#         for model, scores in model_scores.items()\n",
    "#     }\n",
    "    \n",
    "#     sorted_models = sorted(fitness_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "#     return fitness_scores, sorted_models\n",
    "\n",
    "# ### ðŸ“Œ **Step 2: Model vs. Model Battles**\n",
    "# def battle_models(model1, model2, model_scores):\n",
    "#     \"\"\"Simulates a 1v1 battle between two models, winner survives to next gen.\"\"\"\n",
    "#     score1 = sum(model_scores[model1].values())\n",
    "#     score2 = sum(model_scores[model2].values())\n",
    "\n",
    "#     winner, loser = (model1, model2) if score1 > score2 else (model2, model1)\n",
    "#     margin = abs(score1 - score2)\n",
    "\n",
    "#     battle_log.append(f\"ðŸ”¥ {winner} defeated {loser} ({round(score1, 3)} vs. {round(score2, 3)}) | ðŸ† Margin: {round(margin, 3)}\")\n",
    "\n",
    "#     for model in [winner, loser]:\n",
    "#         if model not in stats_tracker:\n",
    "#             stats_tracker[model] = {\"Wins\": 0, \"Losses\": 0, \"Fitness History\": []}\n",
    "    \n",
    "#     stats_tracker[winner][\"Wins\"] += 1\n",
    "#     stats_tracker[loser][\"Losses\"] += 1\n",
    "#     stats_tracker[winner][\"Fitness History\"].append(score1)\n",
    "#     stats_tracker[loser][\"Fitness History\"].append(score2)\n",
    "\n",
    "#     return winner\n",
    "\n",
    "# ### ðŸ“Œ **Step 3: Breeding Function (Concatenated Hybrid Tags & Mutations)**\n",
    "# def breed_models(parent1, parent2, model_scores, gen, mutation_rate=0.1):\n",
    "#     \"\"\"Breeds two models, assigns proper hybrid name, and tracks mutations.\"\"\"\n",
    "#     child = {}\n",
    "\n",
    "#     def extract_fitness(score):\n",
    "#         \"\"\"Ensures we get a numeric fitness value from different types of models.\"\"\"\n",
    "#         if isinstance(score, dict):\n",
    "#             return sum(np.mean(v) if isinstance(v, list) else v for v in score.values() if isinstance(v, (int, float)))\n",
    "#         elif isinstance(score, neat.DefaultGenome):  \n",
    "#             return score.fitness if hasattr(score, \"fitness\") else 0\n",
    "#         else:\n",
    "#             return 0  \n",
    "\n",
    "#     parent1_score = extract_fitness(model_scores[parent1])\n",
    "#     parent2_score = extract_fitness(model_scores[parent2])\n",
    "\n",
    "#     global stagnation_counter\n",
    "#     if stagnation_counter >= 2:\n",
    "#         mutation_rate *= 1.5  \n",
    "\n",
    "#     # âœ… Hybrid Naming Format: H-(S-12 x C-7) (Gen 5)\n",
    "#     child_id = f\"H-({parent1} x {parent2}) (Gen {gen})\"\n",
    "\n",
    "#     # âœ… Mutation Counter\n",
    "#     mutation_count = mutation_counter.get(child_id, 0)\n",
    "\n",
    "#     # âœ… If mutated, append \"M\" + count\n",
    "#     if random.random() < mutation_rate:  \n",
    "#         mutation_counter[child_id] = mutation_count + 1\n",
    "#         child_id += f\" M{mutation_counter[child_id]}\"  \n",
    "#         battle_log.append(f\"ðŸ”¬ Mutation in {child_id}!\")\n",
    "\n",
    "#     # âœ… If either parent is a NEAT model, treat child as a simple fitness dictionary\n",
    "#     if isinstance(model_scores[parent1], neat.DefaultGenome) or isinstance(model_scores[parent2], neat.DefaultGenome):\n",
    "#         return child_id, {\"Fitness Score\": (parent1_score + parent2_score) / 2}\n",
    "\n",
    "#     all_keys = set(model_scores[parent1].keys()) | set(model_scores[parent2].keys())\n",
    "#     for key in all_keys:\n",
    "#         if key in model_scores[parent1] and key in model_scores[parent2]:\n",
    "#             child[key] = (model_scores[parent1][key] + model_scores[parent2][key]) / 2\n",
    "#         elif key in model_scores[parent1]:\n",
    "#             child[key] = model_scores[parent1][key]\n",
    "#         elif key in model_scores[parent2]:\n",
    "#             child[key] = model_scores[parent2][key]\n",
    "#         else:\n",
    "#             child[key] = 0  \n",
    "\n",
    "#     return child_id, child\n",
    "\n",
    "# ### ðŸ“Œ **Step 6: AI Evolution Pipeline**\n",
    "# def run_evolution_pipeline(model_scores, generations=3):\n",
    "#     \"\"\"Executes AI evolution where models fight, breed, and evolve over generations.\"\"\"\n",
    "#     global generation, best_fitness_score, stagnation_counter  \n",
    "\n",
    "#     for gen in range(generations):\n",
    "#         print(f\"\\nðŸš€ **Starting Generation {generation} (AI Evolution)** ðŸš€\")\n",
    "        \n",
    "#         fitness_scores, ranked_models = evaluate_fitness(model_scores)\n",
    "        \n",
    "#         survivors = evolutionary_tournament(model_scores, generation)\n",
    "        \n",
    "#         offspring = generate_hybrids(survivors, model_scores, generation)\n",
    "        \n",
    "#         offspring_scores = {child[0]: child[1] for child in offspring}\n",
    "#         hybrid_fitness_scores, _ = evaluate_fitness(offspring_scores)\n",
    "\n",
    "#         all_fitness_scores = {**fitness_scores, **hybrid_fitness_scores}\n",
    "\n",
    "#         print(\"\\nâš” **Battle Log:**\")\n",
    "#         for log in battle_log:\n",
    "#             print(log)\n",
    "\n",
    "#     return model_scores\n",
    "\n",
    "# ### ðŸ“Œ **Step 7: Translate Supermodel Lineage**\n",
    "# def translate_model_lineage(model_tag):\n",
    "#     \"\"\"\n",
    "#     Translates a structured model tag into a human-readable lineage description.\n",
    "#     \"\"\"\n",
    "#     model_names = {\n",
    "#         \"E\": \"Elastic Net\", \"S\": \"SGD\", \"G\": \"Gradient Boosting\",\n",
    "#         \"GL\": \"GA-Optimized LR\", \"D\": \"Diffusion Model\", \"C\": \"CNN (MLP)\",\n",
    "#         \"N\": \"NeuroEvolution (NEAT)\", \"H\": \"Hybrid\"\n",
    "#     }\n",
    "\n",
    "#     def parse_hybrid(expression):\n",
    "#         \"\"\"Recursively translates model expressions into human-readable form.\"\"\"\n",
    "#         if not \"(\" in expression:  \n",
    "#             model_type, number = expression.split(\"-\")\n",
    "#             return f\"{model_names.get(model_type, model_type)} Model #{number}\"\n",
    "        \n",
    "#         inner_match = re.search(r\"H-\\((.+)\\)\", expression)\n",
    "#         if inner_match:\n",
    "#             inner_expr = inner_match.group(1)\n",
    "#             parts = inner_expr.split(\" x \")\n",
    "#             translated_parts = [parse_hybrid(part) for part in parts]\n",
    "#             return f\"a hybrid of {', '.join(translated_parts)}\"\n",
    "        \n",
    "#         return expression\n",
    "\n",
    "#     return f\"This model is derived from {parse_hybrid(model_tag)}.\"\n",
    "\n",
    "# # âœ… **Run AI Evolution**\n",
    "# final_results = run_evolution_pipeline(model_scores, generations=10)\n",
    "\n",
    "# # âœ… **Supermodel Translation**\n",
    "# # # supermodel_tag = \"H-(((S-12 x C-7) x (C-6 x G-4)) x N-3) (Gen 3) M5\"\n",
    "# # print(translate_model_lineage(supermodel_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "6afcb942-15ee-490e-8b61-b6d86645c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def translate_model_lineage(model_tag):\n",
    "#     \"\"\"\n",
    "#     Translates a hybrid model tag into a human-readable lineage explanation.\n",
    "#     \"\"\"\n",
    "#     # Extract mutation count if present\n",
    "#     mutation_match = re.search(r\"M(\\d+)\", model_tag)\n",
    "#     mutation_count = int(mutation_match.group(1)) if mutation_match else 0\n",
    "    \n",
    "#     # Remove mutation info from the tag\n",
    "#     model_tag = re.sub(r\"\\s*M\\d+\", \"\", model_tag)\n",
    "    \n",
    "#     # Extract generation info\n",
    "#     generation_match = re.search(r\"Gen (\\d+)\", model_tag)\n",
    "#     generation = int(generation_match.group(1)) if generation_match else \"Unknown\"\n",
    "    \n",
    "#     # Remove generation info from the tag\n",
    "#     model_tag = re.sub(r\"\\s*Gen \\d+\", \"\", model_tag)\n",
    "    \n",
    "#     # Mapping model abbreviations to full names\n",
    "#     model_names = {\n",
    "#         \"E\": \"Elastic Net\",\n",
    "#         \"S\": \"SGD\",\n",
    "#         \"G\": \"Gradient Boosting\",\n",
    "#         \"C\": \"CNN (MLP Stand-in)\",\n",
    "#         \"D\": \"Diffusion Model\",\n",
    "#         \"L\": \"GA-Optimized Logistic Regression\",\n",
    "#         \"N\": \"NeuroEvolution (NEAT)\"\n",
    "#     }\n",
    "\n",
    "#     def parse_hybrid(expression):\n",
    "#         \"\"\"Recursively translates model expressions into human-readable form.\"\"\"\n",
    "#         # Base case: If it's a single model\n",
    "#         if not \"(\" in expression:\n",
    "#             model_type, number = expression.split(\"-\")\n",
    "#             return f\"{model_names.get(model_type, model_type)} Model #{number}\"\n",
    "        \n",
    "#         # Recursively break down hybrids\n",
    "#         inner_match = re.search(r\"H-\\((.+)\\)\", expression)\n",
    "#         if inner_match:\n",
    "#             inner_expr = inner_match.group(1)\n",
    "#             parts = inner_expr.split(\" x \")\n",
    "#             translated_parts = [parse_hybrid(part) for part in parts]\n",
    "#             return f\"a hybrid combining {', '.join(translated_parts)}\"\n",
    "        \n",
    "#         return expression  # Default case (shouldn't happen)\n",
    "\n",
    "#     # Process the model tag\n",
    "#     human_readable_description = parse_hybrid(model_tag)\n",
    "\n",
    "#     # Format final output\n",
    "#     output = f\"This model is a {generation}-generation hybrid, derived from {human_readable_description}.\"\n",
    "#     if mutation_count > 0:\n",
    "#         output += f\" It has undergone {mutation_count} mutations, enhancing its adaptability.\"\n",
    "\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "fec4ef99-0e39-4eb6-9c40-5d36956befd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_tag = \"H-(((S-12 x C-7) x (C-6 x G-4)) x N-3) (Gen 3) M5\"\n",
    "# print(translate_model_lineage(model_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "22979e42-6b64-4e65-8f78-3195dcd69e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import random\n",
    "# import neat\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from sklearn.linear_model import ElasticNetCV, SGDRegressor\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.neural_network import MLPClassifier  # CNN Stand-in\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.feature_selection import SelectKBest, f_classif\n",
    "# from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "52bae746-9b85-4a60-a211-5765184a3d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### ðŸ“Œ **Step 1: Preprocess Data**\n",
    "# def preprocess_data(df):\n",
    "#     \"\"\"Handles missing values, computes market stress, and creates lagged features.\"\"\"\n",
    "#     threshold = 0.3 * len(df)\n",
    "#     df_cleaned = df.dropna(axis=1, thresh=threshold)\n",
    "#     df_cleaned.fillna(df_cleaned.median(numeric_only=True), inplace=True)\n",
    "\n",
    "#     # âœ… Compute rolling z-scores\n",
    "#     def compute_rolling_zscores(df, cols, window=90):\n",
    "#         rolling_mean = df[cols].rolling(window=window, min_periods=1).mean()\n",
    "#         rolling_std = df[cols].rolling(window=window, min_periods=1).std()\n",
    "#         return (df[cols] - rolling_mean) / rolling_std\n",
    "\n",
    "#     zscore_cols = [\"inflation\", \"Interest Rate\", \"interest rates\"]\n",
    "#     df_zscores = compute_rolling_zscores(df_cleaned, zscore_cols)\n",
    "#     df_zscores.columns = [f\"{col}_z\" for col in zscore_cols]\n",
    "#     df_cleaned = pd.concat([df_cleaned, df_zscores], axis=1)\n",
    "\n",
    "#     # âœ… Define market stress periods\n",
    "#     df_cleaned[\"spike\"] = ((df_cleaned[\"inflation_z\"] > 1) &\n",
    "#                             (df_cleaned[\"Interest Rate_z\"] > 1) &\n",
    "#                             (df_cleaned[\"interest rates_z\"] > 1)).astype(int)\n",
    "#     df_cleaned[\"market_stress\"] = df_cleaned[\"spike\"]\n",
    "\n",
    "#     # âœ… Create lagged features\n",
    "#     lag_features = [\"inflation\", \"Interest Rate\", \"interest rates\"]\n",
    "#     lags = [5, 10, 30]\n",
    "#     for feature in lag_features:\n",
    "#         for lag in lags:\n",
    "#             df_cleaned[f\"{feature}_lag{lag}\"] = df_cleaned[feature].shift(lag)\n",
    "\n",
    "#     df_cleaned.dropna(inplace=True)\n",
    "#     return df_cleaned\n",
    "\n",
    "# ### ðŸ“Œ **Step 2: Scale Features**\n",
    "# def scale_features(df_cleaned):\n",
    "#     \"\"\"Scales numerical features and returns the full-feature dataset `df_scaled`.\"\"\"\n",
    "#     scaler = StandardScaler()\n",
    "#     num_cols = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "#     num_cols.remove(\"market_stress\")\n",
    "    \n",
    "#     df_scaled_features = pd.DataFrame(scaler.fit_transform(df_cleaned[num_cols]), columns=num_cols)\n",
    "#     df_scaled = pd.concat([df_scaled_features, df_cleaned[[\"market_stress\"]].reset_index(drop=True)], axis=1)\n",
    "#     df_scaled.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "#     df_scaled.fillna(df_scaled.median(), inplace=True)\n",
    "    \n",
    "#     return df_scaled\n",
    "\n",
    "# ### ðŸ“Œ **Step 3: Apply PCA**\n",
    "# def apply_pca(df_scaled, n_components=50):\n",
    "#     \"\"\"Applies PCA to reduce dimensions while retaining `market_stress`.\"\"\"\n",
    "#     df_pca_input = df_scaled.drop(columns=[\"market_stress\"]).copy()\n",
    "#     df_pca_input.dropna(inplace=True)\n",
    "\n",
    "#     if df_pca_input.isna().sum().sum() == 0:\n",
    "#         pca = PCA(n_components=min(n_components, df_pca_input.shape[1]))\n",
    "#         principal_components = pca.fit_transform(df_pca_input)\n",
    "\n",
    "#         df_pca = pd.DataFrame(principal_components, columns=[f\"PC{i+1}\" for i in range(pca.n_components_)])\n",
    "#         df_pca[\"market_stress\"] = df_scaled[\"market_stress\"].iloc[df_pca.index].reset_index(drop=True)\n",
    "#         return df_pca\n",
    "#     else:\n",
    "#         raise ValueError(\"âŒ ERROR: NaN values exist in df_pca_input before PCA!\")\n",
    "\n",
    "# ### ðŸ“Œ **Step 4: Select Top Features**\n",
    "# def select_top_features(df_scaled, df_pca, k=15):\n",
    "#     \"\"\"Selects top `k` most predictive features for models that do not use PCA.\"\"\"\n",
    "#     selector = SelectKBest(score_func=f_classif, k=min(k, df_scaled.shape[1]))\n",
    "#     X_selected = selector.fit_transform(df_scaled.drop(columns=[\"market_stress\"]), df_pca[\"market_stress\"])\n",
    "#     selected_features = df_scaled.drop(columns=[\"market_stress\"]).columns[selector.get_support()]\n",
    "    \n",
    "#     return selected_features\n",
    "\n",
    "# ### ðŸ“Œ **Step 5: Train-Test Split**\n",
    "# def split_data(df_pca):\n",
    "#     \"\"\"Splits the PCA-transformed dataset into training and testing sets.\"\"\"\n",
    "#     X_scaled = df_pca.drop(columns=[\"market_stress\"])\n",
    "#     y = df_pca[\"market_stress\"]\n",
    "#     return train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ### ðŸ“Œ **Step 6: Train & Evaluate Models**\n",
    "# def train_models(X_train, X_test, y_train, y_test):\n",
    "#     \"\"\"Trains multiple models and logs performance metrics.\"\"\"\n",
    "#     models = {\n",
    "#         \"Elastic Net\": ElasticNetCV(cv=5, random_state=42),\n",
    "#         \"SGD\": SGDRegressor(max_iter=2000, tol=1e-4, random_state=42),\n",
    "#         \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100),\n",
    "#         \"CNN (MLP Stand-in)\": MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42),\n",
    "#     }\n",
    "\n",
    "#     model_scores = {}\n",
    "#     for name, model in models.items():\n",
    "#         start_time = time.time()\n",
    "#         model.fit(X_train, y_train)\n",
    "#         training_time = time.time() - start_time\n",
    "\n",
    "#         start_time = time.time()\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         prediction_time = time.time() - start_time\n",
    "\n",
    "#         model_scores[name] = {\n",
    "#             \"R2\": r2_score(y_test, y_pred),\n",
    "#             \"MSE\": mean_squared_error(y_test, y_pred),\n",
    "#             \"ROC_AUC\": roc_auc_score(y_test, y_pred),\n",
    "#             \"Stability\": np.mean(cross_val_score(model, X_train, y_train, cv=5)),\n",
    "#             \"Training Time (s)\": training_time,\n",
    "#             \"Prediction Time (s)\": prediction_time\n",
    "#         }\n",
    "\n",
    "#         print(f\"âœ… {name} Training: {training_time:.2f} sec | Prediction: {prediction_time:.5f} sec\")\n",
    "\n",
    "#     return model_scores\n",
    "\n",
    "# ### ðŸ“Œ **Step 7: Run Full Pipeline**\n",
    "# def run_pipeline(df):\n",
    "#     \"\"\"Executes the full ML pipeline and returns model performance metrics.\"\"\"\n",
    "#     df_cleaned = preprocess_data(df)\n",
    "#     df_scaled = scale_features(df_cleaned)\n",
    "#     df_pca = apply_pca(df_scaled, n_components=50)\n",
    "#     selected_features = select_top_features(df_scaled, df_pca, k=15)\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = split_data(df_pca)\n",
    "#     model_scores = train_models(X_train, X_test, y_train, y_test)\n",
    "\n",
    "#     df_model_performance = pd.DataFrame.from_dict(model_scores, orient=\"index\")\n",
    "#     print(\"\\nðŸ“Š Updated Model Performance Summary:\")\n",
    "#     print(df_model_performance)\n",
    "    \n",
    "#     return df_model_performance\n",
    "\n",
    "# # âœ… **Run the Full ML Pipeline**\n",
    "# df = pd.read_csv(\"data/financial_data_cleaned2.csv\")  # Load actual dataset\n",
    "# results = run_pipeline(df)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "f06fc026-65fc-45bf-babf-1369434d6f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import random\n",
    "# import neat\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from sklearn.linear_model import ElasticNetCV, SGDRegressor, LogisticRegression\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.neural_network import MLPClassifier  # CNN Stand-in\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.feature_selection import SelectKBest, f_classif\n",
    "# from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score\n",
    "# from sklearn.mixture import GaussianMixture  # Diffusion Model\n",
    "# from deap import base, creator, tools, algorithms  # GA-LR\n",
    "\n",
    "# # -----------------------------------------------\n",
    "# # ðŸ“Œ STEP 1: PREPROCESS DATA\n",
    "# # -----------------------------------------------\n",
    "\n",
    "# def preprocess_data(df):\n",
    "#     \"\"\"Handles missing values, computes market stress, and creates lagged features.\"\"\"\n",
    "#     threshold = 0.3 * len(df)\n",
    "#     df_cleaned = df.dropna(axis=1, thresh=threshold)\n",
    "#     df_cleaned.fillna(df_cleaned.median(numeric_only=True), inplace=True)\n",
    "\n",
    "#     # âœ… Compute rolling z-scores\n",
    "#     def compute_rolling_zscores(df, cols, window=90):\n",
    "#         rolling_mean = df[cols].rolling(window=window, min_periods=1).mean()\n",
    "#         rolling_std = df[cols].rolling(window=window, min_periods=1).std()\n",
    "#         return (df[cols] - rolling_mean) / rolling_std\n",
    "\n",
    "#     zscore_cols = [\"inflation\", \"Interest Rate\", \"interest rates\"]\n",
    "#     df_zscores = compute_rolling_zscores(df_cleaned, zscore_cols)\n",
    "#     df_zscores.columns = [f\"{col}_z\" for col in zscore_cols]\n",
    "#     df_cleaned = pd.concat([df_cleaned, df_zscores], axis=1)\n",
    "\n",
    "#     # âœ… Define market stress periods\n",
    "#     df_cleaned[\"market_stress\"] = ((df_cleaned[\"inflation_z\"] > 1) &\n",
    "#                                    (df_cleaned[\"Interest Rate_z\"] > 1) &\n",
    "#                                    (df_cleaned[\"interest rates_z\"] > 1)).astype(int)\n",
    "\n",
    "#     # âœ… Create lagged features\n",
    "#     lag_features = [\"inflation\", \"Interest Rate\", \"interest rates\"]\n",
    "#     lags = [5, 10, 30]\n",
    "#     for feature in lag_features:\n",
    "#         for lag in lags:\n",
    "#             df_cleaned[f\"{feature}_lag{lag}\"] = df_cleaned[feature].shift(lag)\n",
    "\n",
    "#     df_cleaned.dropna(inplace=True)\n",
    "#     return df_cleaned\n",
    "\n",
    "# # -----------------------------------------------\n",
    "# # ðŸ“Œ STEP 2: SCALE FEATURES\n",
    "# # -----------------------------------------------\n",
    "\n",
    "# def scale_features(df_cleaned):\n",
    "#     \"\"\"Scales numerical features.\"\"\"\n",
    "#     scaler = StandardScaler()\n",
    "#     num_cols = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "#     num_cols.remove(\"market_stress\")\n",
    "\n",
    "#     df_scaled = pd.DataFrame(scaler.fit_transform(df_cleaned[num_cols]), columns=num_cols)\n",
    "#     df_scaled[\"market_stress\"] = df_cleaned[\"market_stress\"].values\n",
    "\n",
    "#     return df_scaled\n",
    "\n",
    "# # -----------------------------------------------\n",
    "# # ðŸ“Œ STEP 3: APPLY PCA\n",
    "# # -----------------------------------------------\n",
    "\n",
    "# def apply_pca(df_scaled, n_components=50):\n",
    "#     \"\"\"Applies PCA for dimensionality reduction.\"\"\"\n",
    "#     df_pca_input = df_scaled.drop(columns=[\"market_stress\"])\n",
    "#     pca = PCA(n_components=min(n_components, df_pca_input.shape[1]))\n",
    "#     principal_components = pca.fit_transform(df_pca_input)\n",
    "\n",
    "#     df_pca = pd.DataFrame(principal_components, columns=[f\"PC{i+1}\" for i in range(pca.n_components_)])\n",
    "#     df_pca[\"market_stress\"] = df_scaled[\"market_stress\"].values\n",
    "#     return df_pca\n",
    "\n",
    "# # -----------------------------------------------\n",
    "# # ðŸ“Œ STEP 4: SPLIT DATA\n",
    "# # -----------------------------------------------\n",
    "\n",
    "# def split_data(df):\n",
    "#     \"\"\"Splits dataset into training/testing sets.\"\"\"\n",
    "#     X = df.drop(columns=[\"market_stress\"])\n",
    "#     y = df[\"market_stress\"]\n",
    "#     return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # -----------------------------------------------\n",
    "# # ðŸ“Œ STEP 5: TRAIN MODELS\n",
    "# # -----------------------------------------------\n",
    "\n",
    "# def train_elastic_net(X_train, X_test, y_train, y_test):\n",
    "#     model = ElasticNetCV(cv=5, random_state=42)\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     return roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# def train_sgd(X_train, X_test, y_train, y_test):\n",
    "#     model = SGDRegressor(max_iter=2000, tol=1e-4, random_state=42)\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     return roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# def train_gradient_boosting(X_train, X_test, y_train, y_test):\n",
    "#     model = GradientBoostingClassifier(n_estimators=100)\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     return roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# def train_cnn(X_train, X_test, y_train, y_test):\n",
    "#     model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     return roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# def train_diffusion_model(X_train, X_test, y_train, y_test):\n",
    "#     model = GaussianMixture(n_components=5, covariance_type='full', random_state=42)\n",
    "#     model.fit(X_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     return roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# # -----------------------------------------------\n",
    "# # ðŸ“Œ STEP 6: TRAIN ALL MODELS\n",
    "# # -----------------------------------------------\n",
    "\n",
    "# def train_all_models(X_train, X_test, y_train, y_test, X_train_full, X_test_full, y_train_full, y_test_full):\n",
    "#     \"\"\"Trains all models, ensuring NEAT uses the full dataset.\"\"\"\n",
    "#     models = {\n",
    "#         \"Elastic Net\": train_elastic_net,\n",
    "#         \"SGD\": train_sgd,\n",
    "#         \"Gradient Boosting\": train_gradient_boosting,\n",
    "#         \"CNN (MLP)\": train_cnn,\n",
    "#         \"Diffusion Model\": train_diffusion_model,\n",
    "#         \"NEAT\": lambda *_: train_neat(X_train_full, X_test_full, y_train_full, y_test_full),  # Full dataset for NEAT\n",
    "#         \"GA-Optimized LR\": train_ga_lr\n",
    "#     }\n",
    "\n",
    "#     results = {name: model(X_train, X_test, y_train, y_test) for name, model in models.items()}\n",
    "#     return results\n",
    "\n",
    "# # -----------------------------------------------\n",
    "# # ðŸ“Œ STEP 7: RUN FULL PIPELINE\n",
    "# # -----------------------------------------------\n",
    "\n",
    "# def run_pipeline(df):\n",
    "#     \"\"\"Executes the full ML pipeline, ensuring NEAT uses full features.\"\"\"\n",
    "    \n",
    "#     # âœ… Step 1: Preprocess & Scale Data\n",
    "#     df_cleaned = preprocess_data(df)\n",
    "#     df_scaled = scale_features(df_cleaned)\n",
    "\n",
    "#     # âœ… Step 2: Apply PCA for standard models\n",
    "#     df_pca = apply_pca(df_scaled, n_components=50)\n",
    "\n",
    "#     # âœ… Step 3: Train-Test Splits\n",
    "#     X_train, X_test, y_train, y_test = split_data(df_pca)  # PCA-reduced\n",
    "#     X_train_full, X_test_full, y_train_full, y_test_full = split_data(df_scaled)  # Full dataset for NEAT\n",
    "\n",
    "#     # âœ… Step 4: Train All Models\n",
    "#     model_scores = train_all_models(X_train, X_test, y_train, y_test, X_train_full, X_test_full, y_train_full, y_test_full)\n",
    "\n",
    "#     # âœ… Step 5: Output Results\n",
    "#     df_model_performance = pd.DataFrame.from_dict(model_scores, orient=\"index\")\n",
    "#     print(\"\\nðŸ“Š Updated Model Performance Summary:\")\n",
    "#     print(df_model_performance)\n",
    "\n",
    "#     return df_model_performance\n",
    "\n",
    "# # âœ… RUN FULL PIPELINE\n",
    "# df = pd.read_csv(\"data/financial_data_cleaned2.csv\")  # Load actual dataset\n",
    "# results = run_pipeline(df)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "a22a2823-6c33-46f0-acd9-ef4b3c0f4762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ **Starting Generation 1 (AI Battle Royale)** ðŸš€\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'R2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[714], line 180\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sorted_all_models\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# âœ… **Run AI Battle Royale for 3 Generations**\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m final_results \u001b[38;5;241m=\u001b[39m run_evolution_pipeline(model_scores, generations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[714], line 130\u001b[0m, in \u001b[0;36mrun_evolution_pipeline\u001b[0;34m(model_scores, generations)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸš€ **Starting Generation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (AI Battle Royale)** ðŸš€\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# âœ… Evaluate Fitness of Existing Models\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m fitness_scores, ranked_models \u001b[38;5;241m=\u001b[39m evaluate_fitness(model_scores)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# âœ… Run Tournament Battles\u001b[39;00m\n\u001b[1;32m    133\u001b[0m survivors \u001b[38;5;241m=\u001b[39m evolutionary_tournament(model_scores, generation)\n",
      "Cell \u001b[0;32mIn[714], line 29\u001b[0m, in \u001b[0;36mevaluate_fitness\u001b[0;34m(model_scores)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Computes weighted fitness scores based on model performance metrics.\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m weights \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROC_AUC\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStability\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.2\u001b[39m}\n\u001b[0;32m---> 29\u001b[0m fitness_scores \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     30\u001b[0m     model: \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28msum\u001b[39m(weights[key] \u001b[38;5;241m*\u001b[39m scores[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m weights), \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model, scores \u001b[38;5;129;01min\u001b[39;00m model_scores\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     32\u001b[0m }\n\u001b[1;32m     34\u001b[0m sorted_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(fitness_scores\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fitness_scores, sorted_models\n",
      "Cell \u001b[0;32mIn[714], line 30\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Computes weighted fitness scores based on model performance metrics.\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m weights \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROC_AUC\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStability\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.2\u001b[39m}\n\u001b[1;32m     29\u001b[0m fitness_scores \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 30\u001b[0m     model: \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28msum\u001b[39m(weights[key] \u001b[38;5;241m*\u001b[39m scores[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m weights), \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model, scores \u001b[38;5;129;01min\u001b[39;00m model_scores\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     32\u001b[0m }\n\u001b[1;32m     34\u001b[0m sorted_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(fitness_scores\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fitness_scores, sorted_models\n",
      "Cell \u001b[0;32mIn[714], line 30\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Computes weighted fitness scores based on model performance metrics.\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m weights \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROC_AUC\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStability\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.2\u001b[39m}\n\u001b[1;32m     29\u001b[0m fitness_scores \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 30\u001b[0m     model: \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28msum\u001b[39m(weights[key] \u001b[38;5;241m*\u001b[39m scores[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m weights), \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model, scores \u001b[38;5;129;01min\u001b[39;00m model_scores\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     32\u001b[0m }\n\u001b[1;32m     34\u001b[0m sorted_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(fitness_scores\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fitness_scores, sorted_models\n",
      "\u001b[0;31mKeyError\u001b[0m: 'R2'"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import random\n",
    "# import neat\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from sklearn.linear_model import ElasticNetCV, SGDRegressor\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.neural_network import MLPClassifier  # CNN Stand-in\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.feature_selection import SelectKBest, f_classif\n",
    "# from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score\n",
    "\n",
    "# # âœ… Global Variables for Evolution\n",
    "# generation = 1  \n",
    "# best_fitness_score = 1.0  # Start at a **real** value (1.0 instead of -inf)\n",
    "# stagnation_counter = 0  \n",
    "# battle_log = []\n",
    "\n",
    "# ### ðŸ“Œ **Step 1: Model Fitness Scoring**\n",
    "# def evaluate_fitness(model_scores):\n",
    "#     \"\"\"Computes weighted fitness scores based on model performance metrics.\"\"\"\n",
    "#     weights = {\"R2\": 0.4, \"MSE\": -0.3, \"ROC_AUC\": 0.5, \"Stability\": 0.2}\n",
    "    \n",
    "#     fitness_scores = {\n",
    "#         model: round(sum(weights[key] * scores[key] for key in weights), 3)\n",
    "#         for model, scores in model_scores.items()\n",
    "#     }\n",
    "    \n",
    "#     sorted_models = sorted(fitness_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "#     return fitness_scores, sorted_models\n",
    "\n",
    "# ### ðŸ“Œ **Step 2: Model vs. Model Battles**\n",
    "# def battle_models(model1, model2, model_scores):\n",
    "#     \"\"\"Simulates a 1v1 battle between two models, winner survives to next gen.\"\"\"\n",
    "#     score1 = sum(model_scores[model1].values())\n",
    "#     score2 = sum(model_scores[model2].values())\n",
    "\n",
    "#     # Winner = Model with Higher Fitness Score\n",
    "#     winner, loser = (model1, model2) if score1 > score2 else (model2, model1)\n",
    "    \n",
    "#     # Log battle result\n",
    "#     battle_log.append(f\"ðŸ”¥ {winner} defeated {loser} ({round(score1, 3)} vs. {round(score2, 3)})\")\n",
    "    \n",
    "#     return winner\n",
    "\n",
    "# ### ðŸ“Œ **Step 3: Adaptive Breeding Function**\n",
    "# def breed_models(parent1, parent2, model_scores, gen, mutation_rate=0.1):\n",
    "#     \"\"\"Breeds two models, labels child, and applies adaptive mutations.\"\"\"\n",
    "#     child = {}\n",
    "\n",
    "#     parent1_score = sum(model_scores[parent1].values())\n",
    "#     parent2_score = sum(model_scores[parent2].values())\n",
    "#     fitness_diff = abs(parent1_score - parent2_score)\n",
    "\n",
    "#     global stagnation_counter\n",
    "#     if stagnation_counter >= 2:\n",
    "#         mutation_rate *= 1.5  # Increase mutation rate if no improvement\n",
    "\n",
    "#     # Ensure child gets all required keys\n",
    "#     all_keys = set(model_scores[parent1].keys()) | set(model_scores[parent2].keys())\n",
    "\n",
    "#     for key in all_keys:\n",
    "#         if key in model_scores[parent1] and key in model_scores[parent2]:\n",
    "#             child[key] = (model_scores[parent1][key] + model_scores[parent2][key]) / 2\n",
    "#         elif key in model_scores[parent1]:\n",
    "#             child[key] = model_scores[parent1][key]\n",
    "#         elif key in model_scores[parent2]:\n",
    "#             child[key] = model_scores[parent2][key]\n",
    "#         else:\n",
    "#             child[key] = 0  # Default if both parents lack the key\n",
    "\n",
    "#         # Adaptive Mutation\n",
    "#         if random.random() < mutation_rate:  \n",
    "#             child[key] += random.uniform(-0.05, 0.10)  \n",
    "\n",
    "#     child_label = f\"Child of {parent1.split()[0]} & {parent2.split()[0]} (Gen {gen})\"\n",
    "#     return child_label, child\n",
    "\n",
    "# ### ðŸ“Œ **Step 4: Evolutionary Tournament**\n",
    "# def evolutionary_tournament(model_scores, gen):\n",
    "#     \"\"\"Tournament-style survival: Models fight, winners survive, losers get eliminated.\"\"\"\n",
    "#     global battle_log\n",
    "#     battle_log = []\n",
    "    \n",
    "#     models = list(model_scores.keys())\n",
    "#     random.shuffle(models)  \n",
    "#     survivors = []\n",
    "\n",
    "#     # 1v1 Battles\n",
    "#     while len(models) > 1:\n",
    "#         m1, m2 = models.pop(), models.pop()\n",
    "#         winner = battle_models(m1, m2, model_scores)\n",
    "#         survivors.append(winner)\n",
    "    \n",
    "#     if models:  \n",
    "#         survivors.append(models[0])  \n",
    "    \n",
    "#     return survivors\n",
    "\n",
    "# ### ðŸ“Œ **Step 5: Create Hybrid Models**\n",
    "# def generate_hybrids(survivors, model_scores, gen):\n",
    "#     \"\"\"Generates hybrid models from top survivors.\"\"\"\n",
    "#     offspring = []\n",
    "    \n",
    "#     for _ in range(2):  \n",
    "#         p1, p2 = random.sample(survivors, 2)\n",
    "#         child_label, child_model = breed_models(p1, p2, model_scores, gen)\n",
    "#         offspring.append((child_label, child_model))\n",
    "    \n",
    "#     return offspring\n",
    "\n",
    "# ### ðŸ“Œ **Step 6: AI Battle Royale Pipeline**\n",
    "# def run_evolution_pipeline(model_scores, generations=3):\n",
    "#     \"\"\"\n",
    "#     Executes AI battle royale pipeline where models fight for survival,\n",
    "#     breed the winners, and evolve over generations.\n",
    "#     \"\"\"\n",
    "#     global generation, best_fitness_score, stagnation_counter  \n",
    "\n",
    "#     for gen in range(generations):\n",
    "#         print(f\"\\nðŸš€ **Starting Generation {generation} (AI Battle Royale)** ðŸš€\")\n",
    "        \n",
    "#         # âœ… Evaluate Fitness of Existing Models\n",
    "#         fitness_scores, ranked_models = evaluate_fitness(model_scores)\n",
    "        \n",
    "#         # âœ… Run Tournament Battles\n",
    "#         survivors = evolutionary_tournament(model_scores, generation)\n",
    "        \n",
    "#         # âœ… Generate Hybrid Models from Survivors\n",
    "#         offspring = generate_hybrids(survivors, model_scores, generation)\n",
    "        \n",
    "#         # âœ… Evaluate Hybrid Models\n",
    "#         weights = {\"R2\": 0.4, \"MSE\": -0.3, \"ROC_AUC\": 0.5, \"Stability\": 0.2}\n",
    "#         hybrid_fitness_scores = evaluate_fitness(dict(offspring))[0]\n",
    "        \n",
    "#         # âœ… Combine All Scores\n",
    "#         all_fitness_scores = {**fitness_scores, **hybrid_fitness_scores}\n",
    "#         sorted_all_models = sorted(all_fitness_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "#         # âœ… Log Tournament Results\n",
    "#         print(\"\\nâš” **Battle Log:**\")\n",
    "#         for log in battle_log:\n",
    "#             print(log)\n",
    "\n",
    "#         # âœ… Identify Best Model & Improvement\n",
    "#         best_model = sorted_all_models[0]\n",
    "#         improvement = round(best_model[1] - best_fitness_score, 3)\n",
    "\n",
    "#         # âœ… Update best fitness score & handle stagnation\n",
    "#         if best_model[1] > best_fitness_score:\n",
    "#             best_fitness_score = best_model[1]\n",
    "#             stagnation_counter = 0  \n",
    "#         else:\n",
    "#             stagnation_counter += 1  \n",
    "\n",
    "#         # âœ… **SUDDEN DEATH MUTATION:** If 3 rounds have no improvement, introduce a totally random model\n",
    "#         if stagnation_counter >= 3:\n",
    "#             print(\"\\nðŸ’€ **Stagnation detected! Introducing a new random model!**\")\n",
    "#             random_model_label = f\"Random Mutant (Gen {generation})\"\n",
    "#             random_model_scores = {key: random.uniform(0, 1) for key in weights.keys()}\n",
    "#             model_scores[random_model_label] = random_model_scores\n",
    "#             stagnation_counter = 0  # Reset stagnation\n",
    "\n",
    "#         print(f\"\\nðŸ† **Generation {generation} Summary:**\")\n",
    "#         print(f\"   ðŸ“ˆ Best Model: **{best_model[0]}** (Fitness: {best_model[1]})\")\n",
    "#         print(f\"   ðŸ”„ Improvement: {improvement} vs. previous generation\")\n",
    "\n",
    "#         model_scores.update(dict(offspring))\n",
    "#         generation += 1\n",
    "\n",
    "#     return sorted_all_models\n",
    "\n",
    "# # âœ… **Run AI Battle Royale for 3 Generations**\n",
    "# final_results = run_evolution_pipeline(model_scores, generations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79fc9e1-d81b-40ec-bc89-e96339d7b073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ffdba-3a05-47ac-94e9-4d03d49c9525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa11785-e406-4b05-95f1-888fdd9903b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0816b5-af22-432d-9803-919d82a20dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174c5546-3076-403b-8ffb-a2e4a34f13bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
