{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f7eae896-e7e0-4965-b560-91406fc96cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Feature Names: 214 features\n",
      "✅ Feature preprocessing and scaling completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # ✅ Define Paths\n",
    "# SAVE_DIR = \"saved_models\"\n",
    "# FEATURE_FILE = os.path.join(SAVE_DIR, \"feature_names.json\")\n",
    "\n",
    "# # -----------------------------------------------\n",
    "# # 📌 Data Preprocessing Function (Fixing String to Float Issue)\n",
    "# # -----------------------------------------------\n",
    "# def preprocess_data(df):\n",
    "#     \"\"\"Ensures all data is numeric by converting dates and categorical features.\"\"\"\n",
    "    \n",
    "#     df = df.copy()\n",
    "#     df = df.dropna()  # ✅ Remove missing values\n",
    "\n",
    "#     for col in df.select_dtypes(include=[\"object\"]):\n",
    "#         try:\n",
    "#             df[col] = pd.to_datetime(df[col]).astype(int) / 10**9  # ✅ Convert to Unix timestamp\n",
    "#         except Exception:\n",
    "#             df[col] = df[col].astype(\"category\").cat.codes  # ✅ Convert categorical to numeric\n",
    "\n",
    "#     return df\n",
    "\n",
    "# # ✅ Load Feature Names\n",
    "# if os.path.exists(FEATURE_FILE):\n",
    "#     with open(FEATURE_FILE, \"r\") as f:\n",
    "#         expected_features = json.load(f)\n",
    "#     print(f\"✅ Loaded Feature Names: {len(expected_features)} features\")\n",
    "# else:\n",
    "#     raise FileNotFoundError(\"❌ Feature names file not found!\")\n",
    "\n",
    "# # ✅ Load Dataset and Standardize Features\n",
    "# df = pd.read_csv(\"data/synth_findata.csv\")\n",
    "# df = preprocess_data(df)  # ✅ Apply Fix\n",
    "\n",
    "# X = df.drop(columns=[\"market_stress\"], errors=\"ignore\")  # Ensure \"market_stress\" exists\n",
    "# X = X[expected_features]  # ✅ Ensure feature alignment\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# print(\"✅ Feature preprocessing and scaling completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b582894a-e301-4bbb-bce1-60928b2a093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Feature Names: 214 features\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2023-01-09'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bq/_yk5zsn90t11g7lt1yn4v9w40000gn/T/ipykernel_18936/3161230106.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# ✅ Scale Features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mX_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mX_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpected_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# ✅ Load Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             return_tuple = (\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 )\n\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \"\"\"\n\u001b[1;32m    876\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1469\u001b[0m                 skip_parameter_validation=(\n\u001b[1;32m   1470\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m         \"\"\"\n\u001b[1;32m    913\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1009\u001b[0m                         )\n\u001b[1;32m   1010\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m                 raise ValueError(\n\u001b[1;32m   1015\u001b[0m                     \u001b[0;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m                 ) from complex_warning\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m   2149\u001b[0m     def __array__(\n\u001b[1;32m   2150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m     ) -> np.ndarray:\n\u001b[1;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2153\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2154\u001b[0m         if (\n\u001b[1;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2023-01-09'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ✅ Define Paths\n",
    "SAVE_DIR = \"saved_models\"\n",
    "FEATURE_FILE = os.path.join(SAVE_DIR, \"feature_names.json\")\n",
    "DATA_FILE = \"data/synth_findata.csv\"\n",
    "PERFORMANCE_FILE = os.path.join(SAVE_DIR, \"Model_Performance_Metrics_Updated.csv\")\n",
    "\n",
    "# ✅ Load Feature Names\n",
    "if not os.path.exists(FEATURE_FILE):\n",
    "    raise FileNotFoundError(\"❌ Feature names file not found!\")\n",
    "\n",
    "with open(FEATURE_FILE, \"r\") as f:\n",
    "    expected_features = json.load(f)\n",
    "\n",
    "expected_features = [str(f) for f in expected_features]  # Ensure all feature names are strings\n",
    "print(f\"✅ Loaded Feature Names: {len(expected_features)} features\")\n",
    "\n",
    "# ✅ Load Dataset\n",
    "df = pd.read_csv(DATA_FILE).dropna()\n",
    "X = df.drop(columns=[\"market_stress\"], errors=\"ignore\")\n",
    "y = df[\"market_stress\"]\n",
    "\n",
    "# ✅ Ensure Feature Alignment\n",
    "def align_features(X, model):\n",
    "    \"\"\"Dynamically aligns feature count to match model expectations.\"\"\"\n",
    "    X = X.copy()\n",
    "    if hasattr(model, \"n_features_in_\"):\n",
    "        expected_count = model.n_features_in_\n",
    "        current_features = list(X.columns.astype(str))\n",
    "\n",
    "        if len(current_features) < expected_count:\n",
    "            missing_cols = expected_count - len(current_features)\n",
    "            print(f\"⚠️ Adding {missing_cols} dummy columns to match model input size.\")\n",
    "            for i in range(missing_cols):\n",
    "                X[f\"dummy_feature_{i}\"] = 0\n",
    "\n",
    "        elif len(current_features) > expected_count:\n",
    "            print(f\"⚠️ Reducing {len(current_features) - expected_count} features to match model input size.\")\n",
    "            X = X.iloc[:, :expected_count]\n",
    "\n",
    "        X.columns = X.columns.astype(str)\n",
    "\n",
    "    return X\n",
    "\n",
    "# ✅ Scale Features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=expected_features)\n",
    "\n",
    "# ✅ Load Models\n",
    "model_files = {\n",
    "    \"Elastic Net\": os.path.join(SAVE_DIR, \"Elastic Net.pkl\"),\n",
    "    \"SGD\": os.path.join(SAVE_DIR, \"SGD.pkl\"),\n",
    "    \"Gradient Boosting\": os.path.join(SAVE_DIR, \"Gradient Boosting.pkl\"),\n",
    "    \"CNN (MLP)\": os.path.join(SAVE_DIR, \"CNN (MLP).pkl\"),\n",
    "    \"Diffusion Model\": os.path.join(SAVE_DIR, \"Diffusion Model.pkl\"),\n",
    "    \"GA-Optimized LR\": os.path.join(SAVE_DIR, \"GA-Optimized LR.pkl\"),\n",
    "    \"NeuroEvolution (NEAT)\": os.path.join(SAVE_DIR, \"NeuroEvolution (NEAT).pkl\"),\n",
    "}\n",
    "\n",
    "models = {}\n",
    "for name, path in model_files.items():\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"rb\") as f:\n",
    "                model = pickle.load(f)\n",
    "                if hasattr(model, \"predict\"):\n",
    "                    models[name] = {\"model\": model, \"lifespan\": 3}  # 3 Gen lifespan\n",
    "                    print(f\"✅ Loaded Model: {name}\")\n",
    "                else:\n",
    "                    print(f\"⚠️ {name} does not have `predict()`. Skipping...\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {name}: {e}\")\n",
    "    else:\n",
    "        print(f\"⚠️ {name} not found!\")\n",
    "\n",
    "if not models:\n",
    "    raise RuntimeError(\"❌ No valid models loaded.\")\n",
    "\n",
    "# ✅ Load Performance Metrics\n",
    "if os.path.exists(PERFORMANCE_FILE):\n",
    "    df_performance = pd.read_csv(PERFORMANCE_FILE)\n",
    "else:\n",
    "    raise FileNotFoundError(\"❌ Performance Metrics CSV not found!\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 📌 Model Knockout Tournament\n",
    "# -----------------------------------------------\n",
    "def model_battle(model_1, model_2, X, y):\n",
    "    \"\"\"Models compete on predicting market_stress. Winners proceed.\"\"\"\n",
    "    try:\n",
    "        X1 = align_features(pd.DataFrame(X), model_1[\"model\"])\n",
    "        X2 = align_features(pd.DataFrame(X), model_2[\"model\"])\n",
    "\n",
    "        pred_1 = model_1[\"model\"].predict(X1)\n",
    "        pred_2 = model_2[\"model\"].predict(X2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during model battle: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    conf_1 = np.mean((pred_1 > 0.5) == y) * np.mean(np.abs(pred_1 - 0.5))\n",
    "    conf_2 = np.mean((pred_2 > 0.5) == y) * np.mean(np.abs(pred_2 - 0.5))\n",
    "\n",
    "    return (model_1, model_2) if conf_1 > conf_2 else (model_2, model_1)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 📌 Model Breeding Function\n",
    "# -----------------------------------------------\n",
    "def breed_models(parent_1, parent_2):\n",
    "    \"\"\"Hybrid model creation (favoring CNN & NEAT).\"\"\"\n",
    "    p1_name, p2_name = parent_1[\"name\"], parent_2[\"name\"]\n",
    "    \n",
    "    if \"CNN\" in p1_name or \"NEAT\" in p1_name or \"CNN\" in p2_name or \"NEAT\" in p2_name:\n",
    "        advantage = 1.2  # Hybrids get boosted performance\n",
    "\n",
    "    hybrid_name = f\"{p1_name}-{p2_name}_Hybrid\"\n",
    "    new_model = random.choice([parent_1[\"model\"], parent_2[\"model\"]])  # Basic cross-breeding\n",
    "    return {\"name\": hybrid_name, \"model\": new_model, \"lifespan\": 3}  # Reset lifespan\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 📌 Model Evolution Process\n",
    "# -----------------------------------------------\n",
    "def evolve_models(models, df_performance, X, y, generations=10):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    performance_metrics = []\n",
    "\n",
    "    for gen in range(generations):\n",
    "        print(f\"\\n🚀 **Generation {gen+1}: Running Evolution**\")\n",
    "        model_list = list(models.keys())\n",
    "        random.shuffle(model_list)\n",
    "        survivors = []\n",
    "        \n",
    "        # Tournament Battles\n",
    "        for i in range(0, len(model_list), 2):\n",
    "            if i + 1 < len(model_list):\n",
    "                winner, loser = model_battle(models[model_list[i]], models[model_list[i+1]], X, y)\n",
    "                if winner:\n",
    "                    survivors.append(winner)\n",
    "            else:\n",
    "                survivors.append(models[model_list[i]])\n",
    "\n",
    "        print(f\"✅ Survivors: {len(survivors)} models advancing.\")\n",
    "\n",
    "        # Breeding Phase - Select Top Models\n",
    "        top_models = df_performance.nlargest(2, \"Fitness Score\")  # Select highest-performing models\n",
    "        breeding_pool = []\n",
    "        for _, row in top_models.iterrows():\n",
    "            if row[\"Model\"] in models:\n",
    "                breeding_pool.append({\"name\": row[\"Model\"], \"model\": models[row[\"Model\"]][\"model\"]})\n",
    "\n",
    "        offspring = []\n",
    "        for i in range(0, len(breeding_pool), 2):\n",
    "            if i + 1 < len(breeding_pool):\n",
    "                child = breed_models(breeding_pool[i], breeding_pool[i+1])\n",
    "                offspring.append(child)\n",
    "\n",
    "        # Reduce Lifespan for all models\n",
    "        for model in models.values():\n",
    "            model[\"lifespan\"] -= 1\n",
    "        models = {name: m for name, m in models.items() if m[\"lifespan\"] > 0}  # Remove expired models\n",
    "\n",
    "        models.update({f\"Gen{gen+1}_Model{i}\": m for i, m in enumerate(survivors + offspring)})\n",
    "\n",
    "        # Save Models\n",
    "        for name, model in models.items():\n",
    "            file_path = f\"saved_models/{name}_{timestamp}.pkl\"\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                pickle.dump(model[\"model\"], f)\n",
    "            print(f\"✅ Saved Model: {file_path}\")\n",
    "\n",
    "    return models\n",
    "\n",
    "# ✅ Run Evolution\n",
    "evolved_models = evolve_models(models, df_performance, X_scaled, y, generations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed0ff8b-67fd-46c4-a7d1-8a76dbd62b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b264f139-6e81-48db-baab-205a186988ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6bf6b-07d9-4cf6-92af-d2ef06e25a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497fea9f-b9a4-4deb-8d3e-14cc3f2817de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e437d755-118a-4519-a661-b9451a20bc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f4f50b-b366-4bc4-92fe-8fb6dc918123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb3582-a2ef-415e-bcef-5cca62f94aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a9c6319-6312-48f7-9064-481badba08d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "model_path = \"saved_models/Elastic Net.pkl\"  # Example model\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "print(type(model))  # What type is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "428deb52-6810-4260-bf1d-7098b3464a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Elastic Net Best Alpha', 'Elastic Net Best L1 Ratio', 'Mean Squared Error (MSE)', 'R² Score', 'ROC-AUC Score', 'Accuracy Score', 'Log Loss', 'Training Time (s)', 'Prediction Time (s)', 'Cross-Validation Stability', 'Fitness Score'])\n"
     ]
    }
   ],
   "source": [
    "# Print the contents\n",
    "print(model.keys())  # What keys does this dictionary have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3e7ae5d-2e2e-42b4-bdf1-6c493b94f99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Models successfully trained and saved with dates converted to numerical features!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ✅ Define Paths\n",
    "SAVE_DIR = \"saved_models\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ✅ Load Data\n",
    "df = pd.read_csv(\"data/financial_data_full.csv\")  # Update with your real dataset\n",
    "\n",
    "# ✅ Identify Date Columns (Assuming Dates are in 'YYYY-MM-DD' Format)\n",
    "date_cols = df.select_dtypes(include=[\"object\"]).columns  # Identify non-numeric columns\n",
    "\n",
    "for col in date_cols:\n",
    "    try:\n",
    "        df[col] = pd.to_datetime(df[col])  # Convert to datetime format\n",
    "        df[col + \"_days_since_start\"] = (df[col] - df[col].min()).dt.days  # Convert to numerical\n",
    "        df[col + \"_year\"] = df[col].dt.year\n",
    "        df[col + \"_month\"] = df[col].dt.month\n",
    "        df[col + \"_day\"] = df[col].dt.day\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipping {col}: {e}\")  # Some non-numeric columns may not be dates\n",
    "\n",
    "# ✅ Drop Original Date Columns (Replaced by numerical versions)\n",
    "df = df.drop(columns=date_cols, errors=\"ignore\")\n",
    "\n",
    "# ✅ Ensure Target Column Exists\n",
    "if \"market_stress\" not in df.columns:\n",
    "    raise ValueError(\"❌ Error: 'market_stress' column is missing from the dataset.\")\n",
    "\n",
    "# ✅ Split into Features & Target\n",
    "X = df.drop(columns=[\"market_stress\"])  # Features\n",
    "y = df[\"market_stress\"]  # Target variable\n",
    "\n",
    "# ✅ Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ Standardize Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# ✅ Train Elastic Net Model\n",
    "elastic_net = ElasticNetCV(cv=5, l1_ratio=[0.3, 0.6], alphas=np.logspace(-2, 0.5, 5), random_state=42, max_iter=5000)\n",
    "elastic_net.fit(X_train, y_train)\n",
    "\n",
    "# ✅ Train Gradient Boosting Model\n",
    "gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gbm.fit(X_train, y_train)\n",
    "\n",
    "# ✅ Save Models Properly\n",
    "with open(os.path.join(SAVE_DIR, \"Elastic Net.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(elastic_net, f)\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, \"Gradient Boosting.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(gbm, f)\n",
    "\n",
    "print(\"\\n✅ Models successfully trained and saved with dates converted to numerical features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89c1fe-4621-4fa6-acf2-72039002f51e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7dbaff-95d4-4c7d-a94a-8e4dd90ba991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e01b24ba-1d13-4f37-aefa-3dc0e9941bb8",
   "metadata": {},
   "source": [
    "## now fix sgd..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "299da33a-2b80-48c5-b336-47058c030c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved as: saved_models/SGD.pkl\n",
      "\n",
      "✅ SGDClassifier model is now trained and properly saved as a `.pkl` file!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score, roc_auc_score, accuracy_score, log_loss\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ✅ Define Save Path\n",
    "SAVE_DIR = \"saved_models\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ✅ Load Data\n",
    "df = pd.read_csv(\"data/financial_data_full.csv\")\n",
    "\n",
    "# ✅ Convert Date Columns to Numeric Features\n",
    "date_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "for col in date_cols:\n",
    "    try:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "        df[col + \"_days_since_start\"] = (df[col] - df[col].min()).dt.days\n",
    "        df[col + \"_year\"] = df[col].dt.year\n",
    "        df[col + \"_month\"] = df[col].dt.month\n",
    "        df[col + \"_day\"] = df[col].dt.day\n",
    "    except Exception:\n",
    "        print(f\"⚠️ Skipping non-date column: {col}\")\n",
    "\n",
    "df = df.drop(columns=date_cols, errors=\"ignore\")\n",
    "\n",
    "# ✅ Ensure Target Column Exists\n",
    "if \"market_stress\" not in df.columns:\n",
    "    raise ValueError(\"❌ Error: 'market_stress' column is missing from the dataset.\")\n",
    "\n",
    "# ✅ Split Data\n",
    "X = df.drop(columns=[\"market_stress\"])\n",
    "y = df[\"market_stress\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ Standardize Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# ✅ Train SGDClassifier (Instead of SGDRegressor)\n",
    "def train_sgd(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Trains an SGD model for classification and saves it as a pickle file.\"\"\"\n",
    "    \n",
    "    # ✅ Define SGD Classifier (For Classification)\n",
    "    sgd = SGDClassifier(\n",
    "        max_iter=2000,\n",
    "        tol=1e-4,\n",
    "        random_state=42,\n",
    "        penalty=\"l2\",  # Ridge-style regularization\n",
    "        alpha=0.01,  # Regularization strength\n",
    "        loss=\"log_loss\",  # Enables probability estimates\n",
    "    )\n",
    "\n",
    "    # ✅ Train Model\n",
    "    start_time = time.time()\n",
    "    sgd.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # ✅ Save Model\n",
    "    model_path = os.path.join(SAVE_DIR, \"SGD.pkl\")\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(sgd, f)\n",
    "\n",
    "    print(f\"✅ Model saved as: {model_path}\")\n",
    "\n",
    "    return sgd  # Return trained model for immediate use if needed\n",
    "\n",
    "# ✅ Train & Save Model\n",
    "sgd_model = train_sgd(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"\\n✅ SGDClassifier model is now trained and properly saved as a `.pkl` file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5eb7cf-c2bb-47f0-a512-4bdb1f245e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8518c5ff-c532-422a-b792-84592e3d28d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baba62a-b70f-486b-ade7-33ac51492f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d435011-f891-47bf-a315-e9bfbb5acec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "244d2c2a-ee84-48a1-abf4-fe2fabafe3a2",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f657107-2802-4b0b-b049-000a0fd233be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved as: saved_models/CNN (MLP).pkl\n",
      "\n",
      "✅ MLPClassifier model is now trained and properly saved as a `.pkl` file!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score, roc_auc_score, accuracy_score, log_loss\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ✅ Define Save Path\n",
    "SAVE_DIR = \"saved_models\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ✅ Load Data\n",
    "df = pd.read_csv(\"data/financial_data_full.csv\")\n",
    "\n",
    "# ✅ Convert Date Columns to Numeric Features\n",
    "date_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "for col in date_cols:\n",
    "    try:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "        df[col + \"_days_since_start\"] = (df[col] - df[col].min()).dt.days\n",
    "        df[col + \"_year\"] = df[col].dt.year\n",
    "        df[col + \"_month\"] = df[col].dt.month\n",
    "        df[col + \"_day\"] = df[col].dt.day\n",
    "    except Exception:\n",
    "        print(f\"⚠️ Skipping non-date column: {col}\")\n",
    "\n",
    "df = df.drop(columns=date_cols, errors=\"ignore\")\n",
    "\n",
    "# ✅ Ensure Target Column Exists\n",
    "if \"market_stress\" not in df.columns:\n",
    "    raise ValueError(\"❌ Error: 'market_stress' column is missing from the dataset.\")\n",
    "\n",
    "# ✅ Split Data\n",
    "X = df.drop(columns=[\"market_stress\"])\n",
    "y = df[\"market_stress\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ Standardize Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# ✅ Train CNN (MLP)\n",
    "def train_cnn(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Trains a Multi-Layer Perceptron (MLP) as a CNN stand-in and saves it.\"\"\"\n",
    "\n",
    "    # ✅ Define MLP Model\n",
    "    cnn = MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50),  # Two layers: 100 neurons & 50 neurons\n",
    "        activation=\"relu\",  # ReLU activation function\n",
    "        solver=\"adam\",  # Adam optimizer\n",
    "        max_iter=500,  # Max training iterations\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # ✅ Train Model\n",
    "    start_time = time.time()\n",
    "    cnn.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # ✅ Save Model\n",
    "    model_path = os.path.join(SAVE_DIR, \"CNN (MLP).pkl\")\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(cnn, f)\n",
    "\n",
    "    print(f\"✅ Model saved as: {model_path}\")\n",
    "\n",
    "    return cnn  # Return trained model for immediate use if needed\n",
    "\n",
    "# ✅ Train & Save Model\n",
    "cnn_model = train_cnn(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"\\n✅ MLPClassifier model is now trained and properly saved as a `.pkl` file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62a979d-f797-4858-95f4-e50e26f8eac2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860d3f5-c892-4d0d-b4df-10bab04a56f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6247d-7301-4f67-952a-79cf8ac407e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f0840-f272-48dd-87eb-f57bc374031d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49d8d552-afb0-463b-8d60-b9568072172c",
   "metadata": {},
   "source": [
    "# diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "823ef3be-a418-4469-b9cc-265f97cef1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved as: saved_models/Diffusion Model.pkl\n",
      "\n",
      "✅ GaussianMixture model is now trained and properly saved as a `.pkl` file!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score, roc_auc_score, accuracy_score, log_loss\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ✅ Define Save Path\n",
    "SAVE_DIR = \"saved_models\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ✅ Load Data\n",
    "df = pd.read_csv(\"data/financial_data_full.csv\")\n",
    "\n",
    "# ✅ Convert Date Columns to Numeric Features\n",
    "date_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "for col in date_cols:\n",
    "    try:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "        df[col + \"_days_since_start\"] = (df[col] - df[col].min()).dt.days\n",
    "        df[col + \"_year\"] = df[col].dt.year\n",
    "        df[col + \"_month\"] = df[col].dt.month\n",
    "        df[col + \"_day\"] = df[col].dt.day\n",
    "    except Exception:\n",
    "        print(f\"⚠️ Skipping non-date column: {col}\")\n",
    "\n",
    "df = df.drop(columns=date_cols, errors=\"ignore\")\n",
    "\n",
    "# ✅ Ensure Target Column Exists\n",
    "if \"market_stress\" not in df.columns:\n",
    "    raise ValueError(\"❌ Error: 'market_stress' column is missing from the dataset.\")\n",
    "\n",
    "# ✅ Split Data\n",
    "X = df.drop(columns=[\"market_stress\"])\n",
    "y = df[\"market_stress\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ Standardize Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# ✅ Train Diffusion Model (GMM)\n",
    "def train_diffusion_model(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Trains a Gaussian Mixture Model (GMM) as a stand-in for a diffusion model and saves it.\"\"\"\n",
    "\n",
    "    # ✅ Define GMM Model\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=5,  # Assume 5 mixture components\n",
    "        covariance_type=\"full\",\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # ✅ Train Model\n",
    "    start_time = time.time()\n",
    "    gmm.fit(X_train)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # ✅ Save Model\n",
    "    model_path = os.path.join(SAVE_DIR, \"Diffusion Model.pkl\")\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(gmm, f)\n",
    "\n",
    "    print(f\"✅ Model saved as: {model_path}\")\n",
    "\n",
    "    return gmm  # Return trained model for immediate use if needed\n",
    "\n",
    "# ✅ Train & Save Model\n",
    "diffusion_model = train_diffusion_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"\\n✅ GaussianMixture model is now trained and properly saved as a `.pkl` file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f486e-1d72-4969-8ec0-3ba0a002e08b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a89f7-e475-481e-a96c-32de31de7d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c7ff19-3690-442b-8a35-1e00c61e1644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20455d59-4e9c-4810-993b-64d245421b55",
   "metadata": {},
   "source": [
    "#ga-optimized lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "77f0ec58-ec50-4a2b-b053-4c1f76203bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved as: saved_models/GA-Optimized LR.pkl\n",
      "\n",
      "✅ GA-Optimized Logistic Regression model is now trained and properly saved as a `.pkl` file!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from deap import base, creator, tools, algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score, roc_auc_score, accuracy_score, log_loss\n",
    ")\n",
    "\n",
    "# ✅ Define Save Path\n",
    "SAVE_DIR = \"saved_models\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ✅ Load Data\n",
    "df = pd.read_csv(\"data/financial_data_full.csv\")\n",
    "\n",
    "# ✅ Convert Date Columns to Numeric Features\n",
    "date_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "for col in date_cols:\n",
    "    try:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "        df[col + \"_days_since_start\"] = (df[col] - df[col].min()).dt.days\n",
    "        df[col + \"_year\"] = df[col].dt.year\n",
    "        df[col + \"_month\"] = df[col].dt.month\n",
    "        df[col + \"_day\"] = df[col].dt.day\n",
    "    except Exception:\n",
    "        print(f\"⚠️ Skipping non-date column: {col}\")\n",
    "\n",
    "df = df.drop(columns=date_cols, errors=\"ignore\")\n",
    "\n",
    "# ✅ Ensure Target Column Exists\n",
    "if \"market_stress\" not in df.columns:\n",
    "    raise ValueError(\"❌ Error: 'market_stress' column is missing from the dataset.\")\n",
    "\n",
    "# ✅ Split Data\n",
    "X = df.drop(columns=[\"market_stress\"])\n",
    "y = df[\"market_stress\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ Standardize Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# ✅ Prevent Duplicate Class Definitions in DEAP\n",
    "if \"FitnessMax\" not in creator.__dict__:\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "if \"Individual\" not in creator.__dict__:\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "def train_ga_lr(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Uses a Genetic Algorithm (GA) to optimize feature selection for Logistic Regression and saves the model.\"\"\"\n",
    "\n",
    "    num_features = X_train.shape[1]  # Number of features in dataset\n",
    "\n",
    "    # ✅ Define GA Structure\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"attr_bool\", random.randint, 0, 1)  # Binary feature selection\n",
    "    toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=num_features)\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "    def evaluate(individual):\n",
    "        \"\"\"Evaluates the fitness of an individual feature selection.\"\"\"\n",
    "        selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "        \n",
    "        if not selected_features:\n",
    "            return (0.0,)  # Prevent empty feature sets\n",
    "        \n",
    "        X_train_selected = X_train[:, selected_features]\n",
    "        X_test_selected = X_test[:, selected_features]\n",
    "\n",
    "        model = LogisticRegression(max_iter=2000, solver='liblinear', random_state=42)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        \n",
    "        y_pred_prob = model.predict_proba(X_test_selected)[:, 1]\n",
    "        y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        stability = np.mean(cross_val_score(model, X_train_selected, y_train, cv=5))\n",
    "\n",
    "        # ✅ Fitness Score (Maximize ROC-AUC, Accuracy, and Stability)\n",
    "        fitness = (roc_auc + accuracy + stability) / 3\n",
    "\n",
    "        return (fitness,)\n",
    "\n",
    "    toolbox.register(\"evaluate\", evaluate)\n",
    "    toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "    toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.1)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "    # ✅ Create Initial Population & Run GA\n",
    "    pop = toolbox.population(n=20)  # 20 individuals\n",
    "    start_time = time.time()\n",
    "    algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=10, verbose=False)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # ✅ Select Best Individual\n",
    "    best_individual = tools.selBest(pop, k=1)[0]\n",
    "    selected_features = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "\n",
    "    X_train_selected = X_train[:, selected_features]\n",
    "    X_test_selected = X_test[:, selected_features]\n",
    "\n",
    "    # ✅ Final Model with Best Features\n",
    "    final_model = LogisticRegression(max_iter=2000, solver='liblinear', random_state=42)\n",
    "    final_model.fit(X_train_selected, y_train)\n",
    "\n",
    "    # ✅ Save Model\n",
    "    model_path = os.path.join(SAVE_DIR, \"GA-Optimized LR.pkl\")\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(final_model, f)\n",
    "\n",
    "    print(f\"✅ Model saved as: {model_path}\")\n",
    "\n",
    "    return final_model  # Return trained model for immediate use if needed\n",
    "\n",
    "# ✅ Train & Save Model\n",
    "ga_lr_model = train_ga_lr(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"\\n✅ GA-Optimized Logistic Regression model is now trained and properly saved as a `.pkl` file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c633e-5b7a-4c7d-a474-9204bdd0e236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4872334c-1597-4bad-9a97-0e1f9472d334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c7dd13b-f119-4e7b-9757-1ce8006a690b",
   "metadata": {},
   "source": [
    "# neat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4395885a-85e2-45c7-8971-b3332a6d3cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 **Starting NEAT Test**...\n",
      "\n",
      "\n",
      "🚀 **Starting NEAT Training...**\n",
      "\n",
      "🚀 **Starting NEAT Training...**\n",
      "\n",
      "✅ NEAT Model Saved: `saved_models/NeuroEvolution (NEAT).pkl`\n",
      "\n",
      "✅ NEAT model is now trained and properly saved with `predict()`.\n"
     ]
    }
   ],
   "source": [
    "import neat\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score, roc_auc_score, accuracy_score, log_loss\n",
    ")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 📌 Data Preprocessing Function\n",
    "# -----------------------------------------------\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess dataset by handling missing values and encoding categorical data.\"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    df = df.dropna()  # Remove missing values\n",
    "\n",
    "    # Convert categorical & date columns to numeric\n",
    "    for col in df.select_dtypes(include=['object']):\n",
    "        try:\n",
    "            df[col] = pd.to_datetime(df[col]).astype(int) / 10**9  # Convert to timestamp\n",
    "        except:\n",
    "            df[col] = df[col].astype(\"category\").cat.codes  \n",
    "\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 📌 Custom NEAT Wrapper with Predict Methods\n",
    "# -----------------------------------------------\n",
    "class NEATModelWrapper:\n",
    "    \"\"\"Wrapper to make NEAT behave like a scikit-learn model.\"\"\"\n",
    "    def __init__(self, genome, config):\n",
    "        self.genome = genome\n",
    "        self.config = config\n",
    "        self.net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Returns probability predictions (like logistic regression).\"\"\"\n",
    "        X = np.array(X)\n",
    "        return np.array([[1 - self.net.activate(xi)[0], self.net.activate(xi)[0]] for xi in X])  # [prob_0, prob_1]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Returns binary predictions (0 or 1).\"\"\"\n",
    "        return np.argmax(self.predict_proba(X), axis=1)  # Take class with highest probability\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 📌 TRAIN NEUROEVOLUTION (NEAT)\n",
    "# -----------------------------------------------\n",
    "def eval_genome(genome, config, X_train, y_train, valid_genomes):\n",
    "    \"\"\"Evaluates a single NEAT genome, computing fitness based on multiple metrics.\"\"\"\n",
    "    \n",
    "    net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "    y_pred_prob = np.array([net.activate(xi)[0] for xi in X_train])  # **Continuous probabilities**\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)  # **Convert to binary**\n",
    "\n",
    "    mse = mean_squared_error(y_train, y_pred_prob)  # ✅ Use probabilities\n",
    "    r2 = r2_score(y_train, y_pred_prob)  # ✅ Use probabilities\n",
    "\n",
    "    # **Fix: Convert `y_train` to Binary Labels**\n",
    "    y_train_binary = (y_train >= np.median(y_train)).astype(int)\n",
    "\n",
    "    # ✅ **Fix: Ensure roc_auc_score receives binary labels**\n",
    "    if len(np.unique(y_train_binary)) > 1:\n",
    "        auc = roc_auc_score(y_train_binary, y_pred_prob)  # ✅ **Use probabilities**\n",
    "    else:\n",
    "        auc = 0.5  # Default if only one class present\n",
    "\n",
    "    accuracy = accuracy_score(y_train_binary, y_pred)\n",
    "\n",
    "    eps = 1e-9  \n",
    "    y_pred_prob = np.clip(y_pred_prob, eps, 1 - eps)\n",
    "    log_loss_value = log_loss(y_train_binary, y_pred_prob)\n",
    "\n",
    "    valid_fitness_values = [g.fitness for g in valid_genomes[-5:] if g.fitness is not None]\n",
    "    fitness_variance = np.var(valid_fitness_values) if valid_fitness_values else 1.0\n",
    "    cross_validation_stability = 1 / (1 + fitness_variance)  \n",
    "\n",
    "    fitness_score = (\n",
    "        (accuracy * 0.3) + \n",
    "        (auc * 0.25) + \n",
    "        (cross_validation_stability * 0.2) -  \n",
    "        (mse * 0.15) -  \n",
    "        (log_loss_value * 0.1)  \n",
    "    )\n",
    "\n",
    "    return max(fitness_score, 0), cross_validation_stability  \n",
    "\n",
    "def eval_genomes(genomes, config, X_train, y_train):\n",
    "    \"\"\"Evaluates all genomes in the current NEAT generation.\"\"\"\n",
    "    valid_genomes = [g for _, g in genomes if g.fitness is not None]\n",
    "\n",
    "    for genome_id, genome in genomes:\n",
    "        genome.fitness, _ = eval_genome(genome, config, X_train, y_train, valid_genomes)\n",
    "\n",
    "def train_neat(X_train_full, X_test_full, y_train_full, y_test_full):\n",
    "    \"\"\"Trains NEAT and saves the best model in a wrapper class with `.predict()`.\"\"\"\n",
    "    print(\"\\n🚀 **Starting NEAT Training...**\")\n",
    "\n",
    "    config_path = \"neat_config3.txt\"\n",
    "    config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n",
    "                         neat.DefaultSpeciesSet, neat.DefaultStagnation, config_path)\n",
    "\n",
    "    pop = neat.Population(config)\n",
    "\n",
    "    start_time = time.time()\n",
    "    pop.run(lambda genomes, cfg: eval_genomes(genomes, cfg, X_train_full, y_train_full), 10)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    valid_genomes = [g for g in pop.population.values() if g.fitness is not None]\n",
    "    if not valid_genomes:\n",
    "        raise ValueError(\"❌ No valid genomes with assigned fitness scores.\")\n",
    "\n",
    "    best_genome = max(valid_genomes, key=lambda g: g.fitness)\n",
    "\n",
    "    valid_fitness_values = [g.fitness for g in valid_genomes[-5:] if g.fitness is not None]\n",
    "    fitness_variance = np.var(valid_fitness_values) if valid_fitness_values else 1.0\n",
    "    cross_validation_stability = 1 / (1 + fitness_variance)\n",
    "\n",
    "    neat_model = NEATModelWrapper(best_genome, config)\n",
    "\n",
    "    with open(\"saved_models/NeuroEvolution (NEAT).pkl\", \"wb\") as f:\n",
    "        pickle.dump(neat_model, f)\n",
    "\n",
    "    print(f\"\\n✅ NEAT Model Saved: `saved_models/NeuroEvolution (NEAT).pkl`\")\n",
    "\n",
    "    return neat_model\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 📌 TEST NEUROEVOLUTION (NEAT) - RUN TRAINING\n",
    "# -----------------------------------------------\n",
    "import traceback  \n",
    "\n",
    "def test_train_neat():\n",
    "    \"\"\"Ensures NEAT model runs, saves, and has `predict()`.\"\"\"\n",
    "    print(\"\\n🚀 **Starting NEAT Test**...\\n\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(\"data/financial_data_full.csv\")\n",
    "        df_cleaned = preprocess_data(df)\n",
    "        df_scaled = StandardScaler().fit_transform(df_cleaned)  # ✅ Apply Scaling\n",
    "\n",
    "        X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "            df_scaled[:, :-1], df_scaled[:, -1], test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        print(\"\\n🚀 **Starting NEAT Training...**\")\n",
    "        neat_model = train_neat(X_train_full, X_test_full, y_train_full, y_test_full)\n",
    "\n",
    "        assert hasattr(neat_model, \"predict\"), \"❌ NEAT model does not have `predict()`\"\n",
    "        assert hasattr(neat_model, \"predict_proba\"), \"❌ NEAT model does not have `predict_proba()`\"\n",
    "\n",
    "        print(\"\\n✅ NEAT model is now trained and properly saved with `predict()`.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n❌ **Error in test_train_neat()**\")\n",
    "        traceback.print_exc()  \n",
    "        raise  \n",
    "\n",
    "# ✅ Run the test\n",
    "test_train_neat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce0064e-4493-4f84-9786-39d20121b689",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
