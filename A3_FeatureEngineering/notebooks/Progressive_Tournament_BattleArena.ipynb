{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a95078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))  # go up one level\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from models import get_base_models, make_child_model\n",
    "from utils.resampling_registry import get_resamplers\n",
    "from utils.battle_logger import BattleLogger\n",
    "from utils.breed_and_battle import breed_and_battle, breed_and_battle_with_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bc69a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä baseline_easy_w5_n5_d0.csv ‚Äî Baseline PR AUC: 0.055\n",
      "üìä high_drift_w3_n10_d60.csv ‚Äî Baseline PR AUC: 0.036\n",
      "üìä imbalanced_sparse_w0_n10_d10.csv ‚Äî Baseline PR AUC: 0.011\n",
      "üìä mixed_realistic_w4_n15_d30.csv ‚Äî Baseline PR AUC: 0.045\n",
      "üìä noisy_overlap_w2_n20_d0.csv ‚Äî Baseline PR AUC: 0.026\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# üìÇ Load all datasets with baseline PR AUC\n",
    "def load_all_datasets(folder=\"./synthetic_datasets\"):\n",
    "    files = sorted(glob.glob(os.path.join(folder, \"*.csv\")))\n",
    "    datasets = []\n",
    "\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f)\n",
    "        X = df.drop(\"rare_event\", axis=1)\n",
    "        y = df[\"rare_event\"]\n",
    "        baseline_pr_auc = y.mean()\n",
    "\n",
    "        datasets.append({\n",
    "            \"name\": os.path.basename(f),\n",
    "            \"X\": X,\n",
    "            \"y\": y,\n",
    "            \"baseline_pr_auc\": baseline_pr_auc\n",
    "        })\n",
    "\n",
    "        print(f\"üìä {os.path.basename(f)} ‚Äî Baseline PR AUC: {baseline_pr_auc:.3f}\")\n",
    "\n",
    "    return datasets\n",
    "\n",
    "# Now load + sort\n",
    "datasets = load_all_datasets()\n",
    "datasets = sorted(datasets, key=lambda d: d[\"baseline_pr_auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79995925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåä Stage 1: imbalanced_sparse_w0_n10_d10.csv\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåä Stage 2: noisy_overlap_w2_n20_d0.csv\n",
      "51\n",
      "\n",
      "üåä Stage 3: high_drift_w3_n10_d60.csv\n",
      "465\n",
      "\n",
      "üåä Stage 4: mixed_realistic_w4_n15_d30.csv\n",
      "4191\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# üìä Sort datasets by baseline PR AUC (ascending difficulty)\n",
    "datasets = sorted(datasets, key=lambda d: d[\"baseline_pr_auc\"])\n",
    "\n",
    "survivors = get_base_models()\n",
    "finalists = []\n",
    "\n",
    "# üåä Stage-by-stage tournament\n",
    "for stage_idx, dataset in enumerate(datasets):\n",
    "    dataset_name = dataset[\"name\"]\n",
    "    print(f\"\\nüåä Stage {stage_idx + 1}: {dataset_name}\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataset[\"X\"], dataset[\"y\"], stratify=dataset[\"y\"], test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    resamplers = get_resamplers(X_train, y_train, target_col=\"rare_event\")\n",
    "    resampled_datasets = {name: fn() for name, fn in resamplers.items()}\n",
    "\n",
    "    # ‚úÖ Update dataset metadata on all survivors\n",
    "    for s in survivors:\n",
    "        s[\"dataset_name\"] = dataset_name\n",
    "        s[\"baseline_pr_auc\"] = dataset[\"baseline_pr_auc\"]\n",
    "\n",
    "    with BattleLogger(\n",
    "        to_file=f\"logs/battle_stage_{stage_idx + 1}.txt\",\n",
    "        js_file=f\"logs/battle_stage_{stage_idx + 1}.js\",\n",
    "        inject_html=True,\n",
    "        html_template=\"battle_template.html\",\n",
    "        html_output=f\"logs/battle_stage_{stage_idx + 1}.html\"\n",
    "    ):\n",
    "        survivors = breed_and_battle_with_population(\n",
    "            model_population=survivors,\n",
    "            resampled_datasets=resampled_datasets,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            generations=1,\n",
    "            top_k=3,\n",
    "            dataset_name=dataset_name,\n",
    "            baseline_pr_auc=dataset[\"baseline_pr_auc\"],\n",
    "            debug=True\n",
    "        )\n",
    "\n",
    "    finalists = survivors  # Keep track of final survivors\n",
    "\n",
    "    # üîÅ Rebuild base model configs from survivors for next stage\n",
    "    survivors = [\n",
    "        {\n",
    "            \"name\": s[\"label\"].split(\" + \")[0],  # recover model name\n",
    "            \"model\": make_child_model(s[\"params\"]),\n",
    "            \"params\": s[\"params\"]\n",
    "        }\n",
    "        for s in survivors\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb62e0-fd6f-44bb-8bb1-557e1746a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèÜ Final leaderboard\n",
    "# print(f\"\\nüèÜ Finalists after all rounds: {[m['label'] for m in finalists]}\")\n",
    "\n",
    "# print(\"\\nüèÜ Finalists after all rounds:\")\n",
    "# for i, model in enumerate(finalists, 1):\n",
    "#     print(f\"{i}. {model['label']}_G{model['generation']} | PR AUC: {model['score']['pr_auc']:.3f} | Lineage: {model['lineage']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc218e2-3a91-494d-b5b2-c33c448b041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# üßæ Build summary table from finalists\n",
    "final_summary = []\n",
    "for model in finalists:\n",
    "    pr_auc = model['score']['pr_auc']\n",
    "    baseline = model.get('baseline_pr_auc', 0.0)\n",
    "    final_summary.append({\n",
    "        \"Label\": f\"{model['label']}_G{model['generation']}\",\n",
    "        \"Generation\": model['generation'],\n",
    "        \"PR AUC\": round(pr_auc, 3),\n",
    "        \"Lineage\": model.get(\"lineage\", \"‚Äî\"),\n",
    "        \"Beats Baseline?\": \"‚úÖ Yes\" if pr_auc > baseline else \"‚ùå No\",\n",
    "        \"Baseline PR AUC\": round(baseline, 3),\n",
    "        \"Dataset\": model.get(\"dataset_name\", \"unknown\")\n",
    "    })\n",
    "\n",
    "# df_final = pd.DataFrame(final_summary).sort_values(by=[\"PR AUC\"], ascending=False)\n",
    "df_final = pd.DataFrame(final_summary)\n",
    "\n",
    "# üìä Display final summary\n",
    "print(\"\\nüìã Finalist Leaderboard:\")\n",
    "print(df_final.to_string(index=False))\n",
    "\n",
    "# üíæ Optionally save\n",
    "df_final.to_csv(\"logs/finalists_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf6cb40-8dbf-4896-8461-5a50dde8eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b51b5-05b3-421a-9fc4-34f4aa1967f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
