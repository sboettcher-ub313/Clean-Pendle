{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a95078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))  # go up one level\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from models import make_child_model\n",
    "from models import get_base_models\n",
    "from utils.resampling_registry import get_resamplers\n",
    "from utils.battle_logger import BattleLogger\n",
    "from utils.breed_and_battle import breed_and_battle_with_population\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc69a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 baseline_easy_w5_n5_d0.csv — Baseline PR AUC: 0.055\n",
      "📊 high_drift_w3_n10_d60.csv — Baseline PR AUC: 0.036\n",
      "📊 imbalanced_sparse_w0_n10_d10.csv — Baseline PR AUC: 0.011\n",
      "📊 mixed_realistic_w4_n15_d30.csv — Baseline PR AUC: 0.045\n",
      "📊 noisy_overlap_w2_n20_d0.csv — Baseline PR AUC: 0.026\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# 📂 Load all datasets with baseline PR AUC\n",
    "def load_all_datasets(folder=\"./synthetic_datasets\"):\n",
    "    files = sorted(glob.glob(os.path.join(folder, \"*.csv\")))\n",
    "    datasets = []\n",
    "\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f)\n",
    "        X = df.drop(\"rare_event\", axis=1)\n",
    "        y = df[\"rare_event\"]\n",
    "        baseline_pr_auc = y.mean()\n",
    "\n",
    "        datasets.append({\n",
    "            \"name\": os.path.basename(f),\n",
    "            \"X\": X,\n",
    "            \"y\": y,\n",
    "            \"baseline_pr_auc\": baseline_pr_auc\n",
    "        })\n",
    "\n",
    "        print(f\"📊 {os.path.basename(f)} — Baseline PR AUC: {baseline_pr_auc:.3f}\")\n",
    "\n",
    "    return datasets\n",
    "\n",
    "# Now load + sort\n",
    "datasets = load_all_datasets()\n",
    "datasets = sorted(datasets, key=lambda d: d[\"baseline_pr_auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f646fa4f-835f-4349-9f0f-93d40c66674f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌊 Stage 1: baseline_easy_w5_n5_d0.csv\n",
      "[💾] Saved winner from baseline_easy_w5_n5_d0.csv with PR AUC = 0.673\n",
      "\n",
      "🌊 Stage 2: mixed_realistic_w4_n15_d30.csv\n",
      "[💾] Saved winner from mixed_realistic_w4_n15_d30.csv with PR AUC = 0.209\n",
      "\n",
      "🌊 Stage 3: high_drift_w3_n10_d60.csv\n",
      "[💾] Saved winner from high_drift_w3_n10_d60.csv with PR AUC = 0.178\n",
      "\n",
      "🌊 Stage 4: noisy_overlap_w2_n20_d0.csv\n",
      "[💾] Saved winner from noisy_overlap_w2_n20_d0.csv with PR AUC = 0.110\n",
      "\n",
      "🌊 Stage 5: imbalanced_sparse_w0_n10_d10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/sophiaboettcher/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[💾] Saved winner from imbalanced_sparse_w0_n10_d10.csv with PR AUC = 0.505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# from models.core_models import make_child_model  # ✅ Toggle this if using logistic regression\n",
    "from models.core_models2 import make_child_model  # ✅ Toggle this if using SGDClassifier\n",
    "\n",
    "# 📊 Sort datasets by baseline PR AUC (ascending difficulty)\n",
    "datasets = sorted(datasets, key=lambda d: d[\"baseline_pr_auc\"], reverse=True)  # Easiest to hardest\n",
    "\n",
    "# ✅ Start with base models — include name, params, and model object\n",
    "survivors = [\n",
    "    {\n",
    "        \"name\": cfg[\"name\"],\n",
    "        \"params\": cfg[\"params\"],\n",
    "        \"model\": make_child_model(cfg[\"params\"], max_iter=300)\n",
    "    }\n",
    "    for cfg in get_base_models()\n",
    "]\n",
    "finalists = []  # ← We'll keep ALL stage survivors here\n",
    "\n",
    "# 🌊 Stage-by-stage tournament\n",
    "for stage_idx, dataset in enumerate(datasets):\n",
    "    dataset_name = dataset[\"name\"]\n",
    "    print(f\"\\n🌊 Stage {stage_idx + 1}: {dataset_name}\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataset[\"X\"], dataset[\"y\"], stratify=dataset[\"y\"], test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    resamplers = get_resamplers(X_train, y_train, target_col=\"rare_event\")\n",
    "    resampled_datasets = {name: fn() for name, fn in resamplers.items()}\n",
    "\n",
    "    # ✅ Add metadata for logging/debug\n",
    "    for s in survivors:\n",
    "        s[\"dataset_name\"] = dataset_name\n",
    "        s[\"baseline_pr_auc\"] = dataset[\"baseline_pr_auc\"]\n",
    "\n",
    "    with BattleLogger(\n",
    "        to_file=f\"logs/battle_stage_{stage_idx + 1}.txt\",\n",
    "        # js_file=f\"logs/battle_stage_{stage_idx + 1}.js\",\n",
    "        # inject_html=True,\n",
    "        # html_template=\"battle_template.html\",\n",
    "        # html_output=f\"logs/battle_stage_{stage_idx + 1}.html\"\n",
    "    ):\n",
    "        survivors = breed_and_battle_with_population(\n",
    "            model_population=survivors,\n",
    "            resampled_datasets=resampled_datasets,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            generations=50,\n",
    "            top_k=3,\n",
    "            dataset_name=dataset_name,\n",
    "            baseline_pr_auc=dataset[\"baseline_pr_auc\"],\n",
    "            debug=True\n",
    "        )\n",
    "\n",
    "    # ✅ Accumulate stage winners for leaderboard\n",
    "    finalists.extend(survivors)\n",
    "\n",
    "    # 🏆 Save the top survivor for this stage\n",
    "    top_survivor = sorted(survivors, key=lambda s: s[\"score\"][\"pr_auc\"], reverse=True)[0]\n",
    "    \n",
    "    # Extract model and metadata\n",
    "    model_to_save = top_survivor[\"model\"]\n",
    "    meta_to_save = {\n",
    "        \"label\": top_survivor[\"label\"],\n",
    "        \"generation\": top_survivor[\"generation\"],\n",
    "        \"score\": top_survivor[\"score\"],\n",
    "        \"dataset\": top_survivor[\"dataset_name\"],\n",
    "        \"params\": top_survivor[\"params\"]\n",
    "    }\n",
    "    \n",
    "    # Save both\n",
    "    import joblib\n",
    "    output_path = \"output\"  # or use \"output\" if working locally\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    joblib.dump(model_to_save, f\"{output_path}/winner_{dataset_name}.pkl\")\n",
    "    joblib.dump(meta_to_save, f\"{output_path}/winner_{dataset_name}_meta.pkl\")\n",
    "    \n",
    "    print(f\"[💾] Saved winner from {dataset_name} with PR AUC = {top_survivor['score']['pr_auc']:.3f}\")\n",
    "\n",
    "    # ✅ Keep only top 3 for next stage (to limit memory)\n",
    "    survivors = sorted(survivors, key=lambda s: s[\"score\"][\"pr_auc\"], reverse=True)[:3]\n",
    "\n",
    "    # 🧼 Explicit cleanup\n",
    "    del X_train, X_test, y_train, y_test, resamplers, resampled_datasets\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "    # 🔁 Rebuild new models for next generation\n",
    "    survivors = [\n",
    "        {\n",
    "            \"name\": s[\"label\"].split(\" + \")[0],\n",
    "            \"params\": s[\"params\"],\n",
    "            \"model\": make_child_model(s[\"params\"], max_iter=300)\n",
    "        }\n",
    "        for s in survivors\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79995925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last working 8:53pm Tue Apr 1\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# # from models.core_models import make_child_model  # ✅ Required for model regeneration\n",
    "# # toggle to test sgd\n",
    "# from models.core_models2 import make_child_model\n",
    "\n",
    "# # 📊 Sort datasets by baseline PR AUC (ascending difficulty)\n",
    "# # datasets = sorted(datasets, key=lambda d: d[\"baseline_pr_auc\"])\n",
    "\n",
    "# # ✅ To sort from easiest to hardest:\n",
    "# datasets = sorted(datasets, key=lambda d: d[\"baseline_pr_auc\"], reverse=True)\n",
    "\n",
    "# # Start with base models — include name, params, and model\n",
    "# survivors = [\n",
    "#     {\n",
    "#         \"name\": cfg[\"name\"],\n",
    "#         \"params\": cfg[\"params\"],\n",
    "#         \"model\": make_child_model(cfg[\"params\"], max_iter=300)  # initial model objects\n",
    "#     }\n",
    "#     for cfg in get_base_models()\n",
    "# ]\n",
    "# finalists = []\n",
    "\n",
    "# # 🌊 Stage-by-stage tournament\n",
    "# for stage_idx, dataset in enumerate(datasets):\n",
    "#     dataset_name = dataset[\"name\"]\n",
    "#     print(f\"\\n🌊 Stage {stage_idx + 1}: {dataset_name}\")\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         dataset[\"X\"], dataset[\"y\"], stratify=dataset[\"y\"], test_size=0.3, random_state=42\n",
    "#     )\n",
    "\n",
    "#     resamplers = get_resamplers(X_train, y_train, target_col=\"rare_event\")\n",
    "#     resampled_datasets = {name: fn() for name, fn in resamplers.items()}\n",
    "\n",
    "#     # ✅ Add metadata for this round\n",
    "#     for s in survivors:\n",
    "#         s[\"dataset_name\"] = dataset_name\n",
    "#         s[\"baseline_pr_auc\"] = dataset[\"baseline_pr_auc\"]\n",
    "\n",
    "#     with BattleLogger(\n",
    "#         to_file=f\"logs/battle_stage_{stage_idx + 1}.txt\",\n",
    "#         # js_file=f\"logs/battle_stage_{stage_idx + 1}.js\",\n",
    "#         # inject_html=True,\n",
    "#         # html_template=\"battle_template.html\",\n",
    "#         # html_output=f\"logs/battle_stage_{stage_idx + 1}.html\"\n",
    "#     ):\n",
    "#         survivors = breed_and_battle_with_population(\n",
    "#             model_population=survivors,\n",
    "#             resampled_datasets=resampled_datasets,\n",
    "#             X_test=X_test,\n",
    "#             y_test=y_test,\n",
    "#             generations=50,\n",
    "#             top_k=3,\n",
    "#             dataset_name=dataset_name,\n",
    "#             baseline_pr_auc=dataset[\"baseline_pr_auc\"],\n",
    "#             debug=True # False to skip intensive logging\n",
    "#         )\n",
    "\n",
    "#     finalists = survivors  # Save finalists after this round\n",
    "#     # speedup technique (clearing memory)\n",
    "#     # 🧹 Keep only top performers to limit memory bloat\n",
    "#     survivors = sorted(survivors, key=lambda s: s[\"score\"][\"pr_auc\"], reverse=True)[:3]\n",
    "#     finalists = survivors.copy()\n",
    "\n",
    "#     # 🧼 Clean up explicitly and rebuild new survivors for next stage\n",
    "#     del X_train, X_test, y_train, y_test, resamplers, resampled_datasets\n",
    "#     # Force garbage collection     \n",
    "#     gc.collect()\n",
    "\n",
    "#     # 🔁 Rebuild survivor configs with regenerated model objects\n",
    "#     survivors = [\n",
    "#         {\n",
    "#             \"name\": s[\"label\"].split(\" + \")[0],\n",
    "#             \"params\": s[\"params\"],\n",
    "#             \"model\": make_child_model(s[\"params\"], max_iter=300)\n",
    "#         }\n",
    "#         for s in survivors\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5fb62e0-fd6f-44bb-8bb1-557e1746a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏆 Final leaderboard\n",
    "# print(f\"\\n🏆 Finalists after all rounds: {[m['label'] for m in finalists]}\")\n",
    "\n",
    "# print(\"\\n🏆 Finalists after all rounds:\")\n",
    "# for i, model in enumerate(finalists, 1):\n",
    "#     print(f\"{i}. {model['label']}_G{model['generation']} | PR AUC: {model['score']['pr_auc']:.3f} | Lineage: {model['lineage']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbc218e2-3a91-494d-b5b2-c33c448b041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 🧾 Build summary table from finalists\n",
    "final_summary = []\n",
    "for model in finalists:\n",
    "    pr_auc = model['score']['pr_auc']\n",
    "    baseline = model.get('baseline_pr_auc', 0.0)\n",
    "    final_summary.append({\n",
    "        \"Label\": f\"{model['label']}_G{model['generation']}\",\n",
    "        \"Generation\": model['generation'],\n",
    "        \"PR AUC\": round(pr_auc, 3),\n",
    "        \"Lineage\": model.get(\"lineage\", \"—\"),\n",
    "        \"Beats Baseline?\": \"✅ Yes\" if pr_auc > baseline else \"❌ No\",\n",
    "        \"Baseline PR AUC\": round(baseline, 3),\n",
    "        \"Dataset\": model.get(\"dataset_name\", \"unknown\")\n",
    "    })\n",
    "\n",
    "# df_final = pd.DataFrame(final_summary).sort_values(by=[\"PR AUC\"], ascending=False)\n",
    "df_final = pd.DataFrame(final_summary)\n",
    "\n",
    "# 📊 Display final summary\n",
    "# print(\"\\n📋 Finalist Leaderboard:\")\n",
    "# print(df_final.to_string(index=False))\n",
    "\n",
    "# 💾 Optionally save\n",
    "df_final.to_csv(\"logs/finalists_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edf6cb40-8dbf-4896-8461-5a50dde8eb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Generation</th>\n",
       "      <th>PR AUC</th>\n",
       "      <th>Lineage</th>\n",
       "      <th>Beats Baseline?</th>\n",
       "      <th>Baseline PR AUC</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elastic_L30_C1 + Manual Upsampling_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.664</td>\n",
       "      <td>origin of Elastic_L30_C1 + Manual Upsampling</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elastic_L30_C1 + SMOTE_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.671</td>\n",
       "      <td>origin of Elastic_L30_C1 + SMOTE</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elastic_L30_C1 + ADASYN_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.484</td>\n",
       "      <td>origin of Elastic_L30_C1 + ADASYN</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elastic_L30_C1 + Borderline SMOTE_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.515</td>\n",
       "      <td>origin of Elastic_L30_C1 + Borderline SMOTE</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elastic_L30_C1 + SMOTETomek_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.671</td>\n",
       "      <td>origin of Elastic_L30_C1 + SMOTETomek</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Elastic_L30_C1 + SMOTEENN_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.658</td>\n",
       "      <td>origin of Elastic_L30_C1 + SMOTEENN</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Elastic_L30_C1 + Random Undersample_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.663</td>\n",
       "      <td>origin of Elastic_L30_C1 + Random Undersample</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Elastic_L30_C1 + Cluster Centroids_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.669</td>\n",
       "      <td>origin of Elastic_L30_C1 + Cluster Centroids</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Elastic_L30_C1 + No Resampling_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.671</td>\n",
       "      <td>origin of Elastic_L30_C1 + No Resampling</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Elastic_L50_C5 + Manual Upsampling_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.663</td>\n",
       "      <td>origin of Elastic_L50_C5 + Manual Upsampling</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Elastic_L50_C5 + SMOTE_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.672</td>\n",
       "      <td>origin of Elastic_L50_C5 + SMOTE</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Elastic_L50_C5 + ADASYN_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.481</td>\n",
       "      <td>origin of Elastic_L50_C5 + ADASYN</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Elastic_L50_C5 + Borderline SMOTE_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.511</td>\n",
       "      <td>origin of Elastic_L50_C5 + Borderline SMOTE</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Elastic_L50_C5 + SMOTETomek_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.672</td>\n",
       "      <td>origin of Elastic_L50_C5 + SMOTETomek</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Elastic_L50_C5 + SMOTEENN_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.656</td>\n",
       "      <td>origin of Elastic_L50_C5 + SMOTEENN</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Elastic_L50_C5 + Random Undersample_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.631</td>\n",
       "      <td>origin of Elastic_L50_C5 + Random Undersample</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Elastic_L50_C5 + Cluster Centroids_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.647</td>\n",
       "      <td>origin of Elastic_L50_C5 + Cluster Centroids</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Elastic_L50_C5 + No Resampling_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.669</td>\n",
       "      <td>origin of Elastic_L50_C5 + No Resampling</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Elastic_L70_C10 + Manual Upsampling_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.663</td>\n",
       "      <td>origin of Elastic_L70_C10 + Manual Upsampling</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Elastic_L70_C10 + SMOTE_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.672</td>\n",
       "      <td>origin of Elastic_L70_C10 + SMOTE</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Elastic_L70_C10 + ADASYN_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.479</td>\n",
       "      <td>origin of Elastic_L70_C10 + ADASYN</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Elastic_L70_C10 + Borderline SMOTE_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.511</td>\n",
       "      <td>origin of Elastic_L70_C10 + Borderline SMOTE</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Elastic_L70_C10 + SMOTETomek_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.672</td>\n",
       "      <td>origin of Elastic_L70_C10 + SMOTETomek</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Elastic_L70_C10 + SMOTEENN_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.656</td>\n",
       "      <td>origin of Elastic_L70_C10 + SMOTEENN</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Elastic_L70_C10 + Random Undersample_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.625</td>\n",
       "      <td>origin of Elastic_L70_C10 + Random Undersample</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Elastic_L70_C10 + Cluster Centroids_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.645</td>\n",
       "      <td>origin of Elastic_L70_C10 + Cluster Centroids</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Elastic_L70_C10 + No Resampling_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.668</td>\n",
       "      <td>origin of Elastic_L70_C10 + No Resampling</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Elastic_L100_C10 + Manual Upsampling_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.663</td>\n",
       "      <td>origin of Elastic_L100_C10 + Manual Upsampling</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Elastic_L100_C10 + SMOTE_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.670</td>\n",
       "      <td>origin of Elastic_L100_C10 + SMOTE</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Elastic_L100_C10 + ADASYN_G0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.479</td>\n",
       "      <td>origin of Elastic_L100_C10 + ADASYN</td>\n",
       "      <td>✅ Yes</td>\n",
       "      <td>0.055</td>\n",
       "      <td>baseline_easy_w5_n5_d0.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Label  Generation  PR AUC  \\\n",
       "0     Elastic_L30_C1 + Manual Upsampling_G0           0   0.664   \n",
       "1                 Elastic_L30_C1 + SMOTE_G0           0   0.671   \n",
       "2                Elastic_L30_C1 + ADASYN_G0           0   0.484   \n",
       "3      Elastic_L30_C1 + Borderline SMOTE_G0           0   0.515   \n",
       "4            Elastic_L30_C1 + SMOTETomek_G0           0   0.671   \n",
       "5              Elastic_L30_C1 + SMOTEENN_G0           0   0.658   \n",
       "6    Elastic_L30_C1 + Random Undersample_G0           0   0.663   \n",
       "7     Elastic_L30_C1 + Cluster Centroids_G0           0   0.669   \n",
       "8         Elastic_L30_C1 + No Resampling_G0           0   0.671   \n",
       "9     Elastic_L50_C5 + Manual Upsampling_G0           0   0.663   \n",
       "10                Elastic_L50_C5 + SMOTE_G0           0   0.672   \n",
       "11               Elastic_L50_C5 + ADASYN_G0           0   0.481   \n",
       "12     Elastic_L50_C5 + Borderline SMOTE_G0           0   0.511   \n",
       "13           Elastic_L50_C5 + SMOTETomek_G0           0   0.672   \n",
       "14             Elastic_L50_C5 + SMOTEENN_G0           0   0.656   \n",
       "15   Elastic_L50_C5 + Random Undersample_G0           0   0.631   \n",
       "16    Elastic_L50_C5 + Cluster Centroids_G0           0   0.647   \n",
       "17        Elastic_L50_C5 + No Resampling_G0           0   0.669   \n",
       "18   Elastic_L70_C10 + Manual Upsampling_G0           0   0.663   \n",
       "19               Elastic_L70_C10 + SMOTE_G0           0   0.672   \n",
       "20              Elastic_L70_C10 + ADASYN_G0           0   0.479   \n",
       "21    Elastic_L70_C10 + Borderline SMOTE_G0           0   0.511   \n",
       "22          Elastic_L70_C10 + SMOTETomek_G0           0   0.672   \n",
       "23            Elastic_L70_C10 + SMOTEENN_G0           0   0.656   \n",
       "24  Elastic_L70_C10 + Random Undersample_G0           0   0.625   \n",
       "25   Elastic_L70_C10 + Cluster Centroids_G0           0   0.645   \n",
       "26       Elastic_L70_C10 + No Resampling_G0           0   0.668   \n",
       "27  Elastic_L100_C10 + Manual Upsampling_G0           0   0.663   \n",
       "28              Elastic_L100_C10 + SMOTE_G0           0   0.670   \n",
       "29             Elastic_L100_C10 + ADASYN_G0           0   0.479   \n",
       "\n",
       "                                           Lineage Beats Baseline?  \\\n",
       "0     origin of Elastic_L30_C1 + Manual Upsampling           ✅ Yes   \n",
       "1                 origin of Elastic_L30_C1 + SMOTE           ✅ Yes   \n",
       "2                origin of Elastic_L30_C1 + ADASYN           ✅ Yes   \n",
       "3      origin of Elastic_L30_C1 + Borderline SMOTE           ✅ Yes   \n",
       "4            origin of Elastic_L30_C1 + SMOTETomek           ✅ Yes   \n",
       "5              origin of Elastic_L30_C1 + SMOTEENN           ✅ Yes   \n",
       "6    origin of Elastic_L30_C1 + Random Undersample           ✅ Yes   \n",
       "7     origin of Elastic_L30_C1 + Cluster Centroids           ✅ Yes   \n",
       "8         origin of Elastic_L30_C1 + No Resampling           ✅ Yes   \n",
       "9     origin of Elastic_L50_C5 + Manual Upsampling           ✅ Yes   \n",
       "10                origin of Elastic_L50_C5 + SMOTE           ✅ Yes   \n",
       "11               origin of Elastic_L50_C5 + ADASYN           ✅ Yes   \n",
       "12     origin of Elastic_L50_C5 + Borderline SMOTE           ✅ Yes   \n",
       "13           origin of Elastic_L50_C5 + SMOTETomek           ✅ Yes   \n",
       "14             origin of Elastic_L50_C5 + SMOTEENN           ✅ Yes   \n",
       "15   origin of Elastic_L50_C5 + Random Undersample           ✅ Yes   \n",
       "16    origin of Elastic_L50_C5 + Cluster Centroids           ✅ Yes   \n",
       "17        origin of Elastic_L50_C5 + No Resampling           ✅ Yes   \n",
       "18   origin of Elastic_L70_C10 + Manual Upsampling           ✅ Yes   \n",
       "19               origin of Elastic_L70_C10 + SMOTE           ✅ Yes   \n",
       "20              origin of Elastic_L70_C10 + ADASYN           ✅ Yes   \n",
       "21    origin of Elastic_L70_C10 + Borderline SMOTE           ✅ Yes   \n",
       "22          origin of Elastic_L70_C10 + SMOTETomek           ✅ Yes   \n",
       "23            origin of Elastic_L70_C10 + SMOTEENN           ✅ Yes   \n",
       "24  origin of Elastic_L70_C10 + Random Undersample           ✅ Yes   \n",
       "25   origin of Elastic_L70_C10 + Cluster Centroids           ✅ Yes   \n",
       "26       origin of Elastic_L70_C10 + No Resampling           ✅ Yes   \n",
       "27  origin of Elastic_L100_C10 + Manual Upsampling           ✅ Yes   \n",
       "28              origin of Elastic_L100_C10 + SMOTE           ✅ Yes   \n",
       "29             origin of Elastic_L100_C10 + ADASYN           ✅ Yes   \n",
       "\n",
       "    Baseline PR AUC                     Dataset  \n",
       "0             0.055  baseline_easy_w5_n5_d0.csv  \n",
       "1             0.055  baseline_easy_w5_n5_d0.csv  \n",
       "2             0.055  baseline_easy_w5_n5_d0.csv  \n",
       "3             0.055  baseline_easy_w5_n5_d0.csv  \n",
       "4             0.055  baseline_easy_w5_n5_d0.csv  \n",
       "5             0.055  baseline_easy_w5_n5_d0.csv  \n",
       "6             0.055  baseline_easy_w5_n5_d0.csv  \n",
       "7             0.055  baseline_easy_w5_n5_d0.csv  \n",
       "8             0.055  baseline_easy_w5_n5_d0.csv  \n",
       "9             0.055  baseline_easy_w5_n5_d0.csv  \n",
       "10            0.055  baseline_easy_w5_n5_d0.csv  \n",
       "11            0.055  baseline_easy_w5_n5_d0.csv  \n",
       "12            0.055  baseline_easy_w5_n5_d0.csv  \n",
       "13            0.055  baseline_easy_w5_n5_d0.csv  \n",
       "14            0.055  baseline_easy_w5_n5_d0.csv  \n",
       "15            0.055  baseline_easy_w5_n5_d0.csv  \n",
       "16            0.055  baseline_easy_w5_n5_d0.csv  \n",
       "17            0.055  baseline_easy_w5_n5_d0.csv  \n",
       "18            0.055  baseline_easy_w5_n5_d0.csv  \n",
       "19            0.055  baseline_easy_w5_n5_d0.csv  \n",
       "20            0.055  baseline_easy_w5_n5_d0.csv  \n",
       "21            0.055  baseline_easy_w5_n5_d0.csv  \n",
       "22            0.055  baseline_easy_w5_n5_d0.csv  \n",
       "23            0.055  baseline_easy_w5_n5_d0.csv  \n",
       "24            0.055  baseline_easy_w5_n5_d0.csv  \n",
       "25            0.055  baseline_easy_w5_n5_d0.csv  \n",
       "26            0.055  baseline_easy_w5_n5_d0.csv  \n",
       "27            0.055  baseline_easy_w5_n5_d0.csv  \n",
       "28            0.055  baseline_easy_w5_n5_d0.csv  \n",
       "29            0.055  baseline_easy_w5_n5_d0.csv  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b51b5-05b3-421a-9fc4-34f4aa1967f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b64524-92f5-4d2c-9430-1830871b4d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2552b8-99ac-4e65-84e1-487833dc2353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
