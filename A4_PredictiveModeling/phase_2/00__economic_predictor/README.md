ğŸ² economic_predictor_playtime

This folder contains the core logic and creative modeling process behind our rare event predictor for synthetic economic data. We structure learning like play: training wheels, make-believe data, and gradual progression from naive guesses to interpretable causal diagnostics.

ğŸ§  Purpose

To simulate realistic economic crises (e.g. market shocks, stress), run experiments on rare event prediction, and reverse-engineer what factors may have contributedâ€”all without touching real data.

This approach matters because real-world economic data is messy, biased, and often limited by privacy or availability. By using synthetic datasets:
	â€¢	We train safely â€” avoiding overfitting to history or revealing sensitive info.
	â€¢	We simulate edge cases â€” rare events like crashes or systemic stress are underrepresented in historical data.
	â€¢	We promote interpretability â€” reverse-engineering from synthetic causes helps build intuition, not just prediction.
	â€¢	We democratize modeling â€” anyone (students, researchers, citizen scientists) can explore high-impact scenarios without barriers.

In short: synthetic training is playâ€”with a purpose.

â¸»

ğŸ§¸ economic_predictor_playtime/

A whimsical yet rigorous space to:
	1.	Generate blinded synthetic economic datasets
	2.	Train stacker ensembles (ElasticNet, XGBoost)
	3.	Achieve PR AUC liftoff to detect market stress
	4.	Perform reverse PCA unblinding to understand â€œwhat features caused it?â€

ğŸ§  Notebooks:
	â€¢	Step1_Initial_Exploration.ipynb: Blind runs with base models
	â€¢	Step2_Liftoff_Baseline.ipynb: Proof-of-liftoff for rare events
	â€¢	Step3_Lift_Booster.ipynb: Conservative boosting (avoiding overfit)
	â€¢	Step4_Ensemble_Blender.ipynb: Logistic + XGBoost ensemble
	â€¢	Step5_CausalDiagnostics.ipynb: PCA-based causal interpretation and mapping to economic concepts

ğŸ“ Datasets:
	â€¢	market_shock_synthetic_datasets/: Simulated crisis scenarios
	â€¢	â€œFeaturesâ€ are generic (feature_0, feature_1â€¦), later reverse-mapped to concepts like GDP, CPI, PMI, ETF_flows, etc.

ğŸ“˜ Concept Pools:
	â€¢	layman_friendly: â€œgas pricesâ€, â€œconsumer sentimentâ€
	â€¢	professional_signal: â€œyield curve slopeâ€, â€œcredit spreadâ€
	â€¢	academic_theory: â€œoutput gapâ€, â€œnatural rate of unemploymentâ€
	â€¢	model_outputs: â€œeconomic_surprise_indexâ€, â€œnowcast_gdpâ€
	â€¢	linchpins: â€œFED_FUNDS_RATEâ€, â€œGDPâ€, â€œUNEMPLOYMENT_RATEâ€

â¸»

ğŸ“‚ Structure

economic_predictor_playtime/
â”œâ”€â”€ champion_packages/               # Trained base models (ElasticNet, etc.)
â”œâ”€â”€ champion_stacks/                 # Logistic regression stackers on base models
â”œâ”€â”€ xgboost_stacks/                  # XGBoost stackers for boosting ensemble lift
â”œâ”€â”€ market_shock_synthetic_datasets/ # Diverse generated datasets (easy â†’ extreme)
â”œâ”€â”€ archive/                         # (Optional) Legacy outputs and backups

# ğŸš€ Five Steps of Learning Progression
â”œâ”€â”€ Step1_Initial_Exploration.ipynb     # Baseline models (blind guesswork)
â”œâ”€â”€ Step2_Liftoff_Baseline.ipynb        # Breed-and-battle ElasticNet tournament
â”œâ”€â”€ Step3_Lift_Booster.ipynb            # Boosting (XGBoost) after champion selection
â”œâ”€â”€ Step4_Ensemble_Blender.ipynb        # Weighted blending of logistic + XGBoost
â”œâ”€â”€ Step5_CausalDiagnostics.ipynb       # PCA unblinding to interpret rare event causes

# ğŸ“Š Results and Leaderboards
â”œâ”€â”€ champion_cross_eval_results.csv     # All champion models on all datasets
â”œâ”€â”€ ensemble_lift_leaderboard.csv       # Top stacker + blend model performance
â”œâ”€â”€ ensemble_blended_lift_results.csv   # Final liftoff tracker
â”œâ”€â”€ xgboost_lift_results.csv            # Boost-only benchmark

# ğŸ“˜ Documentation
â”œâ”€â”€ README.md                           # Project guide (you're here)
â”œâ”€â”€ *.pdf                               # Notebook exports for presentation/review

â¸»

ğŸ“ Notebook Guide

âœ… Step 1: Step1_Initial_Exploration.ipynb

Goal: Evaluate naive models on synthetic rare event datasets.
	â€¢	Introduces the concept of rare event detection
	â€¢	Uses a variety of imbalanced, noisy, and overlapping synthetic datasets
	â€¢	Measures model performance vs. a random baseline
ğŸ‘¶ Like a child learning balance with training wheels

â¸»

ğŸš€ Step 2: Step2_Liftoff_Baseline.ipynb

Goal: Run a tournament where elastic net models evolve over generations to beat baseline PR AUC.
	â€¢	Implements a â€œbreed and battleâ€ evolution system
	â€¢	Shows how liftoff emerges: the first moments where models begin to consistently beat randomness
ğŸ§¬ Models that â€œsurviveâ€ are saved as champions for downstream use.

â¸»

ğŸ”§ Step 3: Step3_Lift_Booster.ipynb

Goal: Carefully apply boosting methods like XGBoost to further improve performance
	â€¢	Uses champion models from Step 2 as inputs
	â€¢	Adds stacking (logistic + XGBoost) for smarter ensembling
ğŸ¯ Focuses on staying conservativeâ€”avoiding overfitting and checking for generalizability.

â¸»

ğŸ§ª Step 4: Step4_Ensemble_Blender.ipynb

Goal: Create blended ensemble models that approach or exceed state-of-the-art performance
	â€¢	Uses weighted blending of stackers
	â€¢	Measures final lift over baseline across all datasets
ğŸ”¥ This is where we hit our project target: PR AUC > 0.6 (SOTA liftoff threshold)

â¸»

ğŸ” Step 5: Step5_CausalDiagnostics.ipynb

Goal: Reverse-engineer what scenarios caused rare events using PCA and economic concept mapping
	â€¢	Blind and unblind: features are originally anonymized
	â€¢	Dynamically assigns simulated economic labels
	â€¢	Identifies which real-world signals (like â€œcredit_spreadâ€ or â€œjob_postingsâ€) were most associated with rare outcomes
ğŸ”¬ Helps humans learn from the machineâ€™s point of view

â¸»
