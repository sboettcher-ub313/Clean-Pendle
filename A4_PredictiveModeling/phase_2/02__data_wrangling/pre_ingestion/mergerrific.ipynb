{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a31296dc-5adc-434f-8e27-4d0df1bc0e91",
   "metadata": {},
   "source": [
    "we want to basically take all of the fred-pulled and -derived values\n",
    "\n",
    "and then push them into a unified dataset for downstream processing (unify with the other compatible datasets)\n",
    "\n",
    "note that we probably are dealing with duplicate columns and want to merge on date\n",
    "\n",
    "preserving the date but removing duplicates on non-date columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "019c33d0-475a-429f-adda-77296d7dd7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350237ca-5a57-434e-a5e7-e77fa74fdaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your folder containing all relevant CSVs\n",
    "folder_path = \"fredstuff/\"  # update this to your folder\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "\n",
    "# Initialize merged dataframe\n",
    "merged_df = None\n",
    "merged_columns = set()\n",
    "merge_key = \"date\"\n",
    "\n",
    "# Loop through and merge\n",
    "for filename in csv_files:\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Normalize column names\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "    if merge_key not in df.columns:\n",
    "        print(f\"Skipping {filename}: no 'date' column\")\n",
    "        continue\n",
    "\n",
    "    df[merge_key] = pd.to_datetime(df[merge_key])\n",
    "\n",
    "    # Only keep new columns + 'date'\n",
    "    if merged_df is None:\n",
    "        merged_df = df\n",
    "        merged_columns.update(df.columns)\n",
    "    else:\n",
    "        new_cols = [col for col in df.columns if col == merge_key or col not in merged_columns]\n",
    "        df = df[new_cols]\n",
    "        merged_df = pd.merge(merged_df, df, on=merge_key, how=\"outer\")\n",
    "        merged_columns.update(new_cols)\n",
    "\n",
    "# Interpolate all numeric columns forward + backward\n",
    "merged_df = merged_df.sort_values(\"date\")\n",
    "merged_df = merged_df.interpolate(method=\"linear\", limit_direction=\"both\")\n",
    "\n",
    "# Optionally drop rows or columns that are still empty\n",
    "merged_df = merged_df.dropna(axis=1, how=\"all\")  # remove columns fully NaN\n",
    "merged_df = merged_df.dropna(axis=0, how=\"any\")  # or drop rows with any NaNs\n",
    "\n",
    "# Final preview\n",
    "print(merged_df.head())\n",
    "\n",
    "# Optionally save\n",
    "merged_df.to_csv(\"merged_fredstuff.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cfa5d3-fe20-4ecc-90ed-fdc51974cdf1",
   "metadata": {},
   "source": [
    "note that some signals like aapl are not really clear\n",
    "\n",
    "we considered dropping them, but after unifying our feature matrix for downstream\n",
    "\n",
    "scalarization and blinding, the weak signals or redundant ones will be dropped anyway\n",
    "\n",
    "so we arent really worried\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1f9d7de-0018-4ca2-96b8-64688e078479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  ^gspc  aapl  msft  qqq  xlf  gdp_growth  unemployment_rate  \\\n",
      "0 1997-01-01    0.0   0.0   0.0  0.0  0.0   10002.179                4.0   \n",
      "1 1997-02-01    0.0   0.0   0.0  0.0  0.0   10002.179                4.0   \n",
      "2 1997-03-01    0.0   0.0   0.0  0.0  0.0   10002.179                4.0   \n",
      "3 1997-04-01    0.0   0.0   0.0  0.0  0.0   10002.179                4.0   \n",
      "4 1997-05-01    0.0   0.0   0.0  0.0  0.0   10002.179                4.0   \n",
      "\n",
      "   cpi_inflation  consumer_sentiment_index  ...  Low_FORD  Close_FORD  \\\n",
      "0          169.3                     112.0  ...    28.125      30.625   \n",
      "1          169.3                     112.0  ...    28.125      30.625   \n",
      "2          169.3                     112.0  ...    28.125      30.625   \n",
      "3          169.3                     112.0  ...    28.125      30.625   \n",
      "4          169.3                     112.0  ...    28.125      30.625   \n",
      "\n",
      "   Adj Close_FORD  Volume_FORD  Open_AAPL  High_AAPL  Low_AAPL  Close_AAPL  \\\n",
      "0          30.625       4260.0   0.936384   1.004464  0.907924    0.999442   \n",
      "1          30.625       4260.0   0.936384   1.004464  0.907924    0.999442   \n",
      "2          30.625       4260.0   0.936384   1.004464  0.907924    0.999442   \n",
      "3          30.625       4260.0   0.936384   1.004464  0.907924    0.999442   \n",
      "4          30.625       4260.0   0.936384   1.004464  0.907924    0.999442   \n",
      "\n",
      "   Adj Close_AAPL  Volume_AAPL  \n",
      "0        0.842151  535796800.0  \n",
      "1        0.842151  535796800.0  \n",
      "2        0.842151  535796800.0  \n",
      "3        0.842151  535796800.0  \n",
      "4        0.842151  535796800.0  \n",
      "\n",
      "[5 rows x 155 columns]\n"
     ]
    }
   ],
   "source": [
    "# --- Load and prepare FRED dataset ---\n",
    "fred_df = pd.read_csv(\"merged_fredstuff.csv\")\n",
    "fred_df['date'] = pd.to_datetime(fred_df['date'])\n",
    "\n",
    "# --- Load and prepare stock dataset ---\n",
    "stock_df = pd.read_csv(\"preview_unified_stock_data.csv\")\n",
    "\n",
    "# Normalize the 'Date' column to 'date'\n",
    "if 'Date' in stock_df.columns:\n",
    "    stock_df.rename(columns={\"Date\": \"date\"}, inplace=True)\n",
    "\n",
    "stock_df['date'] = pd.to_datetime(stock_df['date'])\n",
    "\n",
    "# --- Identify shared columns (non-date) ---\n",
    "shared_columns = set(fred_df.columns).intersection(stock_df.columns) - {\"date\"}\n",
    "\n",
    "# Drop shared non-date columns from stock_df before merging\n",
    "stock_df_filtered = stock_df.drop(columns=shared_columns)\n",
    "\n",
    "# --- Merge on 'date' ---\n",
    "merged_combined_df = pd.merge(fred_df, stock_df_filtered, on=\"date\", how=\"outer\")\n",
    "\n",
    "# Interpolate all numeric columns forward + backward\n",
    "merged_combined_df = merged_combined_df.sort_values(\"date\")\n",
    "merged_combined_df = merged_combined_df.interpolate(method=\"linear\", limit_direction=\"both\")\n",
    "\n",
    "# Optionally drop rows or columns that are still empty\n",
    "merged_combined_df = merged_combined_df.dropna(axis=1, how=\"all\")  # remove columns fully NaN\n",
    "merged_combined_df = merged_combined_df.dropna(axis=0, how=\"any\")  # or drop rows with any NaNs\n",
    "\n",
    "# --- Optional: Save or inspect result ---\n",
    "print(merged_combined_df.head())\n",
    "\n",
    "# Save result if desired\n",
    "merged_combined_df.to_csv(\"merged_fred_and_stock.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1900a8a7-d32f-4af5-a432-47c071154367",
   "metadata": {},
   "source": [
    "now we really want to add the first week's stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fed308ad-3cd7-47ef-815b-dc465f4eec0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  ^gspc  aapl  msft  qqq  xlf  gdp_growth  unemployment_rate  \\\n",
      "0 1997-01-01    0.0   0.0   0.0  0.0  0.0   10002.179                4.0   \n",
      "1 1997-02-01    0.0   0.0   0.0  0.0  0.0   10002.179                4.0   \n",
      "2 1997-03-01    0.0   0.0   0.0  0.0  0.0   10002.179                4.0   \n",
      "3 1997-04-01    0.0   0.0   0.0  0.0  0.0   10002.179                4.0   \n",
      "4 1997-05-01    0.0   0.0   0.0  0.0  0.0   10002.179                4.0   \n",
      "\n",
      "   cpi_inflation  consumer_sentiment_index  ...  \\\n",
      "0          169.3                     112.0  ...   \n",
      "1          169.3                     112.0  ...   \n",
      "2          169.3                     112.0  ...   \n",
      "3          169.3                     112.0  ...   \n",
      "4          169.3                     112.0  ...   \n",
      "\n",
      "   interest rate_lag30_lag90_lag180_rolling90_rolling180  \\\n",
      "0                                           1.746524       \n",
      "1                                           1.746524       \n",
      "2                                           1.746524       \n",
      "3                                           1.746524       \n",
      "4                                           1.746524       \n",
      "\n",
      "   consumer sentiment_lag30_lag90_lag180_rolling90_rolling180  \\\n",
      "0                                          91.359951            \n",
      "1                                          91.359951            \n",
      "2                                          91.359951            \n",
      "3                                          91.359951            \n",
      "4                                          91.359951            \n",
      "\n",
      "   gdp_lag30_lag90_lag180_rolling90_rolling180  rolling_std_30d  market crash  \\\n",
      "0                                 10880.208017        20.100536          31.0   \n",
      "1                                 10880.208017        20.100536          31.0   \n",
      "2                                 10880.208017        20.100536          31.0   \n",
      "3                                 10880.208017        20.100536          31.0   \n",
      "4                                 10880.208017        20.100536          31.0   \n",
      "\n",
      "   buy gold  recession  inflation  stock market crash  interest rates  \n",
      "0      28.0        4.0       49.0                 7.0            61.0  \n",
      "1      28.0        4.0       49.0                 7.0            61.0  \n",
      "2      28.0        4.0       49.0                 7.0            61.0  \n",
      "3      28.0        4.0       49.0                 7.0            61.0  \n",
      "4      28.0        4.0       49.0                 7.0            61.0  \n",
      "\n",
      "[5 rows x 357 columns]\n"
     ]
    }
   ],
   "source": [
    "# --- Load existing merged FRED + Stock dataset ---\n",
    "merged_df = pd.read_csv(\"merged_fred_and_stock.csv\")\n",
    "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "\n",
    "# --- Load financial signals ---\n",
    "financial_df = pd.read_csv(\"preview_financial_data_cleaned2.csv\")\n",
    "\n",
    "# Normalize column names\n",
    "if 'date' not in financial_df.columns:\n",
    "    financial_df.columns = [col.lower() for col in financial_df.columns]\n",
    "financial_df['date'] = pd.to_datetime(financial_df['date'])\n",
    "\n",
    "# --- Remove duplicate non-date columns ---\n",
    "shared_cols = set(merged_df.columns).intersection(financial_df.columns) - {\"date\"}\n",
    "financial_df = financial_df.drop(columns=shared_cols)\n",
    "\n",
    "# --- Merge financial signals into the master frame ---\n",
    "final_merged_df = pd.merge(merged_df, financial_df, on=\"date\", how=\"outer\")\n",
    "\n",
    "# Interpolate all numeric columns forward + backward\n",
    "final_merged_df = final_merged_df.sort_values(\"date\")\n",
    "final_merged_df = final_merged_df.interpolate(method=\"linear\", limit_direction=\"both\")\n",
    "\n",
    "# Optionally drop rows or columns that are still empty\n",
    "final_merged_df = final_merged_df.dropna(axis=1, how=\"all\")  # remove columns fully NaN\n",
    "final_merged_df = final_merged_df.dropna(axis=0, how=\"any\")  # or drop rows with any NaNs\n",
    "\n",
    "# --- Preview result ---\n",
    "print(final_merged_df.head())\n",
    "\n",
    "# Optional: save\n",
    "final_merged_df.to_csv(\"merged_fred_stock_financial.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4570dc-de1a-4b9e-873b-d8b080bcdd83",
   "metadata": {},
   "source": [
    "get the cells from nav net asset value column and \n",
    "\n",
    "change the name of the columns by using the associated scheme code\n",
    "\n",
    "to make the nav mutual funds data integrate-able"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "892ecb5e-51aa-4765-b246-ddb8a38d68dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  nav_100794  nav_104681  nav_104683  nav_105603  nav_105604  \\\n",
      "0 2025-04-09     29.7061     15.0257     33.8509     31.4783     16.7995   \n",
      "1 2025-04-10         NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "   nav_106793  nav_106795  nav_106797  nav_115398  ...  nav_149451  \\\n",
      "0      30.231      10.915       20.89         NaN  ...         NaN   \n",
      "1         NaN         NaN         NaN    2477.978  ...      1000.0   \n",
      "\n",
      "   nav_149452  nav_149453  nav_149454  nav_151134  nav_151135  nav_151136  \\\n",
      "0         NaN         NaN         NaN     18.7291     10.5205     11.1456   \n",
      "1   1214.0747      1000.0   1213.8653         NaN         NaN         NaN   \n",
      "\n",
      "   nav_151137  nav_151138  nav_151140  \n",
      "0     10.9532     20.0211     10.5399  \n",
      "1         NaN         NaN         NaN  \n",
      "\n",
      "[2 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Load mutual fund NAV dataset ---\n",
    "mf_df = pd.read_csv(\"preview_cleaned_mutual_fund_nav copy.csv\")\n",
    "\n",
    "# --- Normalize column names and types ---\n",
    "mf_df.columns = [col.strip() for col in mf_df.columns]  # remove any whitespace\n",
    "mf_df['NAV__Scheme Code'] = mf_df['NAV__Scheme Code'].astype(str)\n",
    "mf_df['date'] = pd.to_datetime(mf_df['date'])\n",
    "\n",
    "# --- Pivot: one column per scheme code, values = NAV ---\n",
    "mf_pivot = mf_df.pivot(\n",
    "    index='date',\n",
    "    columns='NAV__Scheme Code',\n",
    "    values='NAV__Net Asset Value'\n",
    ")\n",
    "\n",
    "# --- Rename columns with prefix for clarity ---\n",
    "mf_pivot.columns = [f\"nav_{code}\" for code in mf_pivot.columns]\n",
    "\n",
    "# --- Reset index so 'date' becomes a column again ---\n",
    "mf_pivot = mf_pivot.reset_index()\n",
    "\n",
    "# --- Preview result ---\n",
    "print(mf_pivot.head())\n",
    "\n",
    "# Now this `mf_pivot` DataFrame can be merged on `date` with your final feature matrix like so:\n",
    "final_merged_df_w_nav = pd.merge(final_merged_df, mf_pivot, on='date', how='outer')\n",
    "\n",
    "# Interpolate all numeric columns forward + backward\n",
    "final_merged_df_w_nav = final_merged_df_w_nav.sort_values(\"date\")\n",
    "final_merged_df_w_nav = final_merged_df_w_nav.interpolate(method=\"linear\", limit_direction=\"both\")\n",
    "\n",
    "# Optionally drop rows or columns that are still empty\n",
    "final_merged_df_w_nav = final_merged_df_w_nav.dropna(axis=1, how=\"all\")  # remove columns fully NaN\n",
    "final_merged_df_w_nav = final_merged_df_w_nav.dropna(axis=0, how=\"any\")  # or drop rows with any NaNs\n",
    "\n",
    "# Optional: save\n",
    "final_merged_df_w_nav.to_csv(\"merged_fred_stock_financial_w_nav.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8bc5162-27e2-4f25-aa8f-37a71020c2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>^gspc</th>\n",
       "      <th>aapl</th>\n",
       "      <th>msft</th>\n",
       "      <th>qqq</th>\n",
       "      <th>xlf</th>\n",
       "      <th>gdp_growth</th>\n",
       "      <th>unemployment_rate</th>\n",
       "      <th>cpi_inflation</th>\n",
       "      <th>consumer_sentiment_index</th>\n",
       "      <th>...</th>\n",
       "      <th>nav_149451</th>\n",
       "      <th>nav_149452</th>\n",
       "      <th>nav_149453</th>\n",
       "      <th>nav_149454</th>\n",
       "      <th>nav_151134</th>\n",
       "      <th>nav_151135</th>\n",
       "      <th>nav_151136</th>\n",
       "      <th>nav_151137</th>\n",
       "      <th>nav_151138</th>\n",
       "      <th>nav_151140</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10002.179</td>\n",
       "      <td>4.0</td>\n",
       "      <td>169.3</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1214.0747</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1213.8653</td>\n",
       "      <td>18.7291</td>\n",
       "      <td>10.5205</td>\n",
       "      <td>11.1456</td>\n",
       "      <td>10.9532</td>\n",
       "      <td>20.0211</td>\n",
       "      <td>10.5399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997-02-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10002.179</td>\n",
       "      <td>4.0</td>\n",
       "      <td>169.3</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1214.0747</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1213.8653</td>\n",
       "      <td>18.7291</td>\n",
       "      <td>10.5205</td>\n",
       "      <td>11.1456</td>\n",
       "      <td>10.9532</td>\n",
       "      <td>20.0211</td>\n",
       "      <td>10.5399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997-03-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10002.179</td>\n",
       "      <td>4.0</td>\n",
       "      <td>169.3</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1214.0747</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1213.8653</td>\n",
       "      <td>18.7291</td>\n",
       "      <td>10.5205</td>\n",
       "      <td>11.1456</td>\n",
       "      <td>10.9532</td>\n",
       "      <td>20.0211</td>\n",
       "      <td>10.5399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-04-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10002.179</td>\n",
       "      <td>4.0</td>\n",
       "      <td>169.3</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1214.0747</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1213.8653</td>\n",
       "      <td>18.7291</td>\n",
       "      <td>10.5205</td>\n",
       "      <td>11.1456</td>\n",
       "      <td>10.9532</td>\n",
       "      <td>20.0211</td>\n",
       "      <td>10.5399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-05-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10002.179</td>\n",
       "      <td>4.0</td>\n",
       "      <td>169.3</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1214.0747</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1213.8653</td>\n",
       "      <td>18.7291</td>\n",
       "      <td>10.5205</td>\n",
       "      <td>11.1456</td>\n",
       "      <td>10.9532</td>\n",
       "      <td>20.0211</td>\n",
       "      <td>10.5399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 407 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  ^gspc  aapl  msft  qqq  xlf  gdp_growth  unemployment_rate  \\\n",
       "0 1997-01-01    0.0   0.0   0.0  0.0  0.0   10002.179                4.0   \n",
       "1 1997-02-01    0.0   0.0   0.0  0.0  0.0   10002.179                4.0   \n",
       "2 1997-03-01    0.0   0.0   0.0  0.0  0.0   10002.179                4.0   \n",
       "3 1997-04-01    0.0   0.0   0.0  0.0  0.0   10002.179                4.0   \n",
       "4 1997-05-01    0.0   0.0   0.0  0.0  0.0   10002.179                4.0   \n",
       "\n",
       "   cpi_inflation  consumer_sentiment_index  ...  nav_149451  nav_149452  \\\n",
       "0          169.3                     112.0  ...      1000.0   1214.0747   \n",
       "1          169.3                     112.0  ...      1000.0   1214.0747   \n",
       "2          169.3                     112.0  ...      1000.0   1214.0747   \n",
       "3          169.3                     112.0  ...      1000.0   1214.0747   \n",
       "4          169.3                     112.0  ...      1000.0   1214.0747   \n",
       "\n",
       "   nav_149453  nav_149454  nav_151134  nav_151135  nav_151136  nav_151137  \\\n",
       "0      1000.0   1213.8653     18.7291     10.5205     11.1456     10.9532   \n",
       "1      1000.0   1213.8653     18.7291     10.5205     11.1456     10.9532   \n",
       "2      1000.0   1213.8653     18.7291     10.5205     11.1456     10.9532   \n",
       "3      1000.0   1213.8653     18.7291     10.5205     11.1456     10.9532   \n",
       "4      1000.0   1213.8653     18.7291     10.5205     11.1456     10.9532   \n",
       "\n",
       "   nav_151138  nav_151140  \n",
       "0     20.0211     10.5399  \n",
       "1     20.0211     10.5399  \n",
       "2     20.0211     10.5399  \n",
       "3     20.0211     10.5399  \n",
       "4     20.0211     10.5399  \n",
       "\n",
       "[5 rows x 407 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_merged_df_w_nav.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c5cded-5c62-4912-8d60-6c5cba64c371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
